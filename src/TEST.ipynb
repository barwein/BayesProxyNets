{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import time\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpyro.handlers\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import networkx as nx\n",
    "# import xarray as xr\n",
    "import seaborn as sns \n",
    "# from scipy.special import expit, logit\n",
    "import scipy as sp\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, vmap\n",
    "from jax.scipy.special import expit, logsumexp, logit\n",
    "from functools import partial\n",
    "import numpyro.distributions as dist \n",
    "import numpyro\n",
    "from numpyro.contrib.funsor import config_enumerate\n",
    "# from numpyro.infer.reparam import LocScaleReparam\n",
    "\n",
    "# from numpyro.util import set_host_device_count\n",
    "# from numpyro.contrib.control_flow import scan\n",
    "# from numpyro.ops.indexing import Vindex\n",
    "from tqdm import tqdm\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "from numpyro import handlers\n",
    "from numpyro.infer import MCMC, HMC, NUTS, DiscreteHMCGibbs, MixedHMC, Predictive\n",
    "import pyro\n",
    "# import pymc_experimental as pmx\n",
    "\n",
    "import multiprocessing\n",
    "import socket\n",
    "\n",
    "import hsgp.approximation\n",
    "from hsgp.approximation import hsgp_squared_exponential, eigenfunctions\n",
    "from hsgp.spectral_densities import diag_spectral_density_squared_exponential\n",
    "# from numpyro.contrib.hsgp.approximation import hsgp_squared_exponential\n",
    "\n",
    "print(f\"Running on PyMC v{pm.__version__}\")\n",
    "# print(f\"Running on PyMC-experimental v{pmx.__version__}\")\n",
    "print(f\"Running on NumPyro v{numpyro.__version__}\")\n",
    "print(f\"Running on Pyro v{pyro.__version__}\")\n",
    "print(f\"Running on JAX v{jax.__version__}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Set CPU for JAX\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=4\"\n",
    "print(jax.devices('cpu'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7910fe36bb7d5b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 89253\n",
    "# rng = np.random.default_rng(RANDOM_SEED)\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use(\"arviz-white\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3feb392ce909051",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# n=150\n",
    "# n=300\n",
    "n=500\n",
    "tril_dim = int(n*(n-1)/2)  \n",
    "# x = rng.integers(low=0,high=1,size=n,endpoint=True)\n",
    "x = rng.normal(loc=0,scale=3,size=n)\n",
    "# x = rng.standard_t(df=10, size= n)\n",
    "# x = rng.uniform(size=n,low=-3,high=3)\n",
    "# x = rng.standard_t(size=n,df=5)\n",
    "x2 = rng.binomial(n=1,p=0.1,size=n)\n",
    "\n",
    "U_latent = rng.normal(loc=0,scale=1,size=(n,2))\n",
    "# x2 = np.random.choice([\"A\",\"B\",\"C\"], size = n, p=[0.25,0.35,0.6])\n",
    "# x_diff = []\n",
    "# # x2_equal =[]\n",
    "# for i in range(n):\n",
    "#     for j in range(i+1,n):\n",
    "#         x_diff.append(np.abs(x[i]-x[j]))\n",
    "#         # x2_equal.append(x2[i]==x2[j])\n",
    "# x_diff = np.array(x_diff)\n",
    "# x2_equal = np.array(x2_equal)\n",
    "# plt.hist(x_diff)\n",
    "# Create probs\n",
    "# beta = -3, -.5,2 \n",
    "# beta = -2, -.5\n",
    "\n",
    "idx_pairs = list(combinations(range(len(x)),2))\n",
    "x_diff = jnp.array([abs(x[i] - x[j]) for i, j in idx_pairs])\n",
    "\n",
    "x2_equal = jnp.array([1 if x2[i]==1 and x2[j]==1 else 0 for i,j in idx_pairs])\n",
    "# x2_or = np.array([1 if (x2[i]==1 or x2[j]==1) and (x2[i]!=x2[j]) else 0 for i,j in idx_pairs])\n",
    "# x2_or = jnp.array([1 if (x2[i]==1 or x2[j]==1) else 0 for i,j in idx_pairs])\n",
    "x2_or = jnp.array([1 if (x2[i] + x2[j] == 1) else 0 for i,j in idx_pairs])\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.hist(x_diff)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6999d54995908789",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "jnp.dot(U_latent, U_latent.T).shape",
   "id": "bb9e5218e51a989d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# idx = jnp.triu_indices(n=n, k=1)\n",
    "# U_diff = U_latent[idx[0]] - U_latent[idx[1]]\n",
    "# U_diff_norm_val = jnp.linalg.norm(U_diff, axis=1)\n",
    "\n",
    "def latent_to_norm_of_diff(U):\n",
    "    idx = jnp.triu_indices(n=U.shape[0], k=1)\n",
    "    U_diff = U[idx[0]] - U[idx[1]]\n",
    "    # print(U_diff.shape)\n",
    "    return jnp.linalg.norm(U_diff, axis=1)\n",
    "\n",
    "\n",
    "def cosine_similarity_jax(U):\n",
    "    # Normalize the rows\n",
    "    U_normalized = U / jnp.linalg.norm(U, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute dot products for upper triangle\n",
    "    n = U.shape[0]\n",
    "    triu_indices = jnp.triu_indices(n, k=1)\n",
    "    dot_products = jnp.dot(U_normalized, U_normalized.T)[triu_indices]\n",
    "    \n",
    "    return dot_products\n",
    "\n",
    "U_diff_norm_val = latent_to_norm_of_diff(U_latent)\n",
    "U_cosine = cosine_similarity_jax(U_latent)"
   ],
   "id": "b765251ed206bd46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(x2_or.mean(), U_diff_norm_val.shape, n*(n-1)/2, U_diff_norm_val.mean(), U_cosine.mean(), U_cosine.shape)\n",
    "print(jnp.sum(U_cosine > 0) / U_cosine.shape[0], jnp.sum(U_cosine < 0) / U_cosine.shape[0])"
   ],
   "id": "b38b83e8640f4782",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# beta = -2, -.5\n",
    "beta = -2, -.5\n",
    "# beta = -2, -1\n",
    "# probs = expit(beta[0] + beta[1]*x_diff + rng.normal(-.5,1,tril_dim))\n",
    "# probs = expit(beta[0] + beta[1]*x_diff + beta[2]*x2_equal)\n",
    "# probs = expit(beta[0] + beta[1]*x_diff + beta[2]*x2_equal)\n",
    "# probs = expit(beta[0] + beta[1]*x_diff + 2*x2_or)\n",
    "# probs = expit(-2.5  + 1.5*x2_or)\n",
    "probs = expit(-2  + 1.5*x2_or - U_diff_norm_val)\n",
    "# probs = expit(-3  + 1*x2_or + U_cosine)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "# plt.scatter(x_diff,probs, c=x2_or)\n",
    "plt.scatter(U_diff_norm_val,probs, c=x2_or)\n",
    "# plt.scatter(U_cosine,probs, c=x2_or)\n",
    "\n",
    "# Generate adj. matrix\n",
    "mat = np.zeros((n,n))\n",
    "# idx_lt = np.tril_indices(n=n,k=-1)\n",
    "idx_ut = np.triu_indices(n=n,k=1)\n",
    "# edges = rng.binomial(n=1,p=0.5,size=tril_dim)\n",
    "edges = rng.binomial(n=1,p=probs,size=tril_dim)\n",
    "\n",
    "mat[idx_ut] = edges\n",
    "# mat[idx_lt] = edges\n",
    "mat = mat + mat.T\n",
    "triu_vals = mat[np.triu_indices(n,k=1)]\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8ada7876be596ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# def eigen_centrality(adj_mat):\n",
    "#     sparse_mat = sp.sparse.csr_matrix(adj_mat)\n",
    "#     _, eig_vec = sp.sparse.linalg.eigs(sparse_mat, k=1, which = 'LR')\n",
    "#     largest_v = eig_vec.flatten().real\n",
    "#     norm = np.sign(largest_v.sum()) * sp.linalg.norm(largest_v)\n",
    "#     return largest_v / norm\n",
    "\n",
    "@jit\n",
    "def eigen_centrality(adj_mat):\n",
    "    # Ensure the matrix is symmetric\n",
    "    # adj_mat = (adj_mat + adj_mat.T) / 2\n",
    "    \n",
    "    # Compute the eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = jnp.linalg.eigh(adj_mat)\n",
    "    \n",
    "    # Find the index of the largest eigenvalue\n",
    "    largest_eigenvalue_index = jnp.argmax(eigenvalues)\n",
    "    \n",
    "    # Get the corresponding eigenvector\n",
    "    largest_eigenvector = eigenvectors[:, largest_eigenvalue_index]\n",
    "    \n",
    "    # Scale the eigenvector\n",
    "    norm = jnp.sign(largest_eigenvector.sum()) * jnp.linalg.norm(largest_eigenvector)\n",
    "    scaled_eigenvector = largest_eigenvector / norm\n",
    "    \n",
    "    return scaled_eigenvector"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3909316e33a5898",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "eigen_center = eigen_centrality(mat)\n",
    "# eigen_center_jx = eigen_centrality_jax(mat)\n",
    "\n",
    "# print(jnp.array_equal(jnp.round(eigen_center,5), jnp.round(eigen_center_jx,5)))\n",
    "\n",
    "# plt.figure(figsize=(3,2))\n",
    "# plt.scatter(eigen_center, eigen_center_jx)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6edcd32fc256128",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "print(np.mean(np.sum(mat,axis=1)))\n",
    "plt.hist(np.sum(mat,axis=1))"
   ],
   "id": "c493b8053c346523",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    n = len(data)\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, n+1) / n\n",
    "    return x, y\n",
    "\n",
    "deg_, cdf_ = ecdf(np.sum(mat,axis=1))\n",
    "\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.scatter(deg_, 1-cdf_)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('ECDF')\n",
    "plt.title('Empirical Cumulative Distribution Function (Log-Log Scale)')\n",
    "plt.grid(True, which='both', linestyle='--')\n",
    "plt.show()\n"
   ],
   "id": "5226a43352231b05",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"sum eigen\", np.sum(eigen_center))\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.hist(eigen_center)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9482eb4e7666de9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# mat_net = nx.from_numpy_array(mat)\n",
    "# \n",
    "# degrees = np.array([degree for _, degree in mat_net.degree()])\n",
    "# deg_center = np.array(list(nx.degree_centrality(mat_net).values()))\n",
    "# eigen_center = np.array(list(nx.eigenvector_centrality_numpy(mat_net).values()))\n",
    "# closeness = np.array(list(nx.closeness_centrality(mat_net).values()))\n",
    "# betweeness = np.array(list(nx.betweenness_centrality(mat_net).values()))\n",
    "# # flow_betweeness = np.array(list(nx.current_flow_betweenness_centrality(mat_net).values()))\n",
    "# # flow_betweeness = np.array(list(nx.approximate_current_flow_betweenness_centrality(mat_net,epsilon=0.1).values()))\n",
    "# triangles = np.array(list(nx.triangles(mat_net).values()))\n",
    "# clustering = np.array(list(nx.clustering(mat_net).values()))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f31eba8ce35f20f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# print(min(betweeness),max(betweeness))\n",
    "# print(min(eigen_center),max(eigen_center))\n",
    "# print(min(triangles),max(triangles))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b92b4a2d2e076bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "@jit\n",
    "def zeigen_value(Z, eig_cen, A_mat):\n",
    "    if Z.ndim == 1:  # Case when Z has shape (N,)\n",
    "        return jnp.dot(A_mat, Z * eig_cen)\n",
    "    elif Z.ndim == 2:  # Case when Z has shape (M, N)\n",
    "        return jnp.dot(Z * eig_cen, A_mat.T)  # Transpose A_mat for correct dimensions\n",
    "    # return jnp.dot(Z*eig_cen, A_mat.T)\n",
    "    # return jnp.dot(A_mat, Z*eig_cen)\n",
    "    # return jnp.dot(A_mat, Z*eig_cen)\n",
    "    # return jnp.dot(A_mat, jnp.multiply(Z, eig_cen))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d2f687789ca8e3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Create outcome data\n",
    "\n",
    "Z = rng.binomial(n=1,p=0.5,size=n)\n",
    "degrees = np.sum(mat,1)\n",
    "exposures = np.dot(mat,Z) / degrees\n",
    "exposures_inv_deg = np.dot(mat,Z*degrees)\n",
    "# print(pd.DataFrame(exposures).describe())\n",
    "# x_neighbors = np.dot(mat,x) / degrees\n",
    "# print(pd.DataFrame(x_neighbors).describe())\n",
    "# print(pd.DataFrame(x).describe())\n",
    "\n",
    "# Zeigen = np.dot(mat, Z*eigen_center)\n",
    "Zeigen = zeigen_value(Z, eigen_center, mat)\n",
    "\n",
    "print(pd.DataFrame({\"zeigen\": Zeigen,\"expos\" : exposures, \"exposinv\":exposures_inv_deg,\n",
    "                    \"Z\" : Z}).describe())\n",
    "print(np.corrcoef(Zeigen,exposures))\n",
    "# expos_binary = exposures > 0.5\n",
    "# print(np.mean(expos_binary))\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(Zeigen)\n",
    "# plt.scatter(exposures, Zeigen)\n",
    "# plt.scatter(exposures_inv_deg, exposures)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43ffa2ae3e1b333f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "oned = zeigen_value(Z, eigen_center, mat)\n",
    "multid = zeigen_value(Z.reshape((1,n)), eigen_center, mat)\n",
    "multid_m = jnp.mean(multid, axis=0)\n",
    "print(oned.shape)\n",
    "print(multid.shape)\n",
    "print(multid_m.shape)\n",
    "print(jnp.array_equal(jnp.round(oned, 4), jnp.round(multid_m,4)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bf543e167904a7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(np.mean((0.7 < Zeigen) & (Zeigen < 1)))\n",
    "print(np.mean((0.8 >= Zeigen)))\n",
    "print(np.mean((1 <= Zeigen)))\n",
    "print(np.mean((1.7 <= Zeigen)))\n",
    "print(np.mean((1.2 > Zeigen)))\n"
   ],
   "id": "3db65867baa38287",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "s = np.sort(Zeigen)\n",
    "s -= 0.1\n",
    "# f1 = 2.5*s\n",
    "# f2 = 0*(x < 0.25) + 0.2*np.exp(2*x)*(x>=0.25)\n",
    "# f2 = 0*(s < 0.2) + 5*2.5*np.power(s-0.2, 2)*(s>=0.2)\n",
    "\n",
    "def f2(s):\n",
    "    return np.piecewise(s, \n",
    "                        # [s < .8, (s >= .8) & (s < 1), s >= 1], \n",
    "                        # [s <= 0.6, (s < 1) & (s > 0.6), s >= 1], \n",
    "                        [s <= 1.2, s > 1.2], \n",
    "                        # [lambda s: 0, lambda s: 15*2.5*np.power(s-0.3, 2), 15*2.5*np.power(0.6-0.3, 2)])\n",
    "                        # [lambda s: np.maximum(5*2.5*(s-1), 0), lambda s: np.maximum(5*2.5*0.4 - 2.5*2.5*(s - 1), 0)])\n",
    "                        [lambda s: np.maximum(8*2.5*(s-.8), 0), lambda s: np.maximum(6*2.5*0.5 - 3*2.5*(s - 1.3), 0)])\n",
    "\n",
    "def f(x):\n",
    "    # return 2.5*(np.sin(10 * x) + np.cos(3 * x))\n",
    "    # return 2.5*(np.sin(4*np.pi * x) + np.log(x+1))\n",
    "    # return 1.5*2.5*(np.sin(2.5*(x-.25)) + np.log(x + 1))\n",
    "    return 1.2*2.5*(np.sin(4*(x-np.pi)) + np.log(x + 1))\n",
    "\n",
    "\n",
    "f0 = 2.5*s\n",
    "f1 = f(s)\n",
    "f2v= f2(s)\n",
    "# f3 = 2.5*2 / (1+np.exp(-10*(s-0.8)))\n",
    "# f3 = 2.5*2.5 / (1+np.exp(-5*s + 7.5))\n",
    "# f3 = np.exp(2.5*s)/150\n",
    "f3 = np.maximum(-5*np.power(s-1,2) + 2.5*3*np.log(s+1), 0)\n",
    "f4 = np.maximum(0, np.minimum(4*2.5*(s-.4),4*2.5*0.3))\n",
    "# f4 = np.maximum(0, np.minimum(8*2.5*(s-1.2),4*2.5*(s-1.1)))\n",
    "print(np.mean(f2v), np.median(f2v))\n",
    "# f4 = np.maximum(0, 2*2.5*(s-1.5))\n",
    "# f3 = -2.5*3*np.power(s- 1,2) + 3*2.5*s\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.scatter(s,f0, c=\"darkgreen\")\n",
    "# plt.scatter(s,f1, c=\"red\")\n",
    "plt.scatter(s,4*s*Z + f4, c=\"red\")\n",
    "# plt.scatter(s,f2v, c=\"blue\")\n",
    "# plt.scatter(s,f3, c=\"violet\")\n",
    "# plt.scatter(s,f4, c=\"orange\")\n",
    "# plt.axvline(0.25, c=\"black\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee45364d7d4d32ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# alpha = -.5, 2, 1, -1\n",
    "# alpha = -1, 3, 1, 0.5, -0.25\n",
    "# alpha = -.5, 2, .5, 0.25, -0.25\n",
    "# alpha = -.5, 2, 0.5, -0.25, -0.25\n",
    "# alpha = -2, 3, 1, -.5\n",
    "# alpha = -1, 3, 2, -.25, 5, 5 \n",
    "# alpha = -1, 3, -.25, 2.5, 2 \n",
    "\n",
    "def f_zeigen(s, param):\n",
    "    # Conditions\n",
    "    cond1 = s <= 1.3\n",
    "    # cond1 = s < .8\n",
    "    # cond2 = (s >= .6) & (s < 1)\n",
    "    # cond3 = s >= 1\n",
    "    # Functions\n",
    "    # f1 = 0\n",
    "    # f1 = 4 * param * s\n",
    "    f2 = np.maximum(6 * param * (s - .8) ,0)\n",
    "    f3 = np.maximum(15*param*(1.3-0.6) - 3*param*(s-1.3), 0)\n",
    "    # f2 = 4 * param * (0.9 - 0.3)\n",
    "    # f3 = 10 * param * (1 - .8)\n",
    "    # Using jnp.where to implement piecewise function\n",
    "    result = jnp.where(cond1, f2, f3)\n",
    "    # result = jnp.where(cond1, f1, jnp.where(cond2, f2, f3))\n",
    "    return result\n",
    "\n",
    "# alpha = -1, 3, -.25, 3, 2.5 \n",
    "# alpha = -1, 3, -.25, 0, 2.5 \n",
    "alpha = -1, 3, -.25, 1, 2.5 \n",
    "\n",
    "def gen_y(Z,x, Zeigen, alpha, lin=True):\n",
    "    epsi = rng.normal(loc=0,scale=1,size=n)\n",
    "    dflin = np.transpose(np.array([[1]*n, Z, x, Z*x]))\n",
    "    if lin:\n",
    "        mean_y = np.dot(np.column_stack((dflin,Zeigen)), alpha)\n",
    "        # mean_y += df_lin[:,1]*df_lin[:,2]\n",
    "    else:\n",
    "        # mean_lin = np.dot(dflin[:,0:3], alpha[0:3]) \n",
    "        # mean_nonlin = 0*(Zeigen < 0.2) + (alpha[3]*3)*(Zeigen-0.2)*(Zeigen>=0.2)        \n",
    "        # mean_nonlin = 0*(Zeigen < 0.2)  - alpha[3]*(np.log(1-(Zeigen-0.2)/1.1))*(Zeigen>=0.2)        \n",
    "        # mean_nonlin = alpha[3]*1.5 / (1+np.exp(-15*(Zeigen-0.4)))        \n",
    "        # mean_nonlin = alpha[3]*(np.sin(10 * Zeigen) + np.log(Zeigen + 1))\n",
    "        # mean_nonlin = 1.5*alpha[4]*(np.sin(2.5 * (Zeigen-.25)) + np.log(Zeigen + 1))        \n",
    "        # mean_nonlin = 1.2*alpha[4]*(np.sin(4 * (Zeigen - np.pi)) + np.log(Zeigen + 1))\n",
    "        # mean_nonlin = np.exp(alpha[4]*Zeigen)/40    \n",
    "        # mean_nonlin =  -3*alpha[4]*np.power(Zeigen-1,2) + 3*alpha[4]*Zeigen \n",
    "        # mean_nonlin = f_zeigen(Zeigen, alpha[4])        \n",
    "        # mean_nonlin = np.maximum(-2*alpha[4]*np.power(Zeigen-1.2,2) + 2*alpha[4]*Zeigen, 0)        \n",
    "        # mean_nonlin = np.maximum(-2*alpha[4]*np.power(Zeigen-1.5,2) + 3*alpha[4]*np.log(Zeigen+1), 0)        \n",
    "        mean_nonlin = np.maximum(0, np.minimum(4*alpha[4]*(Zeigen-.4), 4*alpha[4]*0.3))\n",
    "        mean_nonlin += 4*Z*Zeigen\n",
    "        # mean_nonlin += alpha[4]*Z*Zeigen\n",
    "        # mean_nonlin = np.maximum(0, 2*alpha[4]*(Zeigen-1.5))\n",
    "        # mean_nonlin = 2*alpha[4] / (1+np.exp(-5*Zeigen + 7.5))  \n",
    "        # mean_nonlin = alpha[3]*(np.sin(15 * Zeigen) + 2*np.log(Zeigen + 1))        \n",
    "        # mean_nonlin = alpha[3]*(np.sin(4*np.pi * Zeigen) + np.log(Zeigen + 1))       \n",
    "        # xz_interaction_sq = np.power(Z*x,2)\n",
    "        # mean_y = mean_lin + mean_nonlin + 0.25*xz_interaction_sq \n",
    "        mean_lin = np.dot(dflin, alpha[0:4]) \n",
    "        mean_y = mean_lin + mean_nonlin + 0.25*np.power(Z*x,2)\n",
    "        # mean_y = mean_lin + mean_nonlin\n",
    "\n",
    "    Y = mean_y + epsi\n",
    "    return Y, epsi\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbb9c575f0152175",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# df_array = np.transpose(np.array([[1]*n,Z,exposures,x,eigen_center,betweeness]))\n",
    "# df_array = np.transpose(np.array([[1]*n, Z, x, Zeigen,exposures]))\n",
    "\n",
    "# df_array = np.transpose(np.array([[1]*n, Z, Zeigen]))\n",
    "df_array = np.transpose(np.array([[1]*n, Z, x, Z*x, Zeigen]))\n",
    "# df_array = np.transpose(np.array([[1]*n, Z, x,x2, Zeigen]))\n",
    "\n",
    "df_lin = np.transpose(np.array([[1]*n, Z, x, x2]))\n",
    "# mean_y = np.dot(df_array,alpha)\n",
    "# # Y = alpha[0] + alpha[1]*Z + alpha[2]*exposures + alpha[3]*x + alpha[4] + rng.normal(loc=0,scale=1,size=n)\n",
    "# epsilon = rng.normal(loc=0,scale=1,size=n)\n",
    "# Y = mean_y + epsilon\n",
    "\n",
    "# Y, epsilon = gen_y(df_lin, Zeigen, alpha, lin=True)\n",
    "# Y, epsilon = gen_y(df_lin, Zeigen, alpha, lin=False)\n",
    "# Y, epsilon = gen_y(df_lin, Zeigen, alpha, lin=False)\n",
    "Y, epsilon = gen_y(Z, x, Zeigen, alpha, lin=False)\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.hist(Y)\n"
   ],
   "id": "862926e231d08a6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "# plt.scatter(exposures,betweeness)\n",
    "# plt.scatter(exposures,eigen_center)\n",
    "\n",
    "# print(np.corrcoef(eigen_center,Zeigen))\n",
    "print(np.corrcoef(Zeigen,Y))\n",
    "# plt.scatter(eigen_center,Zeigen)\n",
    "# plt.scatter(Zeigen,Y,c=Z)\n",
    "# plt.scatter(Zeigen,Y-epsilon,c=Z)\n",
    "#create basic scatterplot\n",
    "# plt.plot(Zeigen, Y, 'o')\n",
    "# plt.plot(Zeigen, Y, 'o')\n",
    "plt.scatter(Zeigen, Y, c= Z)\n",
    "#obtain m (slope) and b(intercept) of linear regression line\n",
    "m, b = np.polyfit(Zeigen, Y, 1)\n",
    "#add linear regression line to scatterplot \n",
    "plt.plot(Zeigen, m*Zeigen+b)\n",
    "\n",
    "# plt.scatter(exposures,Zeigen)\n",
    "# plt.scatter(exposures/degrees,Zeigen)\n",
    "# plt.scatter(betweeness,eigen_center)\n",
    "# plt.hist(Zeigen)\n",
    "# plt.hist(exposures/degrees)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9013f4ab2398044e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# DG = nx.from_numpy_array(mat)\n",
    "# nx.set_node_attributes(DG, Z, \"Treatments\") \n",
    "# node_sizes = [DG.degree(node) for node in DG.nodes()]\n",
    "# # nx.draw(DG,node_color = Z, node_size=node_sizes, width=0.05)\n",
    "# # nx.draw(DG, nx.kamada_kawai_layout(DG), node_color = eigen_center, node_size=node_sizes, width=0.05)\n",
    "# nx.draw(DG, nx.kamada_kawai_layout(DG), node_color = Zeigen, node_size=node_sizes, width=0.05)\n",
    "# # nx.draw(DG, nx.shell_layout(DG), node_color = Zeigen, node_size=node_sizes, width=0.05)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e14827155b8509da",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Create noisy measurement of `mat`\n",
    "# gamma1 = 0.25  # Remove existing edge with prob `alph`\n",
    "# gamma1 = 0.25  # Remove existing edge with prob `alph`\n",
    "# gamma2 = 0.01\n",
    "# \n",
    "# # gamma0 = gamma1/5 # add non-existing edge with prob `bet` \n",
    "# # gamma0 = 0.05 # add non-existing edge with prob `bet` \n",
    "# gamma0 = 0.025 # add non-existing edge with prob `bet` \n",
    "# \n",
    "# obs_mat = np.zeros((n,n)) # create nXn matrix of zeros\n",
    "# # obs_mat[np.tril_indices(n=n,k=-1)] = tril_vals # init as true network\n",
    "# obs_mat[np.triu_indices(n=n,k=1)] = triu_vals # init as true network\n",
    "# for i in range(0,n): # add noise\n",
    "#     for j in range(i+1,n):\n",
    "#         if mat[i,j] == 1:\n",
    "#             # obs_mat[i,j] = rng.binomial(n=1,p=1-gamma1,size=1)[0] # retain existing edge w.p. `1-gamma1`\n",
    "#             obs_mat[i,j] = rng.binomial(n=1,p=1-expit(2 - 3*x_diff[]),size=1)[0] # retain existing edge w.p. `1-gamma1`\n",
    "#         else:\n",
    "#             obs_mat[i,j] = rng.binomial(n=1,p=gamma0,size=1)[0] # add non-existing edge w.p. `gamma0` \n",
    "#             \n",
    "# obs_mat = obs_mat + obs_mat.T\n",
    "# triu_obs = obs_mat[np.triu_indices(n=n,k=1)]\n",
    "# # tril_obs = obs_mat[np.tril_indices(n=n,k=-1)]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9957375b934c1830",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(logit(1-0.25), expit(1.1), expit(-1), expit(-3.5), expit(-1))",
   "id": "1fc04f516acd1496",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "f = jnp.array([1,2,3,4,5])\n",
    "# torch.from_numpy(np.array(f))"
   ],
   "id": "879aa972bc516e5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# prob_nois = triu_vals*(1-0.3) + (1-triu_vals)*expit(-1 - .25*x_diff)\n",
    "# logit_nois = triu_vals*logit(1-0.25) + (1-triu_vals)*(-1.5 - .1*x_diff + .5*x2_equal)\n",
    "# logit_nois = triu_vals*logit(1-0.25) + (1-triu_vals)*(-2.5 - 1*x2_equal)\n",
    "# logit_nois = triu_vals*logit(1-0.2) + (1-triu_vals)*(-2 + 1*x2_equal + 0.5*x2_or)\n",
    "# logit_nois = triu_vals*logit(1-0.2) + (1-triu_vals)*(-2 + 1*x2_equal + 0.5*x2_or)\n",
    "# logit_nois = triu_vals*logit(0.75) + (1-triu_vals)*(-2 + 1.5*x2_or)\n",
    "# logit_nois = triu_vals*logit(0.75) + (1-triu_vals)*(-2 - .15*x_diff)\n",
    "\n",
    "# logit_nois = triu_vals*1.1 + (1-triu_vals)*(-1*x_diff)\n",
    "# logit_nois = triu_vals*1.1 + (1-triu_vals)*(0.2 - 1*x_diff - .1*U_diff_norm_val)\n",
    "logit_nois = triu_vals*1.1 + (1-triu_vals)*(0.2 - 1*x_diff)\n",
    "\n",
    "# logit_nois = triu_vals*1.1 + (1-triu_vals)*(-2)\n",
    "# logit_nois = triu_vals*logit(0.75) + (1-triu_vals)*(-4  - .5*x_diff)\n",
    "\n",
    "# prob_nois = triu_vals*(1-0.25) + (1-triu_vals)*0.05\n",
    "obs_mat = np.zeros((n,n))\n",
    "# idx_lt = np.tril_indices(n=n,k=-1)\n",
    "idx_ut = np.triu_indices(n=n,k=1)\n",
    "# edges = rng.binomial(n=1,p=0.5,size=tril_dim)\n",
    "edges_noisy = rng.binomial(n=1,p=expit(logit_nois),size=tril_dim)\n",
    "\n",
    "obs_mat[idx_ut] = edges_noisy\n",
    "# mat[idx_lt] = edges\n",
    "obs_mat = obs_mat + obs_mat.T\n",
    "triu_obs = obs_mat[np.triu_indices(n,k=1)]\n",
    "\n",
    "print(np.array_equal(triu_obs, edges_noisy))\n"
   ],
   "id": "7fd2558a6cc65c87",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot noisy observed network \n",
    "# DG_noise = nx.from_numpy_array(obs_mat)\n",
    "# nx.draw(DG_noise, with_labels=True,  node_color = Z)\n",
    "# plt.show()\n",
    "plt.figure(figsize=(4,3))\n",
    "print(np.mean(np.sum(obs_mat,1)))\n",
    "plt.hist(np.sum(obs_mat,1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2af4da8a120d34db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.scatter(np.sum(obs_mat,1), np.sum(mat,1))"
   ],
   "id": "59464224c5c24205",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "trils_pd = pd.DataFrame({'true' : triu_vals, 'obs' : triu_obs})\n",
    "print(pd.crosstab(index=trils_pd['true'], columns=trils_pd['obs']))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edb0b09737e7ec75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " \n",
    "# obs_exposures = np.dot(obs_mat,Z)/np.sum(obs_mat,1) > 0.5\n",
    "# # obs_exposures = np.dot(obs_mat,Z)\n",
    "# obs_exposures = np.dot(obs_mat,Z)\n",
    "\n",
    "\n",
    "# obs_net = nx.from_numpy_array(obs_mat)\n",
    "# obs_deg = np.array([degree for _, degree in obs_net.degree()])\n",
    "# obs_betweeness = np.array(list(nx.betweenness_centrality(obs_net).values()))\n",
    "# obs_triangles = np.array(list(nx.triangles(obs_net).values()))\n",
    "# obs_eigen_center = np.array(list(nx.eigenvector_centrality_numpy(obs_net).values()))\n",
    "obs_degrees = np.sum(obs_mat,1)\n",
    "obs_eigen_cent = eigen_centrality(obs_mat)\n",
    "obs_exposures = np.dot(obs_mat,Z) / obs_degrees\n",
    "# obs_Zeigen = np.dot(obs_mat, Z*obs_eigen_cent)\n",
    "obs_Zeigen = zeigen_value(Z, obs_eigen_cent, obs_mat)\n",
    "\n",
    "# obs_df = np.transpose(np.array([[1]*n, Z, obs_exposures,x, obs_eigen_center, obs_betweeness]))\n",
    "# obs_df = np.transpose(np.array([[1]*n, Z, x, obs_Zeigen, obs_exposures]))\n",
    "\n",
    "# obs_df = np.transpose(np.array([[1]*n, Z, x, x2, obs_Zeigen]))\n",
    "obs_df = np.transpose(np.array([[1]*n, Z, x, Z*x, obs_Zeigen]))\n",
    "# obs_df = np.transpose(np.array([[1]*n, Z, obs_Zeigen]))\n",
    "\n",
    "print(pd.DataFrame(obs_Zeigen).describe())\n",
    "\n",
    "print(\"Corr obs~true exposures: \", np.corrcoef(exposures, obs_exposures)[1,0])\n",
    "print(\"Corr obs~true Zeigen: \", np.corrcoef(Zeigen, obs_Zeigen)[1,0])\n",
    "print(\"Corr true_zeigen~Y : \", np.corrcoef(Zeigen, Y)[1,0])\n",
    "print(\"Corr obs_zeigen~Y : \", np.corrcoef(obs_Zeigen, Y)[1,0])\n",
    "\n",
    "# obs_exposures = np.dot(obs_mat,Z)/np.sum(obs_mat,1)\n",
    "# expos_pd = pd.DataFrame({'true' : exposures, 'obs' : obs_exposures})\n",
    "# pd.crosstab(index=expos_pd['true'], columns=expos_pd['obs'])\n",
    "plt.figure(figsize=(4,3))\n",
    "# plt.scatter(obs_exposures,exposures, c=Z)\n",
    "# plt.scatter(obs_Zeigen,Zeigen, c=x2)\n",
    "\n",
    "\n",
    "plt.plot(obs_Zeigen, Y, 'o')\n",
    "#obtain m (slope) and b(intercept) of linear regression line\n",
    "m, b = np.polyfit(obs_Zeigen, Y, 1)\n",
    "#add linear regression line to scatterplot \n",
    "plt.plot(obs_Zeigen, m*obs_Zeigen+b)\n",
    "\n",
    "# plt.scatter(obs_Zeigen,Y, c=x2)"
   ],
   "id": "930b23ed1fd1ac16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "data = {\n",
    "    'obs_Zigen': obs_Zeigen,\n",
    "    'Zeigen': Zeigen,\n",
    "    'x2': x2,\n",
    "    'x': x\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "formula = 'Zeigen ~ x2 + x'\n",
    "# Fit the model\n",
    "model = smf.ols(formula=formula, data=df).fit()\n",
    "# Print the summary of the regression\n",
    "print(model.summary())"
   ],
   "id": "aa5f0c18c590f345",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(np.corrcoef(obs_Zeigen, x2))\n",
    "print(np.corrcoef(Zeigen, x2))\n",
    "\n",
    "print(np.corrcoef(obs_Zeigen, x))\n",
    "print(np.corrcoef(Zeigen, x))\n"
   ],
   "id": "76bf8f7f6e3486ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e17579ab8fecd8b8"
  },
  {
   "cell_type": "code",
   "source": [
    "# NumPyro model\n",
    "# def triu_to_mat(triu, n):\n",
    "#     adj_mat = jnp.zeros((n,n))\n",
    "#     idx_utri = np.triu_indices(n=n,k=1)\n",
    "#     adj_mat = adj_mat.at[idx_utri].set(triu) \n",
    "#     return adj_mat + adj_mat.T\n",
    "\n",
    "NN = n\n",
    "\n",
    "@jit\n",
    "def Triu_to_mat(triu_v):\n",
    "    adj_mat = jnp.zeros((NN,NN))\n",
    "    # idx_utri = np.triu_indices(n=NN,k=1)\n",
    "    adj_mat = adj_mat.at[np.triu_indices(n=NN,k=1)].set(triu_v) \n",
    "    return adj_mat + adj_mat.T\n",
    "\n",
    "@config_enumerate\n",
    "def network_model(X, X2, TriU):\n",
    "    # Network model\n",
    "    # beta_0 = numpyro.sample(\"beta0\",dist.Normal(0,10))\n",
    "    # beta_1 = numpyro.sample(\"beta1\",dist.Normal(0,10))\n",
    "    with numpyro.plate(\"beta_i\",2):\n",
    "        beta = numpyro.sample(\"beta\", dist.Normal(0,5))\n",
    "    # mu_net = beta[0] + beta[1]*X + beta[2]*X2\n",
    "    mu_net = beta[0] + beta[1]*X2\n",
    "    # triu_n = int(n*(n-1)/2)\n",
    "    \n",
    "    # gamma0 = numpyro.sample(\"gamma0\", dist.Uniform(low=0,high=0.5))\n",
    "    # gamma1 = numpyro.sample(\"gamma1\", dist.Uniform(low=0,high=0.5))\n",
    "    with numpyro.plate(\"gamma_i\",2):\n",
    "        gamma = numpyro.sample(\"gamma\", dist.Normal(0,5))\n",
    "    \n",
    "    # gamma0 = numpyro.sample(\"gamma0\", dist.Beta(2,5))\n",
    "    # gamma1 = numpyro.sample(\"gamma1\", dist.Beta(2,5))\n",
    "    \n",
    "    with numpyro.plate(\"A* and A\", TriU.shape[0]):\n",
    "        triu_star = numpyro.sample(\"triu_star\",dist.Bernoulli(logits=mu_net),\n",
    "                                   infer={\"enumerate\": \"parallel\"})\n",
    "        # prob_misspec = jnp.where(triu_star==1, 1-gamma1, gamma0)\n",
    "        # prob_misspec = triu_star*(1-gamma1) + (1-triu_star)*gamma0\n",
    "        # prob_misspec = triu_star*(1-gamma1) + (1-triu_star)*expit(gamma[0] + gamma[1]*X + gamma[2]*X2)\n",
    "        # logit_misspec = triu_star*gamma[0] + (1-triu_star)*(gamma[1] + gamma[2]*X + gamma[3]*X2)\n",
    "        # logit_misspec = triu_star*gamma[0] + (1-triu_star)*(gamma[1] + gamma[2]*X2)\n",
    "        # logit_misspec = jnp.where(triu_star, gamma[0], gamma[1]*X)\n",
    "        logit_misspec = triu_star*gamma[0] + (1-triu_star)*gamma[1]*X\n",
    "        # logit_misspec = triu_star*gamma[0] + (1-triu_star)*gamma[1]\n",
    "        # numpyro.sample(\"obs_triu\", dist.Bernoulli(probs = prob_misspec), obs = TriU)\n",
    "        numpyro.sample(\"obs_triu\", dist.Bernoulli(logits = logit_misspec), obs = TriU)\n",
    "        \n",
    "    # def triu_fn(carry, inp):\n",
    "    #     # print(\"Starting tril_fn carry no.\", carry)\n",
    "    #     # mu_n = inp\n",
    "    #     mu_n, triu_val = inp\n",
    "    #     # logp of A* \n",
    "    #     # with numpyro.plate(\"Edges A*\", triu_n):\n",
    "    #     triu_star = numpyro.sample(\"triu_star\",dist.Bernoulli(logits=mu_n),\n",
    "    #                                infer={\"enumerate\": \"parallel\"})\n",
    "    #     # print(\"Triu_Star shape (within) is\", triu_star.shape)\n",
    "    #     # # logp of A|A*\n",
    "    #     # # with numpyro.plate(\"A|A*\", 1):\n",
    "    #     prob_misspec = jnp.where(triu_star==1, 1-gamma1, gamma0)\n",
    "    #     # print(\"prob_misspec shape is: \", prob_misspec.shape)\n",
    "    #     # obs_triu_vals = numpyro.sample(\"obs_triu\", dist.Bernoulli(probs = prob_misspec), obs = TriU)\n",
    "    #     return carry+1, triu_star  \n",
    "    # \n",
    "    # sc_input = jnp.array((mu_net, TriU)).T\n",
    "    # _, triu_var = scan(triu_fn, jnp.array(0), sc_input)\n",
    "    # print(\"triu_var shape is: \", triu_var.shape)\n",
    "    # scan(triu_fn, jnp.array(0), sc_input)    \n",
    "    # \n",
    "\n",
    "@config_enumerate\n",
    "# def noisy_networks_model(x_eq: jnp.ndarray, triu_v: jnp.ndarray, N_edges: int, N: int, K = 2):\n",
    "def noisy_networks_model(X_diff: jnp.ndarray, X2_or: jnp.ndarray, triu_v: jnp.ndarray, N = n, K = 2):\n",
    "    # True network priors\n",
    "    # Latent variable for each unit from bi-normal distribution\n",
    "    sigma_sq = numpyro.sample(\"sigma_sq\", dist.InverseGamma(0.1, 1.0))\n",
    "    with numpyro.plate(\"nu_i\", N):\n",
    "        nu_standard = numpyro.sample(\"nu_standard\", dist.MultivariateNormal(loc=jnp.zeros(K), covariance_matrix=jnp.eye(K)))\n",
    "\n",
    "    nu = numpyro.deterministic(\"nu\", nu_standard * jnp.sqrt(sigma_sq))\n",
    "    nu_diff_norm_val = latent_to_norm_of_diff(nu)\n",
    "\n",
    "    # Save logits for A*\n",
    "    with numpyro.plate(\"theta_dim\", 2):\n",
    "        theta = numpyro.sample(\"theta\", dist.Normal(0, 5))\n",
    "    mu_net = theta[0] + X2_or*theta[1] - nu_diff_norm_val\n",
    "\n",
    "    with numpyro.plate(\"gamma_dim\", 4):\n",
    "        gamma = numpyro.sample(\"gamma\", dist.Normal(0, 5))\n",
    "   \n",
    "    with numpyro.plate(\"A* and A\", triu_v.shape[0]):\n",
    "        triu_star = numpyro.sample(\"triu_star\", dist.Bernoulli(logits=mu_net)\n",
    "                                   # , infer={\"enumerate\": \"sequential\"})\n",
    "                                   , infer={\"enumerate\": \"parallel\"})\n",
    "        logit_misspec = jnp.where(triu_star,\n",
    "                                  gamma[0],\n",
    "                                  gamma[1] + gamma[2]*X_diff + gamma[3]*nu_diff_norm_val)\n",
    "        numpyro.sample(\"obs_triu\", dist.Bernoulli(logits=logit_misspec), obs=triu_v)\n",
    "            \n",
    "# def outcome_model(Y,Z,X,A,n):\n",
    "\n",
    "def outcome_model(X, Y=None):\n",
    "    with numpyro.plate(\"Lin coef.\",X.shape[1]):\n",
    "        alpha = numpyro.sample(\"alpha\",dist.Normal(0,5))\n",
    "    sig = numpyro.sample(\"sig\",dist.HalfNormal(scale=2))\n",
    "    # sig = numpyro.sample(\"sig\",dist.Exponential(0.5))\n",
    "    \n",
    "    # expos = jnp.dot(A,Z)\n",
    "    # expos = (jnp.dot(A,Z) / jnp.sum(A,1)) > 0.5\n",
    "    # sum_expos = jnp.dot(A,Z)\n",
    "    # deg = jnp.sum(A,1)\n",
    "    # expos = jnp.where(jnp.equal(deg,0), 0, sum_expos/deg)\n",
    "    # mu_y = alpha[0] + alpha[1]*Z + alpha[2]*expos + alpha[3]*X \n",
    "    # mu_y = numpyro.deterministic(\n",
    "    #             \"mu_star\",\n",
    "    #              jnp.dot(X, alpha),\n",
    "    # )\n",
    "    # mu_y = jnp.dot(X, alpha)\n",
    "    mu_y = numpyro.deterministic(\"mu\", jnp.dot(X, alpha))\n",
    "    # mu_y = alpha[0] + alpha[1]*Z + alpha[2]*expos + alpha[3]*deg  + alpha[4]*X\n",
    "    # mu_y = alpha[0] + alpha[1]*Z + alpha[2]*expos + alpha[3]*expos*Z + alpha[4]*deg\n",
    "    # mu_y = alpha[0] + alpha[1]*Z + alpha[2]*expos + alpha[3]*expos*Z + alpha[4]*deg\n",
    "    with numpyro.plate(\"obs\",X.shape[0]):\n",
    "        numpyro.sample(\"Y\", dist.Normal(loc=mu_y,scale=sig), obs=Y)\n",
    "        \n",
    "        \n",
    "def HSGP_model(Xlin, Xgp, ell, m, Y=None, non_centered=True):\n",
    "    # --- Priors ---\n",
    "    magn = numpyro.sample(\"magn\", dist.HalfNormal(2))\n",
    "    length = numpyro.sample(\"length\", dist.HalfNormal(5))\n",
    "    sig = numpyro.sample(\"sig\", dist.HalfNormal(2))\n",
    "    # --- Parametrization ---\n",
    "    f = numpyro.deterministic(\n",
    "                  \"f_star\",\n",
    "                  hsgp_squared_exponential(\n",
    "                      x=Xgp, alpha=magn, length=length,\n",
    "                      ell=ell, m=m, non_centered=non_centered\n",
    "              ),\n",
    "    )\n",
    "    with numpyro.plate(\"Lin coef.\",Xlin.shape[1]):\n",
    "        alpha = numpyro.sample(\"alpha\",dist.Normal(0,5))\n",
    "    # mu = numpyro.deterministic(\"mu\", jnp.dot(Xlin, alpha) + f)\n",
    "    mu = numpyro.deterministic(\n",
    "            \"mu_star\",\n",
    "             jnp.dot(Xlin, alpha) + f,\n",
    "    )\n",
    "    # mu = beta0 + beta1*x_lin + f\n",
    "    # --- Likelihood ---\n",
    "    with numpyro.plate(\"obs\", Xlin.shape[0]):\n",
    "        # numpyro.sample(\"likelihood\", dist.Normal(loc=mu, scale=noise), obs=y)\n",
    "        # numpyro.sample(\"Y\", dist.Normal(loc=f, scale=noise), obs=y)\n",
    "        # numpyro.sample(\"Y\", dist.Normal(loc=mu, scale=sig), obs=Y, sample_shape=(Xgp.shape[0],))\n",
    "        numpyro.sample(\"Y\", dist.Normal(loc=mu, scale=sig), obs=Y)\n",
    "        \n",
    "        \n",
    "def HSGP_model_full(df1, df2, ell1, ell2, m, y=None):\n",
    "        amplitude1 = numpyro.sample(\"amplitude1\", dist.HalfNormal(2))\n",
    "        length1 = numpyro.sample(\"lengthscale1\", dist.HalfNormal(5))\n",
    "        amplitude2 = numpyro.sample(\"amplitude2\", dist.HalfNormal(2))\n",
    "        length2 = numpyro.sample(\"lengthscale2\", dist.HalfNormal(5))\n",
    "        noise = numpyro.sample(\"noise\", dist.HalfNormal(2))\n",
    "\n",
    "        f1 = numpyro.deterministic(\n",
    "            \"f1_star\",\n",
    "            hsgp_squared_exponential(\n",
    "                df1, alpha=amplitude1, length=length1, ell=ell1, m=m, i = \"1\"\n",
    "            ),\n",
    "        )\n",
    "        f2 = numpyro.deterministic(\n",
    "            \"f2_star\",\n",
    "            hsgp_squared_exponential(\n",
    "                df2, alpha=amplitude2, length=length2, ell=ell2, m=m, i = \"2\"\n",
    "            ),\n",
    "        )\n",
    "        # intercept = numpyro.sample(\"intercept\",dist.Normal(0,5))\n",
    "        # f = numpyro.deterministic(\"f_star\", intercept + f1 + f2)\n",
    "        f = numpyro.deterministic(\"f_star\", f1 + f2)\n",
    "        # site = \"y\" if y is not None else \"y_test\"\n",
    "        # numpyro.sample(site, dist.Normal(f, noise), obs=y)\n",
    "        numpyro.sample(\"Y\", dist.Normal(f, noise), obs=y)\n",
    "        \n",
    "        \n",
    "@jax.tree_util.register_pytree_node_class\n",
    "class HSGPModel:\n",
    "    def __init__(self, m: int, L1: float | list[float | int], L2: float | list[float | int]) -> None:\n",
    "        self.m = m\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "\n",
    "    def model(\n",
    "        self,\n",
    "        df1: jax.Array,\n",
    "        df2: jax.Array,\n",
    "        y: jax.Array | None = None,\n",
    "    ):\n",
    "        amplitude1 = numpyro.sample(\"amplitude1\", dist.HalfNormal(2))\n",
    "        length1 = numpyro.sample(\"lengthscale1\", dist.HalfNormal(5))\n",
    "        amplitude2 = numpyro.sample(\"amplitude2\", dist.HalfNormal(2))\n",
    "        length2 = numpyro.sample(\"lengthscale2\", dist.HalfNormal(5))\n",
    "        noise = numpyro.sample(\"noise\", dist.HalfNormal(2))\n",
    "\n",
    "        # f = numpyro.deterministic(\n",
    "        #     \"f_star\",\n",
    "        #     hsgp_squared_exponential(\n",
    "        #         df1, alpha=amplitude1, length=length1, ell=self.L1, m=self.m\n",
    "        #     ),\n",
    "        # )\n",
    "        # f1 = hsgp_squared_exponential(\n",
    "        #         df1, alpha=amplitude1, length=length1, ell=self.L1, m=self.m, i=str(1)\n",
    "        #     )\n",
    "        # \n",
    "        # f2 = hsgp_squared_exponential(\n",
    "        #         df2, alpha=amplitude2, length=length2, ell=self.L2, m=self.m, i=str(2)\n",
    "        #     )\n",
    "        \n",
    "        f1 = numpyro.deterministic(\n",
    "            \"f1_star\",\n",
    "            hsgp_squared_exponential(\n",
    "                df1, alpha=amplitude1, length=length1, ell=self.L1, m=self.m, i=str(1)\n",
    "            ),\n",
    "        )        \n",
    "        f2 = numpyro.deterministic(\n",
    "            \"f2_star\",\n",
    "            hsgp_squared_exponential(\n",
    "                df2, alpha=amplitude2, length=length2, ell=self.L2, m=self.m, i=str(2)\n",
    "            ),\n",
    "        )\n",
    "        # intercept = numpyro.sample(\"intercept\",dist.Normal(0,5))\n",
    "        f = numpyro.deterministic(\"f_star\", f1 + f2)\n",
    "        # f = numpyro.deterministic(\"f_star\", intercept + f1 + f2)\n",
    "        site = \"y\" if y is not None else \"y_test\"\n",
    "        numpyro.sample(site, dist.Normal(f, noise), obs=y)\n",
    "        # numpyro.sample(site, dist.Normal(f1 + f2 + intercept, noise), obs=y)\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        children = ()  # arrays / dynamic values\n",
    "        aux_data = (\n",
    "            self.L1,\n",
    "            self.L2,\n",
    "            self.m,\n",
    "        )  # static values\n",
    "        return (children, aux_data)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*children, **aux_data)\n",
    "        \n",
    "        \n",
    "# numpyro.render_model(model, model_args=(Y,Z,x_diff,triu_obs,n), render_distributions=True)\n",
    "numpyro.render_model(network_model, model_args=(x_diff, x2_or ,triu_obs), render_distributions=True)\n",
    "# numpyro.render_model(model, model_args=(Y,Z,x_diff,tril_vals))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b16d10dbe16c0b09",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# numpyro.render_model(outcome_model, model_args=(Y,Z,x,mat,n), render_distributions=True)\n",
    "\n",
    "df_array = jnp.array(df_array)\n",
    "Y = jnp.array(Y)\n",
    "\n",
    "numpyro.render_model(outcome_model, model_args=(df_array,Y), render_distributions=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "389e6f5d9dd220f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "c = 3\n",
    "# c = 3.5\n",
    "\n",
    "# Xgp = jnp.array(df_array[:,2:])\n",
    "Xgp = jnp.array(df_array[:,3:])\n",
    "# Xgp = jnp.array(df_array[:,4:])\n",
    "# Xlin = jnp.array(df_array[:,0:4])\n",
    "Xlin = jnp.array(df_array[:,0:3])\n",
    "# Xlin = jnp.array(df_array[:,0:2])\n",
    "\n",
    "# ell = 3.5*np.max(np.abs(df_array[:,3:]))\n",
    "ell = jnp.array(c*np.max(np.abs(Xgp))).reshape((1,1))\n",
    "# m = 20\n",
    "# m = 20\n",
    "m = 15\n",
    "# m = 50\n",
    "\n",
    "\n",
    "\n",
    "# numpyro.render_model(HSGP_model, model_args=(Xgp, Xlin, ell, m, Y), render_distributions=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1234e5bbb77d6e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# A* and A|A* models\n",
    "\n",
    "rng_key = random.PRNGKey(1)\n",
    "rng_key, rng_key_ = random.split(rng_key)\n",
    "\n",
    "start = time.time()\n",
    "# kernel = DiscreteHMCGibbs(NUTS(model))\n",
    "# kernel = MixedHMC(HMC(model, trajectory_length=1.2))  \n",
    "# kernel = NUTS(model, dense_mass=True)\n",
    "# kernel_network = NUTS(network_model, dense_mass=[(\"gamma\",\"beta\")], max_tree_depth=15)\n",
    "# kernel_network = NUTS(network_model)\n",
    "kernel_network = NUTS(network_model, init_strategy=numpyro.infer.init_to_median(num_samples=30))\n",
    "# kernel_network = NUTS(network_model, \n",
    "                      # init_strategy=numpyro.infer.init_to_value(values={\"beta\": jnp.array([0.0,0.0,0.0]),\"gamma\" : jnp.array([0.0, 0.0])}))\n",
    "mcmc_network = MCMC(kernel_network, num_warmup=1000, num_samples=1000,num_chains=4)\n",
    "# mcmc_network = MCMC(kernel_network, num_warmup=2000, num_samples=4000,num_chains=4)\n",
    "# mcmc.run(rng_key_,Y=Y,Z=Z,expos=obs_exposures,X=x_diff,TriL=tril_obs,n=100)\n",
    "# mcmc.run(rng_key_,Y=Y,Z=Z,X=x_diff,TriU=triu_obs,n=n)\n",
    "mcmc_network.run(rng_key_,X=x_diff,X2  = x2_or, TriU=triu_obs)\n",
    "mcmc_network.print_summary()\n",
    "samples_network = mcmc_network.get_samples()\n",
    "print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818342fc9090d3fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A* and A|A* models + latent U\n",
    "\n",
    "rng_key = random.PRNGKey(1)\n",
    "rng_key, rng_key_ = random.split(rng_key)\n",
    "\n",
    "start = time.time()\n",
    "# kernel = DiscreteHMCGibbs(NUTS(model))\n",
    "# kernel = MixedHMC(HMC(model, trajectory_length=1.2))  \n",
    "# kernel = NUTS(model, dense_mass=True)\n",
    "# kernel_network = NUTS(network_model, dense_mass=[(\"gamma\",\"beta\")], max_tree_depth=15)\n",
    "# kernel_network = NUTS(network_model)\n",
    "kernel_network = NUTS(noisy_networks_model, init_strategy=numpyro.infer.init_to_median(num_samples=30), target_accept_prob=0.95)\n",
    "# kernel_network = NUTS(network_model, \n",
    "                      # init_strategy=numpyro.infer.init_to_value(values={\"beta\": jnp.array([0.0,0.0,0.0]),\"gamma\" : jnp.array([0.0, 0.0])}))\n",
    "mcmc_network = MCMC(kernel_network, num_warmup=200, num_samples=500,num_chains=4)\n",
    "# mcmc_network = MCMC(kernel_network, num_warmup=2000, num_samples=4000,num_chains=4)\n",
    "# mcmc.run(rng_key_,Y=Y,Z=Z,expos=obs_exposures,X=x_diff,TriL=tril_obs,n=100)\n",
    "# mcmc.run(rng_key_,Y=Y,Z=Z,X=x_diff,TriU=triu_obs,n=n)\n",
    "mcmc_network.run(rng_key_, X_diff=x_diff, X2_or = x2_or, triu_v=triu_obs)\n",
    "mcmc_network.print_summary()\n",
    "samples_network = mcmc_network.get_samples()\n",
    "print(time.time() - start)"
   ],
   "id": "478e7006e7c18ed6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "@jit\n",
    "def Astar_pred(key, post_samples, Xd, triu):\n",
    "    # if mean_post:\n",
    "    pred_func = Predictive(model=network_model, posterior_samples=post_samples, infer_discrete=True,num_samples=1)\n",
    "    # else:\n",
    "    #     pred_func = Predictive(model=network_model, posterior_samples=post_samples, infer_discrete=True)\n",
    "    return pred_func(key, X=Xd, TriU=triu)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4ad7bda20e92e6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, autoguide, config_enumerate, infer_discrete\n",
    "import torch\n",
    "from pyro import poutine\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "def cosine_similarity_torch(X):\n",
    "    # Normalize the rows\n",
    "    X_normalized = X / torch.linalg.norm(X, dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute dot products for upper triangle\n",
    "    n = X.shape[0]\n",
    "    triu_indices = torch.triu_indices(n, n, offset=1)\n",
    "    dot_products = torch.mm(X_normalized, X_normalized.T)[triu_indices[0], triu_indices[1]]\n",
    "    \n",
    "    return dot_products\n",
    "\n",
    "\n",
    "def orthogonal_transform(reference, target):\n",
    "    target = target.detach().numpy()\n",
    "    reference = reference.detach().numpy()\n",
    "    R, _ = orthogonal_procrustes(target, reference)\n",
    "    transformed = torch.tensor(target @ R, dtype=torch.float32)\n",
    "    return transformed\n",
    "\n",
    "@config_enumerate\n",
    "# @infer_discrete(first_available_dim=-2)\n",
    "def pyro_noisy_networks_model(x, x2, triu_v, N, K=2, eps = 1e-3):\n",
    "    # sigma_sq = pyro.sample(\"sigma_sq\", dist.InverseGamma(0.1, 1.0))\n",
    "    # sigma_sq = pyro.sample(\"sigma_sq\", dist.LogNormal(0., 2.))\n",
    "    # log_sigma_sq = pyro.sample(\"log_sigma_sq\", dist.Normal(0, 5))\n",
    "    # sigma_sq = torch.exp(log_sigma_sq)\n",
    "    with pyro.plate(\"Latent_dim\", N):\n",
    "        nu = pyro.sample(\"nu\", dist.MultivariateNormal(torch.zeros(K) + eps, torch.eye(K)))\n",
    "        # nu_standard = pyro.sample(\"nu_standard\", dist.MultivariateNormal(torch.zeros(K), torch.eye(K)))\n",
    "    # nu = pyro.deterministic(\"nu\", nu_standard * torch.sqrt(sigma_sq))\n",
    "    \n",
    "    # nu_cosine = pyro.deterministic(\"nu_cosine\", cosine_similarity_torch(nu))\n",
    "    \n",
    "    idx = torch.triu_indices(N, N, offset=1)\n",
    "    nu_diff = nu[idx[0]] - nu[idx[1]]\n",
    "    nu_diff_norm_val = torch.norm(nu_diff, dim=1)\n",
    "    # \n",
    "    with pyro.plate(\"theta_dim\", 2):\n",
    "        theta = pyro.sample(\"theta\", dist.Normal(0, 5))\n",
    "   \n",
    "    mu_net = theta[0] + x2*theta[1] - nu_diff_norm_val\n",
    "    # mu_net = theta[0] + x2*theta[1] + nu_cosine\n",
    "    # mu_net = theta[0] + x2*theta[1] \n",
    "    mu_net = torch.clamp(mu_net, min=-30, max=30)\n",
    "    \n",
    "    with pyro.plate(\"gamma_i\", 3):\n",
    "        gamma = pyro.sample(\"gamma\", dist.Normal(0, 5))\n",
    "   \n",
    "    with pyro.plate(\"A* and A\", x.shape[0]):\n",
    "        triu_star = pyro.sample(\"triu_star\", dist.Bernoulli(logits=mu_net),\n",
    "                                infer={\"enumerate\": \"parallel\"})\n",
    "\n",
    "        logit_misspec = torch.where(triu_star==1.0,\n",
    "                                    gamma[0],\n",
    "                                    gamma[1] + gamma[2]*x)\n",
    "                                    # gamma[1] + gamma[2]*x + gamma[3]*nu_diff_norm_val)\n",
    "\n",
    "        pyro.sample(\"obs_triu\", dist.Bernoulli(logits=logit_misspec), obs=triu_v)\n",
    "\n"
   ],
   "id": "401115f75b8fbf10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run full model with reference nu\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# guide = autoguide.AutoMultivariateNormal(poutine.block(pyro_noisy_networks_model, hide=[\"triu_star\"]),\n",
    "#                                          init_loc_fn=autoguide.init_to_median())\n",
    "\n",
    "guide = autoguide.AutoLowRankMultivariateNormal(poutine.block(pyro_noisy_networks_model, hide=[\"triu_star\"]),\n",
    "                                      # init_loc_fn=autoguide.init_to_value(values = {\"nu_standard\": nu_init, 'log_sigma_sq' : guide_init.nodes['log_sigma_sq']['value'],\n",
    "                                      #                                               'theta': guide_init.nodes['theta']['value'], 'gamma': guide_init.nodes['gamma']['value']},\n",
    "                                      #                                     fallback=autoguide.init_to_median())) \n",
    "                                        init_loc_fn = autoguide.init_to_median())\n",
    "\n",
    "# guide_init = autoguide.AutoNormal(poutine.block(pyro_noisy_networks_model, hide=[\"triu_star\"]),\n",
    "#                                       init_loc_fn=autoguide.init_to_median()) \n",
    "\n",
    "# guide = autoguide.AutoNormal(poutine.block(pyro_noisy_networks_model_wref, hide=[\"triu_star\"]),\n",
    "#                                          init_loc_fn=autoguide.init_to_value(values=map_posterior_params))\n",
    "# max_plate_nesting = 1 because there is a single plate in the model\n",
    "loss_func = pyro.infer.TraceEnum_ELBO(max_plate_nesting=1)\n",
    "# loss_func = pyro.infer.discrete.TraceEnumSample_ELBO(max_plate_nesting=1)\n",
    "\n",
    "tt_x_diff = torch.tensor(np.array(x_diff), dtype=torch.float32)\n",
    "tt_x2_or = torch.tensor(np.array(x2_or), dtype=torch.float32)\n",
    "tt_triu_obs = torch.tensor(triu_obs, dtype=torch.float32)\n",
    "\n",
    "optimzer = pyro.optim.ClippedAdam({\"lr\": 0.001})\n",
    "# optimzer = pyro.optim.Adam({\"lr\": 0.001})\n",
    "\n",
    "# svi = SVI(pyro_noisy_networks_model_wref, guide, optimzer, loss=loss_func)\n",
    "svi_full = SVI(pyro_noisy_networks_model, guide, optimzer, loss=loss_func)\n",
    "losses_full = []\n",
    "for _ in tqdm(range(10000)):\n",
    "    loss = svi_full.step(tt_x_diff, tt_x2_or, tt_triu_obs, n)\n",
    "    # loss = svi.step(tt_x_diff, tt_x2_or, tt_triu_obs, n, nu_ref)\n",
    "    losses_full.append(loss)\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(losses_full)\n",
    "plt.show()\n",
    "posterior_params_full = {k: np.array(v.data) for k, v in pyro.get_param_store().items()}"
   ],
   "id": "4c15863b2a3f982b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# guide_trace = poutine.trace(guide).get_trace(tt_x_diff, tt_x2_or, tt_triu_obs)\n",
    "# inferred_model = infer_discrete(poutine.replay(pyro_noisy_networks_model, guide_trace), first_available_dim=-2)\n",
    "# model_trace = poutine.trace(inferred_model).get_trace(tt_x_diff, tt_x2_or, tt_triu_obs)\n",
    "# model_trace.nodes['triu_star']['value'].shape\n",
    "\n",
    "num_samples = 650\n",
    "triu_star_sample = []\n",
    "nu_samples = []\n",
    "for _ in tqdm(range(num_samples), desc=\"Triu samples\"):\n",
    "    # Get a trace from the guide\n",
    "    guide_trace = poutine.trace(guide).get_trace(tt_x_diff, tt_x2_or, tt_triu_obs, n)\n",
    "    # Run infer_discrete\n",
    "    inferred_model = infer_discrete(poutine.replay(pyro_noisy_networks_model, guide_trace), first_available_dim=-2)\n",
    "    # Get a trace from the inferred model\n",
    "    model_trace = poutine.trace(inferred_model).get_trace(tt_x_diff, tt_x2_or, tt_triu_obs, n)\n",
    "    # Extract triu_star from the trace\n",
    "    triu_star_sample.append(model_trace.nodes['triu_star']['value'])\n",
    "    # Extract nu from the trace\n",
    "    nu_samples.append(model_trace.nodes['nu']['value'])\n",
    "\n",
    "# Convert to tensor\n",
    "triu_star_samples = torch.stack(triu_star_sample)\n",
    "nu_samples = torch.stack(nu_samples)\n",
    "print(triu_star_samples.shape, nu_samples.shape)"
   ],
   "id": "c8740e10e788af30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def triu_to_mat(triu_values, N):\n",
    "    \"\"\"\n",
    "    Convert upper triangular values to a full adjacency matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    triu_values (torch.Tensor): Upper triangular values, shape (num_triu_elements,)\n",
    "    N (int): Number of nodes in the network\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Full adjacency matrix, shape (N, N)\n",
    "    \"\"\"\n",
    "    # Create an empty N x N matrix\n",
    "    adj_matrix = torch.zeros((N, N), dtype=triu_values.dtype)\n",
    "    \n",
    "    # Get the indices for the upper triangular part\n",
    "    triu_indices = torch.triu_indices(N, N, offset=1)\n",
    "    \n",
    "    # Fill the upper triangular part\n",
    "    adj_matrix[triu_indices[0], triu_indices[1]] = triu_values\n",
    "    \n",
    "    # Make the matrix symmetric by adding its transpose\n",
    "    adj_matrix = adj_matrix + adj_matrix.T\n",
    "    \n",
    "    return adj_matrix\n",
    "\n",
    "# degs_post = torch.zeros(n, dtype=triu_star_samples.dtype)\n",
    "# degs_post = torch.zeros(n, dtype=samples['obs_triu'].dtype)\n",
    "# degs_post = jnp.zeros(n)\n",
    "z_samp = rng.binomial(n=1, p=.6, size=n)\n",
    "post_stat = []\n",
    "zeigen_new = []\n",
    "for i in tqdm(range(num_samples)):\n",
    "    cur_triu_star = triu_star_samples[i]\n",
    "    # cur_triu_star = samples['obs_triu'][i]\n",
    "    if i == 0:\n",
    "        print(cur_triu_star.shape)\n",
    "    # cur_mat_star = triu_to_mat(cur_triu_star, n)\n",
    "    cur_mat_star = Triu_to_mat(jnp.array(cur_triu_star))\n",
    "    eig_cen = eigen_centrality(cur_mat_star)\n",
    "    cur_zeig = zeigen_value(Z, eig_cen, cur_mat_star)\n",
    "    zeig_new = zeigen_value(z_samp, eig_cen, mat)\n",
    "    # cur_deg = cur_mat_star.sum(dim=1)\n",
    "    # cur_deg = jnp.sum(cur_mat_star, 1)\n",
    "    # degs_post += cur_deg\n",
    "    # degs_post += cur_zeig\n",
    "    post_stat.append(cur_zeig)\n",
    "    zeigen_new.append(zeig_new)\n",
    "    \n",
    "# avg_deg_post = degs_post / num_samples\n",
    "# avg_deg_post = avg_deg_post.cpu().numpy()\n",
    "post_stat = jnp.array(post_stat)\n",
    "zeigen_new = jnp.array(zeigen_new)\n",
    "print(post_stat.shape, zeigen_new.shape, type(post_stat), type(zeigen_new))\n",
    "avg_zeigen = post_stat.mean(axis=0)\n",
    "# true_deg = np.sum(mat,1)\n",
    "# print(\"MAPE:\", np.mean(np.abs(avg_deg_post - Zeigen)))\n",
    "print(\"MAE:\", np.mean(np.abs((avg_zeigen - Zeigen))))\n",
    "print(\"MAE:\", np.mean(np.abs((obs_Zeigen - Zeigen))))\n",
    "# print(\"MAPE:\", np.mean(np.abs((avg_deg_post - true_deg)/true_deg)))\n",
    "# print(\"MAPE:\", np.mean(np.abs((avg_deg_post - np.sum(obs_mat,1))/np.sum(obs_mat,1))))\n",
    "# print(\"MAPE:\", np.mean(np.abs(avg_deg_post - np.sum(mat,1))))\n",
    "plt.figure(figsize=(4,3))\n",
    "# plt.scatter(true_deg, avg_deg_post)\n",
    "# plt.scatter(np.sum(mat,1), avg_deg_post)\n",
    "plt.scatter(Zeigen, avg_zeigen)\n",
    "plt.axline((0, 0), slope=1, color=\"orange\", linestyle=\"--\")   \n",
    "\n"
   ],
   "id": "cb7b89b965cf3578",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_mat = triu_to_mat(triu_star_samples[5], n).cpu().numpy()\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.scatter(np.sum(mat,1), np.sum(test_mat,1))\n",
    "plt.axline((0, 0), slope=1, color=\"orange\", linestyle=\"--\")   \n"
   ],
   "id": "8eb7ba9d32ff9d48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plt.scatter(samples['nu'].mean(axis=0)[0], samples['nu'].mean(axis=0)[0,1])\n",
    "# plt.scatter(samples['nu'].mean(axis=0)[0,:,0], samples['nu'].mean(axis=0)[0,:,1])\n",
    "# plt.scatter(samples['nu'].mean(axis=0)[0,:,0], U_latent[:,0])\n",
    "# plt.scatter(samples['nu'].mean(axis=0)[0,:,1], U_latent[:,1])\n",
    "\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from scipy.spatial import procrustes\n",
    "\n",
    "def procrustes_alignment(posterior_draws):\n",
    "    \"\"\"\n",
    "    Perform Procrustes alignment on multiple posterior draws of U_i.\n",
    "    \n",
    "    :param posterior_draws: List of numpy arrays, each of shape (N, 2), where N is the number of units\n",
    "    :return: List of aligned posterior draws\n",
    "    \"\"\"\n",
    "    # Calculate the mean configuration as the reference\n",
    "    mean_config = np.mean(posterior_draws, axis=0)\n",
    "    # mean_config = posterior_draws[0]\n",
    "    \n",
    "    # Initialize array for aligned draws\n",
    "    aligned_draws = np.zeros_like(posterior_draws)\n",
    "\n",
    "    # Align each draw to the mean configuration\n",
    "    for i in range(posterior_draws.shape[0]):\n",
    "        # _, transformed, _ = procrustes(mean_config, posterior_draws[i])\n",
    "        # # transformed, _ = orthogonal_procrustes(mean_config, posterior_draws[i])\n",
    "        # aligned_draws[i] = transformed\n",
    "        R, _ = orthogonal_procrustes(posterior_draws[i], mean_config)\n",
    "        transformed_nu = posterior_draws[i] @ R\n",
    "        aligned_draws[i] = transformed_nu\n",
    "    # aligned_draws = orthogonal_procrustes()    \n",
    "    return aligned_draws\n",
    "\n",
    "aligned_nu = procrustes_alignment(nu_samples.detach().numpy())\n",
    "print(aligned_nu.shape)\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.scatter(U_diff_norm_val, latent_to_norm_of_diff(np.mean(aligned_nu,axis=0)))\n",
    "# print(np.corrcoef(U_diff_norm_val, latent_to_norm_of_diff(np.mean(nu_samples.detach().numpy(),axis=0))))\n",
    "print(np.corrcoef(U_diff_norm_val, latent_to_norm_of_diff(np.mean(aligned_nu,axis=0))))\n",
    "# plt.scatter(U_diff_norm_val, latent_to_norm_of_diff(np.mean(nu_samples.detach().numpy(),axis=0)))"
   ],
   "id": "3006580a6cdb72a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# ax1.scatter(U_latent[:,1], np.mean(aligned_nu,axis=0)[:,1])\n",
    "# ax1.scatter(np.mean(aligned_nu,axis=0)[:,0], np.mean(aligned_nu,axis=0)[:,1])\n",
    "# ax1.scatter(np.mean(nu_samples.detach().numpy(),axis=0)[:,0], np.mean(nu_samples.detach().numpy(),axis=0)[:,1])\n",
    "# ax1.scatter(nu_ref.detach().numpy()[:,0], nu_ref.detach().numpy()[:,1])\n",
    "# ax2.scatter(U_latent[:,0], U_latent[:,1])\n",
    "plt.scatter(U_latent[:,1],  np.mean(aligned_nu,axis=0)[:,1])\n",
    "# print(np.corrcoef(U_latent[:,0], samples['nu'].mean(axis=0)[0,:,0]))\n",
    "print(np.corrcoef(U_latent[:,1], np.mean(aligned_nu,axis=0)[:,1]))"
   ],
   "id": "ddf1f0af016be508",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pyro GP outcome model\n",
    "import pyro.contrib.gp as gp\n",
    "from torch.multiprocessing import Pool, cpu_count\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# os.environ['OMP_NUM_THREADS'] = 2\n",
    "\n",
    "# def kmeans_selection(X, num_inducing):\n",
    "#     # Convert to numpy for sklearn\n",
    "#     X_np = X.cpu().numpy()\n",
    "#     # Perform K-means clustering\n",
    "#     kmeans = KMeans(n_clusters=num_inducing, random_state=0, n_init=10)\n",
    "#     kmeans.fit(X_np)\n",
    "#     # Convert cluster centers back to torch tensor\n",
    "#     Xu = torch.tensor(kmeans.cluster_centers_, dtype=X.dtype)\n",
    "#     return Xu\n",
    "\n",
    "df_gp = torch.stack([torch.tensor(Z), torch.from_numpy(np.array(Zeigen)), torch.tensor(x)], dim=1)\n",
    "Y_gp = torch.from_numpy(np.array(Y))\n",
    "\n",
    "# Ensure consistent dtype\n",
    "# dtype = torch.float64\n",
    "\n",
    "# Convert your data to the specified dtype\n",
    "# df_gp = df_gp.to(dtype)\n",
    "# Y_gp = Y_gp.to(dtype)\n",
    "\n",
    "dataset = TensorDataset(df_gp, Y_gp)\n",
    "batch_size = n//2  # Adjust based on your dataset size and available memory\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Initialize inducing points with K-means\n",
    "# num_inducing = 100  # Adjust as needed\n",
    "# Xu_init = kmeans_selection(df_gp, num_inducing)\n",
    "\n",
    "# Clear any existing parameters\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Set up the kernel\n",
    "kernel = gp.kernels.RBF(input_dim=3, variance=torch.tensor(2.0), lengthscale=torch.tensor(3.0))\n",
    "\n",
    "# Set up the GP model\n",
    "# gpr = gp.models.GPRegression(df_gp, Y_gp, kernel, noise=torch.tensor(1.0))\n",
    "gpr = gp.models.GPRegression(df_gp[:1], Y_gp[:1], kernel, noise=torch.tensor(1.0))\n",
    "# sgpr = gp.models.SparseGPRegression(df_gp[:1], Y_gp[:1], kernel, Xu=Xu_init, jitter=1e-5)\n",
    "\n",
    "# sgpr.Xu.requires_grad = True\n",
    "\n",
    "# Define priors on the hyperparameters\n",
    "gpr.kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(0.0, 3.0))\n",
    "gpr.kernel.variance = pyro.nn.PyroSample(dist.LogNormal(0.0, 2.0))\n",
    "gpr.noise = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\n",
    "# sgpr.kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(0.0, 3.0))\n",
    "# sgpr.kernel.variance = pyro.nn.PyroSample(dist.LogNormal(0.0, 2.0))\n",
    "# sgpr.noise = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\n",
    "\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(gpr.parameters(), lr=0.001)\n",
    "# optimizer = torch.optim.Adam(sgpr.parameters(), lr=0.005)\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': sgpr.parameters(), 'lr': 0.01},\n",
    "#     {'params': sgpr.Xu, 'lr': 0.001}  # Separate learning rate for inducing points\n",
    "# ])\n",
    "\n",
    "loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "# Training loop\n",
    "# num_steps = 3000\n",
    "# losses = []\n",
    "# for i in tqdm(range(num_steps)):\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = loss_fn(gpr.model, gpr.guide)\n",
    "#     # loss = compute_loss(gpr.model, gpr.guide)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     losses.append(loss.item())\n",
    "# \n",
    "\n",
    "# # Training loop\n",
    "num_epochs = 5000  # Adjust as needed\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    epoch_loss = 0\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Update the model's data for this batch\n",
    "        gpr.set_data(batch_X, batch_y)\n",
    "        loss = loss_fn(gpr.model, gpr.guide)\n",
    "        # sgpr.set_data(batch_X, batch_y)\n",
    "        # loss = loss_fn(sgpr.model, sgpr.guide)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "# \n",
    "# # After training, set the model back to use all data\n",
    "gpr.set_data(df_gp, Y_gp)\n",
    "# sgpr.set_data(df_gp, Y_gp)\n",
    "\n",
    "def plot_loss(loss):\n",
    "    plt.plot(loss)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    _ = plt.ylabel(\"Loss\")  # supress output text\n",
    "\n",
    "\n",
    "plot_loss(losses)"
   ],
   "id": "8ce3a0e2717de0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def robust_cholesky(matrix, max_tries=5, initial_jitter=1e-6):\n",
    "    jitter = initial_jitter\n",
    "    num_tries = 0\n",
    "    while num_tries < max_tries:\n",
    "        try:\n",
    "            L = torch.linalg.cholesky(matrix + torch.eye(matrix.shape[0]) * jitter)\n",
    "            print(\"num_tries = \", num_tries)\n",
    "            return L\n",
    "        except RuntimeError:\n",
    "            jitter *= 10\n",
    "            num_tries += 1\n",
    "    raise ValueError(f\"Matrix is not positive definite, even with jitter of {jitter}\")\n",
    "\n",
    "def predict(gpr, X_new, num_samples=5000, approx=True):\n",
    "    with torch.no_grad():\n",
    "        # mean, cov = gpr(X_new, full_cov=True, noiseless=False)\n",
    "        mean, cov = gpr(X_new, full_cov=True, noiseless=True)\n",
    "        if approx:\n",
    "            # Add small jitter to ensure positive definiteness\n",
    "            # jitter = torch.eye(cov.shape[0]) * 1e-6\n",
    "            # L = torch.linalg.cholesky(cov + jitter)\n",
    "            L = robust_cholesky(cov)\n",
    "            # Generate samples from standard normal distribution\n",
    "            eps = torch.randn(cov.shape[0], num_samples, dtype=torch.float64)\n",
    "            # Transform to samples from multivariate normal\n",
    "            samples = mean.unsqueeze(1) + L @ eps\n",
    "            return mean,cov,samples.T\n",
    "        else:\n",
    "            samples = dist.MultivariateNormal(mean, cov).sample(sample_shape=(num_samples,))\n",
    "            return mean,cov,samples\n",
    "\n",
    "\n",
    "# pred_mean, pred_cov, pred_samples = predict(gpr, df_gp)\n",
    "pred_mean_ap, pred_cov_ap, pred_samples_ap = predict(gpr, df_gp, approx=True)\n",
    "# pred_mean, pred_cov, pred_samples = predict(sgpr, df_gp)\n",
    "# print(pred_mean.shape, pred_cov.shape, pred_samples.shape)\n",
    "print(pred_mean_ap.shape, pred_cov_ap.shape, pred_samples_ap.shape)\n"
   ],
   "id": "3f3b0455fe340421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pred_train_np = pred_samples_ap.detach().numpy()\n",
    "# pred_train_np = pred_samples_ap.detach().numpy()\n",
    "# print(np.std(pred_train_np, axis=0))\n",
    "q025, q975 = np.percentile(pred_train_np, [2.5, 97.5], axis=0)\n",
    "ind_cover = np.mean(((Y-epsilon) > q025) & ((Y-epsilon) < q975))\n",
    "ind_cover"
   ],
   "id": "aa1b2a90a904bc12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "# pred_mean_np = pred_mean.detach().numpy()\n",
    "pred_mean_np = pred_train_np.mean(axis=0)\n",
    "print(\"MAE:\", np.mean(np.abs((pred_mean_np - np.array(Y-epsilon)))))\n",
    "plt.errorbar(Y-epsilon, pred_mean_np, yerr=[pred_mean_np-q025, q975-pred_mean_np], \n",
    "             fmt='o', capsize=3, capthick=.6, ecolor='gray', alpha=0.8)\n",
    "# plt.scatter(Y-epsilon, pred_mean_np)\n",
    "# plt.scatter(Y, q975)\n",
    "# plt.fill_between(Y, q025, q975, alpha=0.5)\n",
    "plt.axline((0, 0), slope=1, color=\"orange\", linestyle=\"--\")   \n",
    "\n",
    "# TODO: continue here to check prediction on X_test (new z and zeigen) and with SparseGP"
   ],
   "id": "e47b2fa000ddbb56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z_samp = rng.binomial(n=1, p=.3, size=n)\n",
    "zeigen_sample = zeigen_value(z_samp, eigen_center, mat)\n",
    "df_gp_new = torch.stack([torch.tensor(z_samp), torch.from_numpy(np.array(zeigen_sample)), torch.tensor(x)], dim=1)\n",
    "y_samp, eps = gen_y(z_samp, x, zeigen_sample, alpha, lin=False)\n",
    "samp_esti = y_samp - eps\n",
    "Y_gp_new = torch.from_numpy(np.array(y_samp))\n",
    "\n",
    "pred_mean_new, pred_cov_new, pred_samples_new = predict(gpr, df_gp_new)\n",
    "\n",
    "pred_test_np = pred_samples_new.detach().numpy()\n",
    "# print(np.std(pred_train_np, axis=0))\n",
    "q025_test, q975_test = np.percentile(pred_test_np, [2.5, 97.5], axis=0)\n",
    "ind_cover_test = np.mean((samp_esti > q025_test) & (samp_esti < q975_test))\n",
    "# ind_cover_test = np.mean((y_samp > q025_test) & (y_samp < q975_test))\n",
    "print(\"pted shape:\", pred_samples_new.shape)\n",
    "print(\"Test cover:\", ind_cover_test)\n",
    "\n",
    "pred_mean_test = pred_test_np.mean(axis=0)\n",
    "plt.figure(figsize=(4,3))\n",
    "print(\"MAE:\", np.mean(np.abs((pred_mean_test - samp_esti))))\n",
    "plt.errorbar(samp_esti, pred_mean_test, yerr=[pred_mean_test-q025_test, q975_test-pred_mean_test], \n",
    "             fmt='o', capsize=3, capthick=.6, ecolor='gray', alpha=0.8)\n",
    "# plt.scatter(samp_esti, pred_mean_test)\n",
    "# plt.scatter(Y, q975)\n",
    "# plt.fill_between(Y, q025, q975, alpha=0.5)\n",
    "plt.axline((0, 0), slope=1, color=\"orange\", linestyle=\"--\") \n",
    "\n",
    "# TODO: GP works well on test set as well.  Check how SVI network module (LSM) works for data analysis (e.g., do predictive posterior checks for A obs for one school)"
   ],
   "id": "f8939f681b70080a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z_samp = rng.binomial(n=1, p=0.8, size=n)\n",
    "zeigen_sample = zeigen_value(z_samp, eigen_center, mat)\n",
    "df_gp_new = torch.stack([torch.tensor(z_samp), torch.from_numpy(np.array(zeigen_sample)), torch.tensor(x)], dim=1)\n",
    "\n",
    "z_samp2 = rng.binomial(n=1, p=0.6, size=n)\n",
    "zeigen_sample2 = zeigen_value(z_samp2, eigen_center, mat)\n",
    "df_gp_new2 = torch.stack([torch.tensor(z_samp2), torch.from_numpy(np.array(zeigen_sample2)), torch.tensor(x)], dim=1)\n",
    "\n",
    "z_new = torch.tensor(np.array([z_samp, z_samp2]))\n",
    "zeigen_new = torch.tensor(np.array([zeigen_sample, zeigen_sample2]))\n",
    "print(z_new.shape, zeigen_new.shape, z_new.ndim, z_samp.ndim)\n",
    "n_z = z_new.shape[0]\n",
    "df_new = torch.stack([z_new, zeigen_new, torch.tensor(x).repeat(n_z, 1)], dim=2)\n",
    "print(df_new.shape)\n",
    "print(df_new[0].shape, df_gp.shape)\n",
    "samples_multi = []\n",
    "for i in range(n_z):\n",
    "    pred_mean_new, pred_cov_new, pred_samples_new = predict(gpr, df_new[i])\n",
    "    samples_multi.append(pred_samples_new)\n",
    "\n",
    "# samples_multi = torch.stack(samples_multi)\n",
    "samples_multi = jnp.array(samples_multi)\n",
    "# with torch.no_grad():\n",
    "#         mean, cov = gpr(df_new, full_cov=True, noiseless=False)\n",
    "#         samples = dist.MultivariateNormal(mean, cov).sample(sample_shape=(168,))\n",
    "# \n",
    "print(samples_multi.shape, samples_multi.mean(axis=0).shape)\n"
   ],
   "id": "74634501480f6ff8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax2torch import jax2torch\n",
    "import torch.multiprocessing as mp\n",
    "# import multiprocessing\n",
    "# from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class Outcome_GP:\n",
    "    def __init__(self, X, Y, Z, Zeigen, n_iter = 1000, n_samples = 100):\n",
    "        self.X = torch.from_numpy(np.array(X))\n",
    "        # self.X = jax2torch(lambda x: x)(X)\n",
    "        # self.X2 = torch.tensor(X2)\n",
    "        self.Y = torch.from_numpy(np.array(Y))\n",
    "        # self.Y = jax2torch(lambda x: x)(Y)\n",
    "        # self.Z = jax2torch(lambda x: x)(Z)\n",
    "        self.Z = torch.from_numpy(np.array(Z))\n",
    "        # self.adj_mat = data[\"adj_mat\"]\n",
    "        # self.eig_cen = eigen_centrality(self.adj_mat)\n",
    "        # self.zeigen = torch.from_numpy(np.array(zeigen_value(self.Z, self.eig_cen, self.adj_mat)))\n",
    "        # self.zeigen = jax2torch(lambda x: x)(Zeigen)\n",
    "        self.zeigen = torch.from_numpy(np.array(Zeigen))\n",
    "        self.n = n\n",
    "        self.df = self.get_df()\n",
    "        self.gpr = self.gpr_model()\n",
    "        # self.rng_key = rng_key\n",
    "        self.n_iter = n_iter\n",
    "        self.n_samples = n_samples\n",
    "        self.dataset = TensorDataset(self.df, self.Y)\n",
    "        self.batch_size = self.n // 2  # Adjust based on your dataset size and available memory\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.post_samples = None\n",
    "\n",
    "\n",
    "    def get_df(self):\n",
    "        return torch.stack([self.Z, self.zeigen, self.X], dim=1)\n",
    "        # return torch.stack([torch.tensor(self.Z), self.zeigen, self.X], dim=1)\n",
    "\n",
    "    def gpr_model(self):\n",
    "        kernel = gp.kernels.RBF(input_dim=3, variance=torch.tensor(2.0), lengthscale=torch.tensor(3.0))\n",
    "        # return gp.models.GPRegression(self.df, self.Y, kernel, noise=torch.tensor(1.0))\n",
    "        return gp.models.GPRegression(self.df[:1], self.Y[:1], kernel, noise=torch.tensor(1.0))\n",
    "\n",
    "    def train_model(self):\n",
    "        pyro.clear_param_store()\n",
    "        # Define priors on the hyperparameters\n",
    "        self.gpr.kernel.lengthscale = pyro.nn.PyroSample(pyro.distributions.LogNormal(0.0, 3.0))\n",
    "        self.gpr.kernel.variance = pyro.nn.PyroSample(pyro.distributions.LogNormal(0.0, 2.0))\n",
    "        self.gpr.noise = pyro.nn.PyroSample(pyro.distributions.LogNormal(0.0, 1.0))\n",
    "        # Set up the optimizer\n",
    "        optimizer = torch.optim.Adam(self.gpr.parameters(), lr=0.01)\n",
    "        loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "        # Training loop\n",
    "        # losses = []\n",
    "        for epoch in range(self.n_iter):\n",
    "            # epoch_loss = 0\n",
    "            for batch_X, batch_y in self.dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                # Update the model's data for this batch\n",
    "                self.gpr.set_data(batch_X, batch_y)\n",
    "                loss = loss_fn(self.gpr.model, self.gpr.guide)\n",
    "                # sgpr.set_data(batch_X, batch_y)\n",
    "                # loss = loss_fn(sgpr.model, sgpr.guide)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # epoch_loss += loss.item()\n",
    "            # avg_loss = epoch_loss / len(dataloader)\n",
    "            # losses.append(avg_loss)\n",
    "            # print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        # \n",
    "        # # After training, set the model back to use all data\n",
    "        gpr.set_data(self.df, self.Y)\n",
    "        # for i in tqdm(range(self.n_iter), desc=\"GP training\"):\n",
    "        #     optimizer.zero_grad()\n",
    "        #     loss = loss_fn(self.gpr.model, self.gpr.guide)\n",
    "        #     # loss = compute_loss(gpr.model, gpr.guide)\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "\n",
    "    def predict_one_df(self, df):\n",
    "        with torch.no_grad():\n",
    "            mean, cov = self.gpr(df, full_cov=True, noiseless=False)\n",
    "            # cov = cov + torch.eye(cov.shape[0]) * self.jitter\n",
    "            # Add small jitter to ensure positive definiteness\n",
    "            jitter = torch.eye(cov.shape[0]) * 1e-6\n",
    "            L = torch.linalg.cholesky(cov + jitter)\n",
    "            print(\"L dtype: \", L.dtype)\n",
    "\n",
    "            # Generate samples from standard normal distribution\n",
    "            eps = torch.randn(cov.shape[0], self.n_samples, dtype=torch.float64)\n",
    "            print(\"eps dtype: \", eps.dtype)\n",
    "            # Transform to samples from multivariate normal\n",
    "            samples = mean.unsqueeze(1) + L @ eps\n",
    "\n",
    "        return jnp.array(samples.T)\n",
    "            # samples = pyro.distributions.MultivariateNormal(mean, cov).sample(sample_shape=(self.n_samples,))\n",
    "        # return jnp.array(samples)\n",
    "\n",
    "    def predict(self, z_new, zeigen_new):\n",
    "        z_new = torch.tensor(np.array(z_new))\n",
    "        zeigen_new = torch.tensor(np.array(zeigen_new))\n",
    "        if z_new.ndim == 1:\n",
    "            df_new = torch.stack([z_new, zeigen_new, self.X], dim=1)\n",
    "            return self.predict_one_df(df_new)\n",
    "        elif z_new.ndim == 2:\n",
    "            n_z = z_new.shape[0]\n",
    "            df_new = torch.stack([z_new, zeigen_new, self.X.repeat(n_z, 1)], dim=2)\n",
    "            print(\"df_new shape: \", df_new.shape)\n",
    "            samples_multi = []\n",
    "            for i in range(n_z):\n",
    "                samples_multi.append(self.predict_one_df(df_new[i]))\n",
    "            return jnp.array(samples_multi).mean(axis=0)\n",
    "\n",
    "def train_and_predict_single_gpr(args):\n",
    "    X, Y, Z_obs, zeigen_m, z_h, z_stoch, zeigen_h, zeigen_stoch, n_iter, n_samples = args\n",
    "    \n",
    "    # Train the model\n",
    "    gpr = Outcome_GP(X, Y, Z_obs, zeigen_m, n_iter=n_iter, n_samples=n_samples)\n",
    "    gpr.train_model()\n",
    "    \n",
    "    # Make prediction\n",
    "    # prediction = gpr.predict(z_new, zeigen_new_m)\n",
    "    # \n",
    "    # return prediction\n",
    "    gp_h_pred = gpr.predict(z_h, zeigen_h)\n",
    "    gp_stoch_pred = gpr.predict(z_stoch, zeigen_stoch)\n",
    "    return jnp.array([gp_h_pred, gp_stoch_pred])\n",
    "\n",
    "def parallel_gpr_training_and_prediction(X, Y, Z_obs, Zeigen_df, z_new, zeigen_new, n_iter=1000, n_samples=100, num_processes=4):\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        # Prepare arguments for each row\n",
    "        args_list = [(X, Y, Z_obs, zeigen_m, z_new, zeigen_new_m, n_iter, n_samples) \n",
    "                     for zeigen_m, zeigen_new_m in zip(Zeigen_df, zeigen_new)]\n",
    "        \n",
    "        # Train and predict in parallel\n",
    "        results = list(tqdm(pool.imap(train_and_predict_single_gpr, args_list), \n",
    "                            total=len(args_list), \n",
    "                            desc=\"Training GPRs and Predicting\"))\n",
    "    \n",
    "    # Separate the results into trained models and predictions\n",
    "    # trained_models, predictions = zip(*results)\n",
    "    return results\n",
    "    # return list(trained_models), list(predictions)\n",
    "\n",
    "def process_gpr(args):\n",
    "    return train_and_predict_single_gpr(args)\n",
    "\n",
    "def run_parallel_gpr(args_list, num_processes=-1):\n",
    "    with tqdm(total=len(args_list), desc=\"GP multiple (parallel)\") as pbar:\n",
    "        results = Parallel(n_jobs=num_processes, backend=\"loky\")(\n",
    "            delayed(train_and_predict_single_gpr)(args) for args in args_list\n",
    "         )\n",
    "        pbar.update(len(args_list))\n",
    "    \n",
    "    return jnp.array(results)\n",
    "    # if num_processes is None:\n",
    "    #     num_processes = multiprocessing.cpu_count()  # Use all available CPU cores\n",
    "    # num_processes = min(num_processes, len(args_list))\n",
    "    # \n",
    "    # with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "    #      preds = list(tqdm(\n",
    "    #         pool.imap_unordered(train_and_predict_single_gpr, args_list, chunksize=1),\n",
    "    #         total=len(args_list),\n",
    "    #         desc=f\"GP multiple (parallel, {num_processes} cores)\"\n",
    "    #      ))\n",
    "    # \n",
    "    # return jnp.array(preds)\n",
    "\n",
    "def run_sequential_gpr(args_list):\n",
    "    return jnp.array([train_and_predict_single_gpr(args) for args in tqdm(args_list, desc=\"GP multiple (sequential)\")])"
   ],
   "id": "4619f4b249ea0b09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(zeigen_new.shape)\n",
    "zeigen_new_duplicate = jnp.array([zeigen_new, zeigen_new])\n",
    "zeigen_new_duplicate = jnp.transpose(zeigen_new_duplicate, axes=(1, 0, 2))\n",
    "\n",
    "print(zeigen_new_duplicate.shape)"
   ],
   "id": "f9baa9fc3184ae55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z_samp_stoch = jnp.array([z_samp, z_samp])\n",
    "z_samp_stoch.shape"
   ],
   "id": "5a5afc4437319379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test_trained, test_pred = train_and_predict_single_gpr((x, Y, Z, post_stat[0:12], z_samp, zeigen_new[0:12], 2500, 100))\n",
    "# print(len(test_pred), test_pred[0].shape)\n",
    "# with mp.Pool(processes=4) as pool:\n",
    "#     # Prepare arguments for each row\n",
    "#     args_list = [(x, Y, Z, zeigen_m, z_samp, zeigen_new_m, 150, 100) \n",
    "#                  for zeigen_m, zeigen_new_m in zip(post_stat[0:8], zeigen_new[0:8])]\n",
    "#     \n",
    "#     # Train and predict in parallel\n",
    "#     results = list(tqdm(pool.imap(train_and_predict_single_gpr, args_list), \n",
    "#                         total=len(args_list), \n",
    "#                         desc=\"Training GPRs and Predicting\"))\n",
    "# \n",
    "from time import time\n",
    "\n",
    "args_list = [(x, Y, Z, zeigen_m, z_samp, z_samp_stoch, zeig_h, zeig_stoch, 200, 19)\n",
    "                     for zeigen_m, zeig_h, zeig_stoch in zip(post_stat[0:3], zeigen_new[0:3], zeigen_new_duplicate[0:3])]\n",
    "\n",
    "# preds = []\n",
    "# for i in tqdm(range(4),\"GP multiple\"):\n",
    "#     preds.append(train_and_predict_single_gpr(args_list[i]))\n",
    "#     \n",
    "# preds = jnp.array(preds)\n",
    "\n",
    "# preds = run_parallel_gpr(args_list, num_processes=4)\n",
    "# \n",
    "# print(preds.shape)\n",
    "\n",
    "# Time sequential execution\n",
    "# start = time()\n",
    "# preds_sequential = run_sequential_gpr(args_list)\n",
    "# sequential_time = time() - start\n",
    "# print(f\"Sequential execution time: {sequential_time:.2f} seconds\")\n",
    "\n",
    "# Time parallel execution\n",
    "start = time()\n",
    "preds_parallel = run_parallel_gpr(args_list, 4)\n",
    "parallel_time = time() - start\n",
    "print(f\"Parallel execution time: {parallel_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# print(f\"Speedup: {sequential_time / parallel_time:.2f}x\")     \n",
    "\n",
    "# TODO: debug this part. Make GP parallel. Chane simulations code to first get posterior samples of zeigen and z values and then run multistage and one stage."
   ],
   "id": "ce7dbacfa63127f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print(preds_parallel.shape, preds_sequential.shape)\n",
    "print(preds_parallel.shape)\n",
    "pred_h = preds_parallel[:,0,:,:]\n",
    "pred_h_long = pred_h.reshape(-1, pred_h.shape[-1])\n",
    "print(pred_h_long.shape)\n",
    "pred_stoch = preds_parallel[:,1,:,:]\n",
    "pred_stoch_long = pred_stoch.reshape(-1, pred_stoch.shape[-1])\n",
    "print(pred_stoch_long.shape)"
   ],
   "id": "b8dd13eb4f2e61c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "args_list = [(x, Y, Z, zeigen_m, z_samp, zeigen_new_m, 1000, 100) \n",
    "                 for zeigen_m, zeigen_new_m in zip(post_stat[0:5], zeigen_new_duplicate[0:5])]\n",
    "len(args_list)"
   ],
   "id": "ae3edda0c432c795",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(zeigen_new.shape)",
   "id": "df4f1e883ac48628",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print([a.shape for a in args_list[0][0:6]])",
   "id": "45867b5d8b5aa7ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test_trained, test_pred = train_and_predict_single_gpr((x, Y, Z, post_stat[0:4], z_samp, zeigen_new[0:4], 1000, 100))\n",
    "\n",
    "args_list = [(x, Y, Z, zeigen_m, z_new, zeigen_new_m, 1000, 100) \n",
    "                     for zeigen_m, zeigen_new_m in zip(post_stat[0:4], zeigen_new[0:4])]\n",
    "Xs, Ys, Z_obss, zeigen_ms, z_news, zeigen_new_ms, n_iters, n_sampless = args_list[0]\n",
    "# print(len(args_list), args_list[0], args_list[0])\n",
    "print(Xs.shape, Ys.shape, Z_obss.shape, zeigen_ms.shape, z_news.shape, zeigen_new_ms.shape, n_iters, n_sampless)"
   ],
   "id": "1f8fd16ac18c8420",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Outcome model with A* (true network)\n",
    "rng_key = random.PRNGKey(1)\n",
    "rng_key, rng_key_ = random.split(rng_key)\n",
    "# start = time.time()\n",
    "kernel_outcome = NUTS(outcome_model)\n",
    "mcmc_network_true = MCMC(kernel_outcome, num_warmup=2000, num_samples=4000,num_chains=4, jit_model_args=True)\n",
    "                         # chain_method=\"vectorized\")\n",
    "# mcmc_network_true = MCMC(kernel_outcome, num_warmup=2000, num_samples=4000,num_chains=4)\n",
    "# mcmc_network_true = MCMC(kernel_outcome, num_warmup=100, num_samples=10,num_chains=1)\n",
    "# mcmc_network_true.run(rng_key_,Y=Y,Z=Z,A=obs_mat,n=n)\n",
    "# mcmc_network_true.run(rng_key_,Y=Y,Z=Z,X=x,A=mat,n=n)\n",
    "mcmc_network_true.run(rng_key_,X=df_array,Y=Y)\n",
    "mcmc_network_true.print_summary()\n",
    "samples_net_true = mcmc_network_true.get_samples()\n",
    "true_net_lin_predictive = Predictive(outcome_model, samples_net_true, exclude_deterministic=False, return_sites=[\"Y\",\"mu\"])\n",
    "# print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d513060616ffd48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(true_net_lin_predictive(rng_key_,df_array).keys())\n",
    "print(samples_net_true.keys())"
   ],
   "id": "45e5be2ba9dab855",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(samples_net_true[\"alpha\"].shape, df_array.shape, np.transpose(df_array).shape, np.dot(samples_net_true[\"alpha\"],np.transpose(df_array)).shape)",
   "id": "12880003f7b0c67b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def mod_outcome_model(X):\n",
    "#     with numpyro.plate(\"Lin coef.\",X.shape[1]):\n",
    "#         alpha = numpyro.sample(\"alpha\",dist.Normal(0,5))\n",
    "#     sig = numpyro.sample(\"sig\",dist.HalfNormal(scale=2))\n",
    "#     return numpyro.deterministic(\"mu\", jnp.dot(X, alpha))\n",
    "    \n",
    "    \n",
    "# Create a modified model for prediction\n",
    "def pred_model(X):\n",
    "    return numpyro.handlers.substitute(outcome_model, data={\"X\": X})(X)\n",
    "\n",
    "# Use Predictive with the modified model\n",
    "predictive_test = Predictive(pred_model, samples_net_true)\n",
    "\n",
    "@jit\n",
    "def manual_linear_pred(samples, X):\n",
    "    alpha = samples[\"alpha\"]\n",
    "    return jnp.dot(alpha, jnp.transpose(X))\n",
    "\n"
   ],
   "id": "6fceeffde258e830",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictive_test(rng_key_,df_array).keys()",
   "id": "ce9c43da17c4e8d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# mu_hat = predictive_test(rng_key_,df_array)[\"mu\"]\n",
    "mu_hat = manual_linear_pred(samples_net_true,df_array)\n",
    "# mu_hat = true_net_lin_predictive(rng_key_,df_array)[\"mu\"]\n",
    "# mu_hat = true_net_lin_predictive(rng_key_,df_array)[\"mu_star\"]\n",
    "# y_hat = true_net_lin_predictive(rng_key_,df_array)[\"Y\"]\n",
    "q025 = np.quantile(mu_hat,0.025,axis=0)\n",
    "q975 = np.quantile(mu_hat,0.975,axis=0)\n",
    "mean_mu_hat = mu_hat.mean(axis=0)\n",
    "\n",
    "train_esti = Y-epsilon\n",
    "print(mean_mu_hat[0], q025[0], q975[0], train_esti[0])\n",
    "print(\"MAE (point): \", np.mean(np.abs(mean_mu_hat - train_esti)),\n",
    "      \"MAE (all): \", np.mean(np.abs(mu_hat - train_esti)),\n",
    "      \"RMSE: \", np.sqrt(np.mean((mean_mu_hat - train_esti)**2)),\n",
    "      \"mean_ind_coverage: \", np.mean((train_esti > q025) & (train_esti < q975)),\n",
    "      \"mean true estimand: \", np.mean(train_esti),\n",
    "        \"mean posterior mean: \", np.mean(mean_mu_hat),\n",
    "        \"mean CI length: \", np.mean(q975 - q025),\n",
    "              \"q025 estimand: \", np.quantile(mu_hat.mean(axis=1),0.025),\n",
    "      \"q975 estimand: \", np.quantile(mu_hat.mean(axis=1),0.975))\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(train_esti, mean_mu_hat, 'o')\n",
    "# plt.plot(mu_hat.mean(axis=0), y_hat.mean(axis=0), 'o')\n",
    "plt.fill_between(train_esti, q025, q975, alpha=0.5)\n",
    "plt.xlabel(\"True estimand\")\n",
    "plt.ylabel(\"Posterior mean\")"
   ],
   "id": "8bd7724bdf91ec1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z_samp = rng.binomial(n=1, p=0.6, size=n)\n",
    "zeigen_sample = zeigen_value(z_samp, eigen_center, mat)\n",
    "dflin = jnp.transpose(jnp.array([[1]*n, z_samp ,x, z_samp*x, zeigen_sample]))\n",
    "y_samp, eps = gen_y(z_samp, x, zeigen_sample, alpha, lin=False)\n",
    "esti1 = y_samp - eps\n",
    "z_samp2 = rng.binomial(n=1, p=0.4, size=n)\n",
    "zeigen_sample2 = zeigen_value(z_samp2, eigen_center, mat)\n",
    "dflin2 = jnp.transpose(jnp.array([[1]*n, z_samp2 ,x, z_samp2*x, zeigen_sample2]))\n",
    "y_samp2, eps2 = gen_y(z_samp, x, zeigen_sample2, alpha, lin=False)\n",
    "esti2 = y_samp2 - eps2\n",
    "samp_esti = esti2 - esti1\n",
    "\n",
    "# mu_hat2 = true_net_lin_predictive(rng_key_, dflin)[\"mu_star\"]\n",
    "mu_hat1 = manual_linear_pred(samples_net_true, dflin)\n",
    "mu_hat2 = manual_linear_pred(samples_net_true, dflin2)\n",
    "mu_hat =  mu_hat1\n",
    "# mu_hat = mu_hat2 - mu_hat1\n",
    "# mu_hat2 = predictive_test(rng_key_, dflin)[\"mu\"]\n",
    "# mu_hat2 = true_net_lin_predictive(rng_key_, dflin)[\"Y\"]\n",
    "# mu_hat = np.dot(samples_net_true[\"alpha\"],np.transpose(dflin))\n",
    "q025 = np.quantile(mu_hat,0.025,axis=0)\n",
    "q975 = np.quantile(mu_hat,0.975,axis=0)\n",
    "mean_mu_hat2 = mu_hat.mean(axis=0)\n",
    "\n",
    "print(\"MAE (point): \", np.mean(np.abs(mean_mu_hat2 - esti1)),\n",
    "      \"MAE (all): \", np.mean(np.abs(mu_hat - esti1)),\n",
    "      \"RMSE: \", np.sqrt(np.mean((mean_mu_hat2 - esti1)**2)),\n",
    "      \"mean_ind_coverage: \", np.mean((esti1 > q025) & (esti1 < q975)),\n",
    "      \"mean true estimand: \", np.mean(esti1),\n",
    "      \"mean posterior mean: \", np.mean(mean_mu_hat2),\n",
    "      \"mean CI length: \", np.mean(q975 - q025),\n",
    "        \"q025 estimand: \", np.quantile(mu_hat.mean(axis=1),0.025),\n",
    "      \"q975 estimand: \", np.quantile(mu_hat.mean(axis=1),0.975))\n",
    "      \n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "# plt.plot(mean_mu_hat2, mean_mu_hat, 'o')\n",
    "plt.plot(esti1, mean_mu_hat2, 'o')\n",
    "plt.xlabel(\"Posterior mean\")\n",
    "plt.ylabel(\"True estimand\")\n",
    "# plt.fill_between(samp_esti, q025, q975, alpha=0.5)\n",
    "\n"
   ],
   "id": "a9ecb822189cc12",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "mcmc_network_true.run(rng_key,X=df_array,Y=Y)\n",
    "true_samps = mcmc_network_true.get_samples()\n",
    "# mcmc_network_true.print_summary()\n",
    "print(time.time()-start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "465d099c8a034c48",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "rng_key = random.PRNGKey(0)\n",
    "rng_key, rng_key_ = random.split(rng_key)\n",
    "\n",
    "\n",
    "@jit\n",
    "def linear_model_samples(key, Y, df):\n",
    "    kernel_outcome = NUTS(outcome_model)\n",
    "    # lin_mcmc = MCMC(kernel_outcome, num_warmup=2000, num_samples=4000,num_chains=4, progress_bar=False)\n",
    "    lin_mcmc = MCMC(kernel_outcome, num_warmup=2000, num_samples=4000,num_chains=4, progress_bar=False, chain_method=\"vectorized\")\n",
    "    # lin_mcmc = MCMC(kernel_outcome, num_warmup=2000, num_samples=4000,num_chains=4, progress_bar=False, chain_method=\"vectorized\")\n",
    "    # mcmc.run(key, Y=Y, Z=Z, X=X, A=A, n=NN)\n",
    "    lin_mcmc.run(key, X=df, Y=Y)\n",
    "    return lin_mcmc.get_samples()\n",
    "\n",
    "@jit\n",
    "def outcome_jit_pred(post_samples, df_arr, key=rng_key):\n",
    "    pred_func = Predictive(outcome_model, post_samples)\n",
    "    return pred_func(key, df_arr)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfc5cac0da2be48c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "samples_net_true = linear_model_samples(rng_key,Y=Y, df=df_array)\n",
    "print(time.time()-start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e419773379c49748",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print({\"alpha\" : jnp.expand_dims(jnp.mean(samples_net_true[\"alpha\"],axis=0),-2)})",
   "id": "abc11d36d8155ded",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(outcome_jit_pred(samples_net_true, df_array).keys())",
   "id": "4f5c5d16a211a8c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mu_hat = outcome_jit_pred(samples_net_true, df_array)[\"Y\"]\n",
    "q025 = np.quantile(mu_hat,0.025,axis=0)\n",
    "q975 = np.quantile(mu_hat,0.975,axis=0)\n",
    "mean_mu_hat = mu_hat.mean(axis=0)\n",
    "\n",
    "train_esti = Y-epsilon\n",
    "\n",
    "print(\"MAE (point): \", np.mean(np.abs(mean_mu_hat - train_esti)),\n",
    "      \"MAE (all): \", np.mean(np.abs(mu_hat - train_esti)),\n",
    "      \"RMSE: \", np.sqrt(np.mean((mean_mu_hat - train_esti)**2)),\n",
    "      \"mean_ind_coverage: \", np.mean((train_esti > q025) & (train_esti < q975)))\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(Y-epsilon, mean_mu_hat, 'o')"
   ],
   "id": "1c84cad302150fca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z_samp = rng.binomial(n=1, p=0.6, size=n)\n",
    "zeigen_sample = zeigen_value(z_samp, eigen_center, mat)\n",
    "dflin = jnp.transpose(jnp.array([[1]*n, z_samp ,x, z_samp*x, zeigen_sample]))\n",
    "y_samp, eps = gen_y(z_samp, x, zeigen_sample, alpha, lin=True)\n",
    "samp_esti = y_samp - eps\n",
    "\n",
    "# mu_hat2 = true_net_lin_predictive(rng_key_, dflin)[\"mu_star\"]\n",
    "mu_hat2 = outcome_jit_pred(samples_net_true, dflin)[\"Y\"]\n",
    "# mu_hat = np.dot(samples_net_true[\"alpha\"],np.transpose(dflin))\n",
    "q025 = np.quantile(mu_hat2,0.025,axis=0)\n",
    "q975 = np.quantile(mu_hat2,0.975,axis=0)\n",
    "mean_mu_hat = mu_hat2.mean(axis=0)\n",
    "\n",
    "print(\"MAE (point): \", np.mean(np.abs(mean_mu_hat - samp_esti)),\n",
    "      # \"MAE (all): \", np.mean(np.abs(mu_hat - samp_esti)),\n",
    "      \"RMSE: \", np.sqrt(np.mean((mean_mu_hat - samp_esti)**2)),\n",
    "      \"mean_ind_coverage: \", np.mean((samp_esti > q025) & (samp_esti < q975)),\n",
    "      \"mean true estimand: \", np.mean(samp_esti),\n",
    "      \"mean posterior mean: \", np.mean(mean_mu_hat))\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(samp_esti, mean_mu_hat, 'o')\n",
    "plt.xlabel(\"Posterior mean\")\n",
    "plt.ylabel(\"True estimand\")\n",
    "plt.fill_between(samp_esti, q025, q975, alpha=0.5)"
   ],
   "id": "d2b0928a0b42fc9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Outcome model with A (observed network)\n",
    "\n",
    "# start = time.time()\n",
    "kernel_outcome = NUTS(outcome_model)\n",
    "mcmc_network_obs = MCMC(kernel_outcome, num_warmup=2000, num_samples=4000,num_chains=4)\n",
    "# mcmc_network_obs.run(rng_key_,Y=Y,Z=Z,X=x,A=obs_mat,n=n)\n",
    "mcmc_network_obs.run(rng_key_, X=obs_df, Y=Y)\n",
    "# mcmc_network_obs.run(rng_key_,Y=Y,Z=Z,A=mat,n=n)\n",
    "mcmc_network_obs.print_summary()\n",
    "samples_net_obs = mcmc_network_obs.get_samples()\n",
    "obs_net_lin_predictive = Predictive(outcome_model, samples_net_obs)\n",
    "\n",
    "# print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7027822d4d0e646",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "samples_net_obs = linear_model_samples(rng_key_,Y=Y, df=jnp.array(obs_df))\n",
    "print({\"alpha\" : jnp.expand_dims(jnp.mean(samples_net_obs[\"alpha\"],axis=0),-2)})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19cafff8d3b65e29",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.hist(samples_net_true[\"alpha\"][:,4])\n",
    "# plt.axvline(x=alpha[4], color=\"red\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6073a50a018be492",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# print([f\"beta_{i} = {bet}\" for i,bet in enumerate(beta)], \"gamma0:\", gamma0, \"gamma1:\", gamma1)\n",
    "# print([f\"alpha_{i} = {al}\" for i,al in enumerate(alpha)],\"sig:\", 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91c7915d0d6264c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# start = time.time()\n",
    "kernel_gp_outcome = NUTS(HSGP_model, target_accept_prob=0.9)\n",
    "mcmc_GP_network_true = MCMC(kernel_gp_outcome, num_warmup=2000, num_samples=4000,num_chains=4)\n",
    "# mcmc_network_true = MCMC(kernel_outcome, num_warmup=100, num_samples=10,num_chains=1)\n",
    "# mcmc_network_true.run(rng_key_,Y=Y,Z=Z,A=obs_mat,n=n)\n",
    "# mcmc_network_true.run(rng_key_,Y=Y,Z=Z,X=x,A=mat,n=n)\n",
    "mcmc_GP_network_true.run(rng_key_,Xgp=Xgp, Xlin=Xlin, ell=ell ,m=m, Y=Y)\n",
    "# mcmc_GP_network_true.run(rng_key_,Xgp=Zeigen, Xlin=Xlin, Y=Y, ell=ell ,m=m)\n",
    "mcmc_GP_network_true.print_summary()\n",
    "# idata_gp_true = az.from_numpyro(posterior=mcmc_GP_network_true)\n",
    "\n",
    "samples_GP_net_true = mcmc_GP_network_true.get_samples()\n",
    "# print(time.time() - start)\n",
    "HSGP_true_predictive = Predictive(HSGP_model, samples_GP_net_true)\n",
    "\n",
    "# axes = az.plot_trace(\n",
    "#     data=idata_gp_true,\n",
    "#     kind=\"rank_bars\",\n",
    "#     backend_kwargs={\"figsize\": (10, 7), \"layout\": \"constrained\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f52680a9a3f4638",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(HSGP_true_predictive(rng_key_,Xgp=Xgp, Xlin = Xlin, ell=ell, m=m).keys())",
   "id": "fcd37d4c6257be6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mu_hat = HSGP_true_predictive(rng_key_,Xgp=Xgp, Xlin = Xlin, ell=ell, m=m)[\"mu_star\"]\n",
    "# mu_hat = HSGP_true_predictive(rng_key_,Xgp=Xgp, Xlin = Xlin, ell=ell, m=m)[\"f_star\"]\n",
    "q025 = np.quantile(mu_hat,0.025,axis=0)\n",
    "q975 = np.quantile(mu_hat,0.975,axis=0)\n",
    "mean_mu_hat = mu_hat.mean(axis=0)\n",
    "\n",
    "print(\"MAE: \", np.mean(np.abs(Y-epsilon - mean_mu_hat)),\n",
    "      \"RMSE: \", np.sqrt(np.mean((Y-epsilon - mean_mu_hat)**2)),\n",
    "      \"mean_ind_coverage: \", np.mean((Y-epsilon > q025) & (Y-epsilon < q975)))\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(Zeigen, mean_mu_hat, 'o')\n",
    "plt.fill_between(Zeigen, q025, q975, alpha=0.5)\n",
    "# plt.plot(Y-epsilon, mean_mu_hat, 'o')\n",
    "# plt.fill_between(Y-epsilon, q025, q975, alpha=0.5)"
   ],
   "id": "78833bb97c38e761",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "M = m\n",
    "num_warmup=2000\n",
    "# num_warmup=2000\n",
    "# num_warmup=250\n",
    "# num_samples=25\n",
    "num_samples=4000\n",
    "# num_samples=4000\n",
    "num_chains=4\n",
    "\n",
    "@jit    \n",
    "def HSGP_model_samples(key, Y, Xgp, Xlin, ell):\n",
    "    kernel_hsgp = NUTS(HSGP_model)\n",
    "    hsgp_mcmc = MCMC(kernel_hsgp, num_warmup=num_warmup, num_samples=num_samples,num_chains=num_chains, progress_bar=False, chain_method=\"vectorized\")\n",
    "    # mcmc.run(key, Y=Y, Z=Z, X=X, A=A, n=NN)\n",
    "    hsgp_mcmc.run(key, Xgp=Xgp, Xlin=Xlin, ell=ell ,m=M, Y=Y)\n",
    "    # hsgp_mcmc.print_summary()\n",
    "    # samples = hsgp_mcmc.get_samples()\n",
    "    # hsgp_pred = Predictive(HSGP_model, hsgp_mcmc.get_samples())\n",
    "    # return hsgp_pred\n",
    "    return hsgp_mcmc.get_samples()\n",
    "\n",
    "@jit\n",
    "def HSGP_jit_pred(post_samples, Xgp, Xlin, ell):\n",
    "    pred_func = Predictive(HSGP_model, post_samples)\n",
    "    return pred_func(rng_key, Xgp=Xgp, Xlin = Xlin, ell=ell, m=M)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5df2ae46cd8f600",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "samples_GP_net_true = HSGP_model_samples(rng_key_,Y=Y, Xgp=Xgp, Xlin=Xlin, ell=ell)\n",
    "# HSGP_true_predictive = Predictive(HSGP_model, samples_GP_net_true)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "738dc3d783679108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print({\"GP\" : jnp.expand_dims(jnp.mean(samples_GP_net_true[\"alpha\"],axis=0),-2)})\n",
    "print({\"GP\" : jnp.expand_dims(jnp.mean(samples_GP_net_true[\"sig\"],axis=0),-1)})\n",
    "print({\"GP\" : jnp.expand_dims(jnp.mean(samples_GP_net_true[\"magn\"],axis=0),-1)})\n",
    "print({\"GP\" : jnp.expand_dims(jnp.mean(samples_GP_net_true[\"length\"],axis=0),-1)})\n",
    "print(samples_GP_net_true['beta'].shape)\n",
    "# samples_GP_net_true"
   ],
   "id": "f79b970d0fa426e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# HSGP with noisy data\n",
    "Xlin_obs = jnp.array(obs_df[:,0:3])\n",
    "# Xlin_obs = jnp.array(obs_df[:,0:4])\n",
    "Xgp_obs = jnp.array(obs_df[:,3:])\n",
    "# Xgp_obs = jnp.array(obs_df[:,4:])\n",
    "ell_obs = jnp.array(c*jnp.max(jnp.abs(Xgp_obs))).reshape(1,1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f9af3adb7ce2d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "samples_GP_net_obs = HSGP_model_samples(rng_key_,Y=Y, Xgp=Xgp_obs, Xlin=Xlin_obs, ell=ell_obs)\n",
    "# mcmc_GP_network_obs = HSGP_model_samples(rng_key_,Y=Y, Xgp=jnp.array(Xgp_obs), Xlin=jnp.array(Xlin_obs), ell=jnp.array(ell_obs).reshape(1,1))\n",
    "# HSGP_obs_predictive = Predictive(HSGP_model, samples_GP_net_obs)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cf1bad4900b7606",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print({\"GP\" : jnp.expand_dims(jnp.mean(samples_GP_net_obs[\"alpha\"],axis=0),-2)})\n",
    "print({\"GP\" : jnp.expand_dims(jnp.mean(samples_GP_net_obs[\"sig\"],axis=0),-1)})\n",
    "print({\"GP\" : jnp.expand_dims(jnp.mean(samples_GP_net_obs[\"magn\"],axis=0),-1)})\n",
    "print({\"GP\" : jnp.expand_dims(jnp.mean(samples_GP_net_obs[\"length\"],axis=0),-1)})\n"
   ],
   "id": "2cf0ec687dfd5a4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_array[:,1:3].shape",
   "id": "9afe11e39176e440",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df1_gp = jnp.transpose(jnp.array([Z, x]))\n",
    "# df1_gp = jnp.transpose(jnp.array([Z, x]))\n",
    "df2_gp = jnp.transpose(jnp.array([Z, Zeigen]))\n",
    "# df2_gp = jnp.array(df_array[:,(1,3)])\n",
    "# ell_gp_comb = jnp.max(c*jnp.max(df_gp))\n",
    "# ell_gp_comb = jnp.array(c*jnp.max(jnp.abs(df_gp))).reshape((1,1))\n",
    "ell_x = c*np.max(np.abs(x))\n",
    "ell_zeigen = c*np.max(np.abs(Zeigen))\n",
    "# ell1_gp_comb = c*np.max(np.abs(df1_gp))\n",
    "# ell2_gp_comb = c*np.max(np.abs(df1_gp))\n",
    "# ell2_gp_comb = c*np.max(np.abs(df2_gp))\n",
    "print(ell_x, ell_zeigen)\n",
    "# print(type(ell2_gp_comb), ell2_gp_comb)\n",
    "print(df1_gp.shape[-1])\n",
    "print(df2_gp.shape[-1])\n",
    "print(m)"
   ],
   "id": "4aa89fa9089f3032",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "m=15\n",
    "def fit_mcmc(\n",
    "    seed: int,\n",
    "    model: callable,\n",
    "    num_warmup: int = 1000,\n",
    "    num_samples: int = 4000,\n",
    "    target_accept_prob: float = 0.8,\n",
    "    init_strategy: callable = numpyro.infer.init_to_uniform,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    rng_key = random.PRNGKey(seed)\n",
    "    kernel = NUTS(\n",
    "        model, target_accept_prob=target_accept_prob, init_strategy=init_strategy\n",
    "    )\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=num_warmup,\n",
    "        num_samples=num_samples,\n",
    "        num_chains=4,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "    mcmc.run(rng_key, **model_kwargs)\n",
    "    return mcmc\n",
    "\n",
    "\n",
    "# HSGP full\n",
    "# hsgp_full = HSGPModel(m=m, L1=ell1_gp_comb, L2=ell2_gp_comb)\n",
    "# hsgp_full = HSGPModel(m=m, L1=ell1_gp_comb)\n",
    "hsgp_full = HSGPModel(m=m, L1=[2,ell_x], L2=[2,ell_zeigen])\n",
    "\n",
    "hsgp_mcmc = fit_mcmc(\n",
    "        2,\n",
    "        hsgp_full.model,\n",
    "        df1=df1_gp,\n",
    "        df2=df2_gp,\n",
    "        y=Y,\n",
    "        num_warmup=2000,\n",
    "        num_samples=4000,\n",
    "        target_accept_prob=0.95,\n",
    "        # init_strategy=numpyro.infer.init_to_median(num_samples=25),\n",
    "    )\n",
    "\n",
    "# idata_hsgp = az.from_numpyro(posterior=hsgp_mcmc)\n",
    "hsgp_mcmc.print_summary()\n",
    "# \n",
    "# VAR_NAMES = [\"amplitude\", \"lengthscale\", \"noise\"]\n",
    "# axes = az.plot_trace(\n",
    "#         data=idata_hsgp,\n",
    "#         var_names=VAR_NAMES,\n",
    "#         kind=\"rank_bars\",\n",
    "#         backend_kwargs={\"figsize\": (10, 7), \"layout\": \"constrained\"})"
   ],
   "id": "f5f889964abcbd67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# HSGP full pred\n",
    "\n",
    "def hsgp_full_pred(\n",
    "    seed: int,\n",
    "    model: callable,\n",
    "    mcmc: MCMC,\n",
    "    **model_kwargs,\n",
    ") -> dict[str, jax.Array]:\n",
    "    samples = mcmc.get_samples()\n",
    "    predictive = Predictive(model, samples, parallel=True)\n",
    "    return predictive(seed, **model_kwargs)\n",
    "\n",
    "# post_y_hsgp = hsgp_full_pred(rng_key_, hsgp_full.model, hsgp_mcmc, df1=df1_gp, df2=df2_gp)[\"y_test\"]\n",
    "post_y_hsgp = hsgp_full_pred(rng_key_, hsgp_full.model, hsgp_mcmc, df1=df1_gp, df2=df2_gp)[\"f_star\"]\n",
    "q025 = np.quantile(post_y_hsgp,0.025,axis=0)\n",
    "q975 = np.quantile(post_y_hsgp,0.975,axis=0)\n",
    "mean_mu_hat = post_y_hsgp.mean(axis=0)\n",
    "\n",
    "print(\"MAE (point): \", np.mean(np.abs(mean_mu_hat - train_esti)),\n",
    "      \"MAE (all): \", np.mean(np.abs(post_y_hsgp - train_esti)),\n",
    "      \"RMSE: \", np.sqrt(np.mean((mean_mu_hat - train_esti)**2)),\n",
    "      \"mean_ind_coverage: \", np.mean((train_esti > q025) & (train_esti < q975)))\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.scatter(train_esti,mean_mu_hat, c=Z)\n",
    "# add abline y=x\n",
    "plt.axline((0, 0), slope=1, color=\"blue\", linestyle=\"--\")   \n",
    "# plt.plot(train_esti,mean_mu_hat, \"o\")\n",
    "plt.fill_between(train_esti, q025, q975, alpha=0.5)\n",
    "# plt.scatter(Z,mean_mu_hat, c=Z)\n"
   ],
   "id": "7604e4a46cbc0f62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a modified model for prediction\n",
    "def gp_pred_model(df1, df2):\n",
    "    return numpyro.handlers.substitute(hsgp_full.model, data={\"df1\": df1,\"df2\" : df2})(df1, df2)\n",
    "\n",
    "# Use Predictive with the modified model\n",
    "predictive_test_gp = Predictive(gp_pred_model, hsgp_mcmc.get_samples())\n"
   ],
   "id": "3389295f5aefc816",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# start = time.time()\n",
    "kernel_gp_outcome = NUTS(HSGP_model_full, target_accept_prob=0.9)\n",
    "mcmc_GP_network_true = MCMC(kernel_gp_outcome, num_warmup=2000, num_samples=4000,num_chains=4)\n",
    "# mcmc_network_true = MCMC(kernel_outcome, num_warmup=100, num_samples=10,num_chains=1)\n",
    "# mcmc_network_true.run(rng_key_,Y=Y,Z=Z,A=obs_mat,n=n)\n",
    "# mcmc_network_true.run(rng_key_,Y=Y,Z=Z,X=x,A=mat,n=n)\n",
    "mcmc_GP_network_true.run(rng_key_,df1=df1_gp, df2=df2_gp, ell1=[c,ell_x], ell2=[c,ell_zeigen],m=m, y=Y)\n",
    "# mcmc_GP_network_true.run(rng_key_,Xgp=Zeigen, Xlin=Xlin, Y=Y, ell=ell ,m=m)\n",
    "mcmc_GP_network_true.print_summary()\n",
    "# idata_gp_true = az.from_numpyro(posterior=mcmc_GP_network_true)\n",
    "\n",
    "samples_GP_net_true = mcmc_GP_network_true.get_samples()\n",
    "# print(time.time() - start)\n"
   ],
   "id": "cf714362b44ab1d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HSGP_true_predictive = Predictive(HSGP_model_full, samples_GP_net_true, exclude_deterministic=False)\n",
    "# def gp_pred_model(**kwargs):\n",
    "#     # print(\"Arguments received in gp_pred_model:\", kwargs)\n",
    "#     return numpyro.handlers.substitute(HSGP_model_full,\n",
    "#                        data=kwargs)(**kwargs)\n",
    "\n",
    "def gp_pred_model(df1, df2, ell1, ell2, m):\n",
    "    # print(\"Arguments received in gp_pred_model:\", kwargs)\n",
    "    return numpyro.handlers.substitute(HSGP_model_full,\n",
    "                      data={\"df1\": df1,\"df2\" : df2, \"ell1\" : ell1, \"ell2\" : ell2, \"m\" : m})(df1, df2, ell1, ell2, m)\n",
    "# Use Predictive with the modified model\n",
    "# predictive_test_gp_f = Predictive(gp_pred_model, samples_GP_net_true, exclude_deterministic=False)\n",
    "predictive_test_gp_f = Predictive(gp_pred_model, samples_GP_net_true, exclude_deterministic=False)\n",
    "\n",
    "# def gp_pred_model_wrapper(**kwargs):\n",
    "#     df1 = kwargs.get('df1')\n",
    "#     df2 = kwargs.get('df2')\n",
    "#     ell1 = kwargs.get('ell1')\n",
    "#     ell2 = kwargs.get('ell2')\n",
    "#     m = kwargs.get('m')\n",
    "#     return HSGP_model_full(df1, df2, ell1, ell2, m)\n",
    "\n",
    "# Use Predictive with the wrapper function\n",
    "# predictive_test_gp_f = Predictive(gp_pred_model_wrapper, samples_GP_net_true, exclude_deterministic=False)\n",
    "\n",
    "# When calling the predictive function\n",
    "# predictions = predictive_test_gp_f(rng_key_, df1=df1_gp, df2=df2_gp, ell1=[2,ell_x], ell2=[2,ell_zeigen], m=m)\n",
    "# print(predictions.keys())\n"
   ],
   "id": "fb7967c526680232",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "def compute_f_star(df, ell, amplt, length, beta, m):\n",
    "    dim = df.shape[-1] if df.ndim > 1 else 1\n",
    "    phi_new = eigenfunctions(x=df, ell=ell, m=m)\n",
    "\n",
    "    def compute_single(alpha, length, beta):\n",
    "        spd_post = jnp.sqrt(\n",
    "            diag_spectral_density_squared_exponential(\n",
    "                alpha=alpha, length=length, ell=ell, m=m, dim=dim\n",
    "            )\n",
    "        )\n",
    "        return phi_new @ (spd_post * beta)\n",
    "    \n",
    "    # Vectorize the computation across the first dimension of amplt and beta\n",
    "    compute_single_vectorized = jax.vmap(compute_single, in_axes=(0, 0, 0))\n",
    "    f_res = compute_single_vectorized(amplt, length, beta)\n",
    "\n",
    "    return f_res\n",
    "\n",
    "def manual_gp_f_star_pred(df1, df2, ell1 ,ell2, m, post_samples):\n",
    "    f_1 = compute_f_star(df1, ell1,\n",
    "                         post_samples[\"amplitude1\"],\n",
    "                         post_samples[\"lengthscale1\"],\n",
    "                         post_samples[\"beta1\"], m)\n",
    "    f_2 = compute_f_star(df2, ell2, \n",
    "                        post_samples[\"amplitude2\"],\n",
    "                        post_samples[\"lengthscale2\"],\n",
    "                        post_samples[\"beta2\"], m)\n",
    "    return f_1 + f_2 \n",
    "    # return f_1 + f_2 + post_samples[\"intercept\"][:,np.newaxis]\n",
    "\n",
    "f_star_pred = manual_gp_f_star_pred(df1_gp, df2_gp,\n",
    "                                    [c,ell_x], [c,ell_zeigen],\n",
    "                                    m, samples_GP_net_true)\n",
    "print(f_star_pred.shape)\n",
    "    "
   ],
   "id": "1ff8aa4c1539f6bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# f_star_pred = HSGP_true_predictive(rng_key_, df1=df1_gp, df2=df2_gp, ell1=[2,ell_x], ell2=[2,ell_zeigen],m=m)[\"Y\"]\n",
    "# f_star_pred2 = HSGP_true_predictive(rng_key_, df1=df1_gp, df2=df2_gp, ell1=[2,ell_x], ell2=[2,ell_zeigen],m=m)[\"f_star\"]\n",
    "f_star_pred = manual_gp_f_star_pred(df1_gp, df2_gp,\n",
    "                                    [c,ell_x], [c,ell_zeigen],\n",
    "                                    m, samples_GP_net_true)\n",
    "# post_y_hsgp_f = HSGP_true_predictive(rng_key_, df1=df1_gp, df2=df2_gp, ell1=[2,ell_x], ell2=[2,ell_zeigen],m=m)[\"Y\"]\n",
    "# post_y_hsgp_f = HSGP_true_predictive(rng_key_, df1=df1_gp, df2=df2_gp, ell1=[2,ell_x], ell2=[2,ell_zeigen],m=m)[\"f_star\"]\n",
    "# post_y_hsgp_f3 = predictive_test_gp_f(rng_key_, df1=df1_gp, df2=df2_gp, ell1=[2,ell_x], ell2=[2,ell_zeigen],m=m)[\"f_star\"]\n",
    "# post_y_hsgp_f = predictive_test_gp_f(rng_key_, df1=df1_gp, df2=df2_gp, ell1=[2,ell_x], ell2=[2,ell_zeigen],m=m)[\"f_star\"]\n",
    "q025 = np.quantile(f_star_pred,0.025,axis=0)\n",
    "q975 = np.quantile(f_star_pred,0.975,axis=0)\n",
    "# mean_mu_hat = post_y_hsgp_f.mean(axis=0)\n",
    "mean_mu_hat = f_star_pred.mean(axis=0)\n",
    "\n",
    "print(\"MAE (point): \", np.mean(np.abs(mean_mu_hat - train_esti)),\n",
    "      \"MAE (all): \", np.mean(np.abs(f_star_pred - train_esti)),\n",
    "      # \"MAE (all): \", np.mean(np.abs(post_y_hsgp - train_esti)),\n",
    "      \"RMSE: \", np.sqrt(np.mean((mean_mu_hat - train_esti)**2)),\n",
    "      \"mean_ind_coverage: \", np.mean((train_esti > q025) & (train_esti < q975)),\n",
    "      \"mean true estimand: \", np.mean(train_esti),\n",
    "        \"mean posterior mean: \", np.mean(mean_mu_hat),\n",
    "        \"mean CI length: \", np.mean(q975 - q025),\n",
    "            \"q025 estimand: \", np.quantile(f_star_pred.mean(axis=1),0.025),\n",
    "      \"q975 estimand: \", np.quantile(f_star_pred.mean(axis=1),0.975))\n",
    "      \n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.scatter(train_esti,mean_mu_hat, c=Z)\n",
    "# plt.scatter(f_star_pred2.mean(axis=0),mean_mu_hat, c=Z)\n",
    "# add abline y=x\n",
    "plt.axline((0, 0), slope=1, color=\"blue\", linestyle=\"--\")   \n",
    "# plt.plot(train_esti,mean_mu_hat, \"o\")\n",
    "plt.fill_between(train_esti, q025, q975, alpha=0.5)\n",
    "# plt.scatter(Z,mean_mu_hat, c=Z)\n"
   ],
   "id": "5e972d60082ef99f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df1_t = jnp.transpose(jnp.array([z_samp, x]))\n",
    "df2_t = jnp.transpose(jnp.array([z_samp, zeigen_sample]))\n",
    "\n",
    "df1_t2 = jnp.transpose(jnp.array([z_samp2, x]))\n",
    "df2_t2 = jnp.transpose(jnp.array([z_samp2, zeigen_sample2]))\n",
    "# post_y_hsgp_f = HSGP_true_predictive(rng_key_, df1=df1_gp, df2=df2_gp, ell1=[2,ell_x], ell2=[2,ell_zeigen],m=m)[\"Y\"]\n",
    "# post_y_hsgp_f = HSGP_true_predictive(rng_key_, df1=df1_t, df2=df2_t, ell1=[2,ell_x], ell2=[2,ell_zeigen],m=m)[\"Y\"]\n",
    "# post_y_hsgp_f = predictive_test_gp_f(rng_key_, df1=df1_t, df2=df2_t, ell1=[2,ell_x], ell2=[2,ell_zeigen],m=m)[\"f_star\"]\n",
    "# post_y_hsgp_f = predictive_test_gp_f(rng_key_, df1=df1_t, df2=df2_t, ell1=[2,ell_x], ell2=[2,ell_zeigen],m=m)[\"f_star\"]\n",
    "mu_hat1 = manual_gp_f_star_pred(df1_t, df2_t,\n",
    "                                    [c,ell_x], [c,ell_zeigen],\n",
    "                                    m, samples_GP_net_true)\n",
    "mu_hat2 = manual_gp_f_star_pred(df1_t2, df2_t2,\n",
    "                                    [c,ell_x], [c,ell_zeigen],\n",
    "                                    m, samples_GP_net_true)\n",
    "# post_y_hsgp_f = mu_hat2 - mu_hat1\n",
    "post_y_hsgp_f = mu_hat1\n",
    "\n",
    "q025 = np.quantile(post_y_hsgp_f,0.025,axis=0)\n",
    "q975 = np.quantile(post_y_hsgp_f,0.975,axis=0)\n",
    "mean_mu_hat = post_y_hsgp_f.mean(axis=0)\n",
    "\n",
    "print(mean_mu_hat[0], q025[0], q975[0], esti1[0])\n",
    "print(\"mean CI length is \", np.mean(q975 - q025))\n",
    "print(\"MAE (point): \", np.mean(np.abs(mean_mu_hat - esti1)),\n",
    "      \"MAE (all): \", np.mean(np.abs(post_y_hsgp_f - esti1)),\n",
    "      \"RMSE: \", np.sqrt(np.mean((mean_mu_hat - esti1)**2)),\n",
    "      \"mean_ind_coverage: \", np.mean((esti1 > q025) & (esti1 < q975)),\n",
    "      \"mean true estimand: \", np.mean(esti1),\n",
    "        \"mean posterior mean: \", np.mean(mean_mu_hat),\n",
    "        \"mean CI length: \", np.mean(q975 - q025),\n",
    "      \"q025 estimand: \", np.quantile(post_y_hsgp_f.mean(axis=1),0.025),\n",
    "      \"q975 estimand: \", np.quantile(post_y_hsgp_f.mean(axis=1),0.975))\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "# plt.scatter(f_star_pred.mean(axis=0),post_y_hsgp_f.mean(axis=0), c=Z)\n",
    "plt.scatter(esti1,mean_mu_hat, c=Z)\n",
    "# add abline y=x\n",
    "plt.axline((0, 0), slope=1, color=\"blue\", linestyle=\"--\")   \n",
    "# plt.plot(train_esti,mean_mu_hat, \"o\")\n",
    "# plt.fill_between(train_esti, q025, q975, alpha=0.5)\n",
    "# plt.scatter(Z,mean_mu_hat, c=Z)\n"
   ],
   "id": "33ae717ddc8e36f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "df1_t = jnp.transpose(jnp.array([z_samp, x]))\n",
    "df2_t = jnp.transpose(jnp.array([z_samp, zeigen_sample]))\n",
    "test_y_hsgp = hsgp_full_pred(rng_key_, hsgp_full.model, hsgp_mcmc, df1=df1_t, df2=df2_t)[\"y_test\"]\n",
    "# test_y_hsgp = hsgp_full_pred(rng_key_, hsgp_full.model, hsgp_mcmc, df1=df1_h, df2=df2_h)[\"f2_star\"]\n",
    "q025 = np.quantile(test_y_hsgp,0.025,axis=0)\n",
    "q975 = np.quantile(test_y_hsgp,0.975,axis=0)\n",
    "mean_mu_hat = test_y_hsgp.mean(axis=0)\n",
    "\n",
    "print(\"MAE (point): \", np.mean(np.abs(mean_mu_hat - samp_esti)),\n",
    "      \"MAE (all): \", np.mean(np.abs(test_y_hsgp - samp_esti)),\n",
    "      \"RMSE: \", np.sqrt(np.mean((mean_mu_hat - samp_esti)**2)),\n",
    "      \"mean_ind_coverage: \", np.mean((samp_esti > q025) & (samp_esti < q975)),\n",
    "      \"mean true estimand: \", np.mean(samp_esti),\n",
    "        \"mean posterior mean: \", np.mean(mean_mu_hat))\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.scatter(samp_esti,mean_mu_hat, c = z_samp)\n",
    "plt.axline((0, 0), slope=1, color=\"blue\", linestyle=\"--\")   \n",
    "# plt.plot(zeigen_sample, mean_mu_hat, \"o\")"
   ],
   "id": "12f893f5cff6fd33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "num_warmup=2000\n",
    "# num_warmup=2000\n",
    "# num_warmup=250\n",
    "# num_samples=25\n",
    "num_samples=4000\n",
    "# num_samples=4000\n",
    "num_chains=4\n",
    "\n",
    "@jit    \n",
    "def HSGP_mod_model_samples(key, Y, df1, df2, ell1, ell2):\n",
    "    kernel_hsgp = NUTS(HSGP_model_full, target_accept_prob=0.9)\n",
    "    hsgp_mcmc = MCMC(kernel_hsgp, num_warmup=num_warmup, num_samples=num_samples,num_chains=num_chains, progress_bar=False, chain_method=\"vectorized\")\n",
    "    # mcmc.run(key, Y=Y, Z=Z, X=X, A=A, n=NN)\n",
    "    hsgp_mcmc.run(key, df1=df1, df2=df2, ell1=ell1, ell2=ell2 ,m=m, y=Y)\n",
    "    # hsgp_mcmc.print_summary()\n",
    "    # samples = hsgp_mcmc.get_samples()\n",
    "    # hsgp_pred = Predictive(HSGP_model, hsgp_mcmc.get_samples())\n",
    "    # return hsgp_pred\n",
    "    return hsgp_mcmc.get_samples()\n",
    "\n",
    "@jit\n",
    "def HSGP_jit_pred(post_samples, df1, df2, ell1, ell2):\n",
    "    pred_func = Predictive(HSGP_model_full, post_samples)\n",
    "    return pred_func(rng_key, df1=df1, df2=df2, ell1=ell1, ell2=ell2 ,m=m)\n"
   ],
   "id": "73b99df86aaec316",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# hsgp_full_samples = HSGP_mod_model_samples(rng_key_,Y=Y, df1=df1_gp, df2=df2_gp, ell1=ell1_gp_comb, ell2=ell2_gp_comb)\n",
    "hsgp_full_samples = HSGP_mod_model_samples(rng_key_,Y=Y, df1=df1_gp, df2=df2_gp, ell1=[1,ell1_gp_comb], ell2=[1,ell2_gp_comb],)\n"
   ],
   "id": "2ad75a664347ce0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z_samp = rng.binomial(n=1, p=0.7, size=n)\n",
    "zeigen_sample = zeigen_value(z_samp, eigen_center, mat)\n",
    "df1_h = jnp.transpose(jnp.array([z_samp, x]))\n",
    "df2_h = jnp.transpose(jnp.array([z_samp, zeigen_sample]))\n",
    "dflin = jnp.transpose(jnp.array([[1]*n, z_samp ,x, x2]))\n",
    "y_samp, eps = gen_y(dflin, zeigen_sample, alpha, lin=False)\n",
    "samp_esti = y_samp - eps\n",
    "print(ell2_gp_comb, np.max(np.abs(zeigen_sample)))\n",
    "\n",
    "# post_y_hsgp = HSGP_jit_pred(hsgp_full_samples, df1=df1_h, df2=df2_h, ell1=[ell1_gp_comb]*2, ell2=[ell2_gp_comb]*2)[\"f_star\"]\n",
    "post_y_hsgp = HSGP_jit_pred(hsgp_full_samples, df1=df1_gp, df2=df2_gp, ell1=[ell1_gp_comb]*2, ell2=[ell2_gp_comb]*2)[\"f_star\"]\n",
    "q025 = np.quantile(post_y_hsgp,0.025,axis=0)\n",
    "q975 = np.quantile(post_y_hsgp,0.975,axis=0)\n",
    "mean_mu_hat = post_y_hsgp.mean(axis=0)\n",
    "samp_esti = train_esti\n",
    "\n",
    "print(\"MAE (point): \", np.mean(np.abs(mean_mu_hat - samp_esti)),\n",
    "      \"MAE (all): \", np.mean(np.abs(post_y_hsgp - samp_esti)),\n",
    "      \"RMSE: \", np.sqrt(np.mean((mean_mu_hat - samp_esti)**2)),\n",
    "      \"mean_ind_coverage: \", np.mean((samp_esti > q025) & (samp_esti < q975)))\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(samp_esti,mean_mu_hat, \"o\")\n"
   ],
   "id": "8cdf44a9d55a148d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# h_x = ((x > 1.5) | (x < -1.5)).astype(int)\n",
    "# h_x2 = ((x > 2) | (x < 2)).astype(int)\n",
    "\n",
    "def Dynamic_intervention(x, threshold = 1.5):\n",
    "    return np.where((x > threshold) | (x < -threshold), 1, 0)\n",
    "    # return ((x > threshold) | (x < threshold)).astype(int)\n",
    "\n",
    "def Stochastic_interention(alph, n_approx = 1e4):\n",
    "    return rng.binomial(n=1, p=alph, size=(n_approx,n))\n",
    "# \n",
    "# # h_x = np.array([1]*n)\n",
    "# h_x = rng.binomial(n=1,p=0.6,size=n)\n",
    "# h_x2 = rng.binomial(n=1,p=0.2,size=n)\n",
    "\n",
    "# h_1 = np.where(((x >= -1) & (x <= 1)) | (x2 == 1), 1, 0)\n",
    "# h_2 = np.where(x2 == 1, 1, 0)\n",
    "# h_1 = Dynamic_intervention(x)\n",
    "h_1 = Dynamic_intervention(x)\n",
    "h_2 = Dynamic_intervention(x, threshold=2)\n",
    "Z_stoch = Stochastic_interention(alph=0.7, n_approx=100)\n",
    "Z_stoch2 = Stochastic_interention(alph=0.3, n_approx=100)\n",
    "Z_all = np.array([1]*n)\n",
    "Z_none = np.array([0]*n)\n",
    "\n",
    "h_zeigen = zeigen_value(h_1, eigen_center, mat)\n",
    "h2_zeigen = zeigen_value(h_2, eigen_center, mat)\n",
    "h_zeigen_obs = zeigen_value(h_1, obs_eigen_cent, obs_mat)\n",
    "h2_zeigen_obs = zeigen_value(h_2, obs_eigen_cent, obs_mat)\n",
    "\n",
    "Stoch_zeigen = zeigen_value(Z_stoch, eigen_center, mat)\n",
    "Stoch_zeigen2 = zeigen_value(Z_stoch2, eigen_center, mat)\n",
    "\n",
    "all_zeigen = zeigen_value(Z_all, eigen_center, mat)\n",
    "none_zeigen = zeigen_value(Z_none, eigen_center, mat)\n",
    "\n",
    "print(h_1.shape)\n",
    "print(h_zeigen.shape)\n",
    "print(Z_stoch.shape)\n",
    "print(Z_stoch[0,].shape)\n",
    "print(Z_stoch[0].shape)\n",
    "print(Stoch_zeigen.shape)\n",
    "# print(np.mean(Stoch_zeigen,axis=1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6778b1facc18b955",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# X_h_true = jnp.transpose(jnp.array([[1]*n, h_1, x, h_zeigen]))\n",
    "# X_h_obs = jnp.transpose(jnp.array([[1]*n, h_1, x, h_zeigen_obs]))\n",
    "X_h_true = jnp.transpose(jnp.array([[1]*n, h_1, x, x2, h_zeigen]))\n",
    "X_h_obs = jnp.transpose(jnp.array([[1]*n, h_1, x, x2, h_zeigen_obs]))\n",
    "\n",
    "ell_h = jnp.array(c*jnp.max(jnp.abs(h_zeigen))).reshape(1,1)\n",
    "ell_h_obs = jnp.array(c*jnp.max(jnp.abs(h_zeigen_obs))).reshape(1,1)\n",
    "\n",
    "# Y_test,epsi_test = gen_y(X_h_true[:,0:3], h_zeigen, alpha, lin=False)\n",
    "# Y_test,epsi_test = gen_y(X_h_true[:,0:3], h_zeigen, alpha, lin=False)\n",
    "Y_test, epsi_test = gen_y(X_h_true[:,0:4], h_zeigen, alpha, lin=False)\n",
    "# Y_test2,_ = gen_y(X_test_true2[:,0:3], true_h_x_eigen2,alpha,lin=False)\n",
    "\n",
    "mu_test = Y_test - epsi_test\n",
    "true_estimand = np.mean(mu_test)\n",
    "print(true_estimand)\n",
    "# true_ce = Y_test - Y_test2\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.scatter(h_zeigen,mu_test, c=h_1)\n",
    "# plt.scatter(mean_y,mu_test, c =h_x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f95bf009fb5931d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# @jit\n",
    "def get_true_estimand(z_new, zeigen_new , x,x2, alpha, lin=True):\n",
    "    n = x.shape[0]\n",
    "    if z_new.ndim == 2:\n",
    "        n_stoch = z_new.shape[0]\n",
    "        # results = np.zeros((n_stoch,1))\n",
    "        results = np.zeros((n_stoch,n))\n",
    "        for i in range(n_stoch):\n",
    "            df = np.transpose(jnp.array([[1]*n, z_new[i,], x, x2]))\n",
    "            y, epsi = gen_y(df, zeigen_new[i,], alpha, lin=lin)\n",
    "            # results[i,] = np.mean(y-epsi)\n",
    "            results[i,] = y-epsi\n",
    "        return np.mean(results, axis=0).squeeze()\n",
    "        # return results\n",
    "    else:\n",
    "        # assert Z_stoch.ndim == 1\n",
    "        df = np.transpose(jnp.array([[1]*n, z_new, x, x2]))\n",
    "        y, epsi = gen_y(df, zeigen_new, alpha, lin=lin)\n",
    "        # return np.mean(y-epsi)\n",
    "        return y-epsi\n",
    "    \n",
    "stoch_estimand = get_true_estimand(Z_stoch, Stoch_zeigen, x,x2, alpha, False)\n",
    "# print(stoch_estimand)\n",
    "print(stoch_estimand.shape)\n",
    "print(np.mean(stoch_estimand))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e080ab51c9c2fd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "hestimand = get_true_estimand(h_1, h_zeigen, x, x2, alpha, False)\n",
    "# print(hestimand)\n",
    "print(hestimand.shape)\n",
    "print(np.mean(hestimand))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32a320ee0af2433a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_esti = get_true_estimand(Z_all, all_zeigen, x,x2,  alpha, False)\n",
    "none_esti = get_true_estimand(Z_none, none_zeigen, x,x2,  alpha, False)\n",
    "gte = all_esti - none_esti\n",
    "# print(all_esti, none_esti, gte)\n",
    "print(all_esti.shape, none_esti.shape, gte.shape)\n",
    "print(all_esti.mean(), none_esti.mean(), gte.mean())\n"
   ],
   "id": "ea5e50faec1856d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "h1_esti = get_true_estimand(h_1, h_zeigen, x,x2,  alpha, False)\n",
    "h2_esti = get_true_estimand(h_2, h2_zeigen, x, x2, alpha, False)\n",
    "hte = h1_esti - h2_esti\n",
    "print(h1_esti.shape, h2_esti.shape, hte.shape)\n",
    "print(h1_esti.mean(), h2_esti.mean(), hte.mean())"
   ],
   "id": "83dfa64231743943",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stoch_esti = get_true_estimand(Z_stoch, Stoch_zeigen, x,x2,  alpha, False)\n",
    "stoch_esti2 = get_true_estimand(Z_stoch2, Stoch_zeigen2, x,x2,  alpha, False)\n",
    "te_stoch = stoch_esti - stoch_esti2\n",
    "print(stoch_esti.shape, stoch_esti2.shape, te_stoch.shape)\n",
    "print(stoch_esti.mean(), stoch_esti2.mean(), te_stoch.mean())"
   ],
   "id": "150b27b9167f1d48",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_error_stats(esti_post_draws, true_estimand, method=\"TEST\", idx=None):\n",
    "    # esti_post_draws has shape (M, N)\n",
    "    # true_estimand has shape (N,)\n",
    "    \n",
    "    mean_estimand = np.mean(true_estimand) # scalar\n",
    "    mean_units = np.mean(esti_post_draws, axis=1) # shape (M,)\n",
    "    mean_samples = np.mean(esti_post_draws, axis=0) # shape (N,)\n",
    "    mean_all = np.round(np.mean(esti_post_draws),3) # scalar\n",
    "    # medi = np.round(np.median(esti_post_draws),3) \n",
    "    medi = np.round(np.median(mean_units),3) \n",
    "    # std = np.round(np.std(esti_post_draws),3)\n",
    "    std = np.round(np.std(mean_units),3)\n",
    "    # RMSE_all = np.round(np.sqrt(np.mean(np.power(mean_units - mean_estimand, 2))),3)\n",
    "    # RMSE_ind = np.round(np.sqrt(np.mean(np.power(esti_post_draws - true_estimand, 2))),3)\n",
    "    RMSE = np.round(np.sqrt(np.mean(np.power(mean_samples - true_estimand, 2))),3)\n",
    "    # MAE_all = np.round(np.mean(np.abs(mean_units - mean_estimand)),3)\n",
    "    # MAE_ind = np.round(np.mean(np.abs(esti_post_draws - true_estimand)),3)\n",
    "    MAE_point = np.round(np.mean(np.abs(mean_samples - true_estimand)),3)\n",
    "    MAE_all = np.round(np.mean(np.abs(esti_post_draws - true_estimand)),3)\n",
    "    MAPE_all = np.round(np.mean(np.abs((mean_units - mean_estimand) / mean_estimand)),3)\n",
    "    # MAPE_ind = np.round(np.mean(np.abs((esti_post_draws - true_estimand) / true_estimand)),3)\n",
    "    # MAPE_ind2 = np.round(np.mean(np.abs((mean_samples - true_estimand) / true_estimand)),3)\n",
    "    # MAPE_ind = np.round(np.mean(np.abs(esti_post_draws - true_estimand)/np.abs(true_estimand)),3)\n",
    "    # q025 = np.quantile(esti_post_draws, 0.025)\n",
    "    q025 = np.quantile(mean_units, 0.025)\n",
    "    q025_ind = np.quantile(esti_post_draws, 0.025, axis=0)\n",
    "    # q975 = np.quantile(esti_post_draws, 0.975)\n",
    "    q975 = np.quantile(mean_units, 0.975)\n",
    "    q975_ind = np.quantile(esti_post_draws, 0.975, axis=0)\n",
    "    cover = q025 <= mean_estimand <= q975\n",
    "    mean_cover = (q025_ind <= true_estimand) & (true_estimand <= q975_ind)\n",
    "    return pd.DataFrame([{\"idx\" : idx, \"method\" : method,\n",
    "            \"mean\" : mean_all, \"median\" : medi, \"true\" : np.round(mean_estimand,3), \n",
    "            \"bias\" : np.round(mean_all - mean_estimand,3), \"std\" : std, \n",
    "            \"RMSE\" : RMSE, \"MAE_point\" : MAE_point, \"MAE_all\": MAE_all , \"MAPE_all\" : MAPE_all,\n",
    "                          \"q025\" : np.round(q025,3), \"q975\" : np.round(q975,3), \n",
    "              \"covering\" : cover, \"cover_ind_mean\" : np.mean(mean_cover)}])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7910df5f081f0d15",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "@jit\n",
    "def linear_model_outcome_pred(z, zeigen, post_samples, x, x2):\n",
    "    # df = jnp.transpose(jnp.array([[1]*x.shape[0], z, x, x2, zeigen]))\n",
    "    df = jnp.transpose(jnp.array([[1]*x.shape[0], z, x, zeigen]))\n",
    "    # pred = outcome_jit_pred(post_samples, df)\n",
    "    # return jnp.mean(pred[\"Y\"], axis=0)\n",
    "    # return pred[\"Y\"]\n",
    "    # return pred[\"mu_star\"]\n",
    "    return outcome_jit_pred(post_samples, df)[\"mu_star\"]\n",
    "\n",
    "linear_model_pred = jax.vmap(linear_model_outcome_pred, in_axes= (0, 0, None, None, None))\n",
    "\n",
    "def linear_pred(z, zeigen, post_samples, x,x2):\n",
    "    if z.ndim == 2:\n",
    "        # return linear_model_pred(z, zeigen, post_samples, x, x2)\n",
    "        return np.mean(linear_model_pred(z, zeigen, post_samples, x, x2), axis=0)\n",
    "        # return linear_model_pred(z, zeigen, post_samples, x)\n",
    "    if z.ndim == 1:\n",
    "        n_z = z.shape[0]\n",
    "        return linear_model_pred(z.reshape((1,n_z)), zeigen.reshape((1,n_z)), post_samples, x,x2).squeeze()\n",
    "        # return linear_model_pred(z.reshape((1,n_z)), zeigen.reshape((1,n_z)), post_samples, x)\n",
    "        \n",
    "@jit\n",
    "def hsgp_model_outcome_pred(z, zeigen, post_samples, x, x2, ell1, ell2):\n",
    "    # ell_ = jnp.array(c*jnp.max(jnp.abs(zeigen))).reshape(1,1)\n",
    "    # df = jnp.transpose(jnp.array([[1]*x.shape[0], z, x, x2, zeigen]))\n",
    "    # df = jnp.transpose(jnp.array([[1]*x.shape[0], z, x, zeigen]))\n",
    "    df1 = jnp.transpose(jnp.array([z, x]))\n",
    "    df2 = jnp.transpose(jnp.array([z, zeigen]))\n",
    "    # pred = HSGP_jit_pred(post_samples, Xgp=df[:,4:], Xlin = df[:,0:4], ell=ell)\n",
    "    # pred = HSGP_jit_pred(post_samples, Xgp=df[:,3:], Xlin = df[:,0:3], ell=ell)\n",
    "    # pred = HSGP_jit_pred(post_samples, df1=df1, df2=df2, ell1=ell1, ell2=ell2)\n",
    "    # return jnp.mean(pred[\"Y\"], axis=1)\n",
    "    # return jnp.mean(pred[\"Y\"], axis=0)\n",
    "    # return pred[\"Y\"]\n",
    "    return HSGP_jit_pred(post_samples, df1=df1, df2=df2, ell1=ell1, ell2=ell2)[\"f_star\"]\n",
    "\n",
    "hsgp_model_pred = jax.vmap(hsgp_model_outcome_pred, in_axes= (0, 0, None, None, None, None, None))\n",
    "\n",
    "def hsgp_pred(z, zeigen, post_samples, x, x2, ell1, ell2):\n",
    "    if z.ndim == 2:\n",
    "        # return hsgp_model_pred(z, zeigen, post_samples, x,x2,  ell)\n",
    "        return np.mean(hsgp_model_pred(z, zeigen, post_samples, x,x2, ell1, ell2), axis=0)\n",
    "    if z.ndim == 1:\n",
    "        n_z = z.shape[0]\n",
    "        # return hsgp_model_pred(z.reshape((1,n_z)), zeigen.reshape((1,n_z)), post_samples, x,x2,  ell)\n",
    "        return hsgp_model_pred(z.reshape((1,n_z)), zeigen.reshape((1,n_z)), post_samples, x,x2, ell1, ell2).squeeze()\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b3ed08374c89af5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# GTE\n",
    "lin_all_pred = linear_pred(Z_all, all_zeigen, samples_net_true, x, x2)\n",
    "lin_none_pred = linear_pred(Z_none, none_zeigen, samples_net_true, x, x2)\n",
    "# lin_gte_pred = jnp.mean(lin_all_pred,axis=0) - jnp.mean(lin_none_pred, axis=0)\n",
    "lin_gte_pred = lin_all_pred - lin_none_pred \n",
    "# lin_gte_pred =  lin_none_pred \n",
    "# lin_gte_pred =  lin_all_pred \n",
    "print(lin_gte_pred[0:5,0:5])\n",
    "print(compute_error_stats(lin_gte_pred, gte))\n",
    "print(compute_error_stats(lin_all_pred, all_esti))\n"
   ],
   "id": "564a37ab35c465fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# GTE HSGP\n",
    "hsgp_all_pred = hsgp_pred(Z_all, all_zeigen, hsgp_full_samples, x, x2, [ell1_gp_comb]*2, [ell2_gp_comb]*2)   \n",
    "hsgp_none_pred = hsgp_pred(Z_none, none_zeigen, hsgp_full_samples, x,x2,  [ell1_gp_comb]*2, [ell2_gp_comb]*2)\n",
    "hsgp_gte_pred = hsgp_all_pred - hsgp_none_pred\n",
    "# hsgp_gte_pred = jnp.mean(hsgp_all_pred - hsgp_none_pred,axis=0)\n",
    "print(compute_error_stats(hsgp_gte_pred, gte))\n",
    "print(compute_error_stats(hsgp_all_pred, all_esti))"
   ],
   "id": "f533498cc968931c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# HTE\n",
    "lin_h1_pred = linear_pred(h_1, h_zeigen, samples_net_true, x,x2)\n",
    "lin_h2_pred = linear_pred(h_2, h2_zeigen, samples_net_true, x, x2)\n",
    "# lin_hte_pred = jnp.mean(lin_h1_pred,axis=0) - jnp.mean(lin_h2_pred, axis=0)\n",
    "lin_hte_pred = lin_h1_pred - lin_h2_pred\n",
    "print(compute_error_stats(lin_hte_pred, hte))\n",
    "# print(compute_error_stats(lin_h1_pred, h1_esti))\n",
    "# print(compute_error_stats(lin_h1_pred, h1_esti))"
   ],
   "id": "a474233035dd5f5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# HTE HSGP\n",
    "hsgp_h1_pred = hsgp_pred(h_1, h_zeigen, hsgp_full_samples, x, x2, [ell1_gp_comb]*2, [ell2_gp_comb]*2)\n",
    "hsgp_h2_pred = hsgp_pred(h_2, h2_zeigen, hsgp_full_samples, x,x2,  [ell1_gp_comb]*2, [ell2_gp_comb]*2)\n",
    "# hsgp_hte_pred = jnp.mean(hsgp_h1_pred,axis=0) - jnp.mean(hsgp_h2_pred, axis=0)\n",
    "hsgp_hte_pred = hsgp_h1_pred - hsgp_h2_pred\n",
    "print(compute_error_stats(hsgp_hte_pred, hte))\n",
    "# print(compute_error_stats(hsgp_h1_pred, h1_esti))\n"
   ],
   "id": "3261de6b8feda334",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# HTE Obs\n",
    "lin_h1_pred_obs = linear_pred(h_1, h_zeigen_obs, samples_net_obs, x, x2)\n",
    "lin_h2_pred_obs = linear_pred(h_2, h2_zeigen_obs, samples_net_obs, x, x2)\n",
    "# lin_hte_pred_obs = jnp.mean(lin_h1_pred_obs,axis=0) - jnp.mean(lin_h2_pred_obs, axis=0)\n",
    "lin_hte_pred_obs = lin_h1_pred_obs - lin_h2_pred_obs\n",
    "print(compute_error_stats(lin_hte_pred_obs, hte))\n",
    "print(compute_error_stats(lin_h1_pred_obs, h1_esti))\n"
   ],
   "id": "330e52b210fec074",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# HTE HSGP Obs\n",
    "hsgp_h1_pred_obs = hsgp_pred(h_1, h_zeigen_obs, samples_GP_net_obs, x, x2, ell_obs)\n",
    "hsgp_h2_pred_obs = hsgp_pred(h_2, h2_zeigen_obs, samples_GP_net_obs, x, x2, ell_obs)\n",
    "# hsgp_hte_pred_obs = jnp.mean(hsgp_h1_pred_obs,axis=0) - jnp.mean(hsgp_h2_pred_obs, axis=0)\n",
    "hsgp_hte_pred_obs = hsgp_h1_pred_obs - hsgp_h2_pred_obs\n",
    "print(compute_error_stats(hsgp_hte_pred_obs, hte))\n",
    "print(compute_error_stats(hsgp_h1_pred_obs, h1_esti))"
   ],
   "id": "7a8bbea2b121fb5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stochastic TE\n",
    "lin_stoch_pred = linear_pred(Z_stoch, Stoch_zeigen, samples_net_true, x, x2)\n",
    "lin_stoch_pred2 = linear_pred(Z_stoch2, Stoch_zeigen2, samples_net_true, x, x2)\n",
    "# lin_te_stoch_pred = jnp.mean(lin_stoch_pred,axis=0) - jnp.mean(lin_stoch_pred2, axis=0)\n",
    "lin_te_stoch_pred = lin_stoch_pred- lin_stoch_pred2\n",
    "print(compute_error_stats(lin_te_stoch_pred, te_stoch))\n",
    "print(compute_error_stats(lin_stoch_pred, stoch_esti))"
   ],
   "id": "e5b26eb768397dbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stochastic TE HSGP\n",
    "hsgp_stoch_pred = hsgp_pred(Z_stoch, Stoch_zeigen, samples_GP_net_true, x,x2,  ell)\n",
    "hsgp_stoch_pred2 = hsgp_pred(Z_stoch2, Stoch_zeigen2, samples_GP_net_true, x,x2,  ell)\n",
    "# hsgp_te_stoch_pred = jnp.mean(hsgp_stoch_pred,axis=0) - jnp.mean(hsgp_stoch_pred2, axis=0)\n",
    "hsgp_te_stoch_pred = hsgp_stoch_pred - hsgp_stoch_pred2\n",
    "print(compute_error_stats(hsgp_te_stoch_pred, te_stoch))\n",
    "print(compute_error_stats(hsgp_stoch_pred, stoch_esti))\n"
   ],
   "id": "bf2e6299cc5f3a09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stochastic TE obs\n",
    "lin_stoch_pred_obs = linear_pred(Z_stoch, Stoch_zeigen, samples_net_obs, x,x2)\n",
    "lin_stoch_pred2_obs = linear_pred(Z_stoch2, Stoch_zeigen2, samples_net_obs, x, x2)\n",
    "# lin_te_stoch_pred_obs = jnp.mean(lin_stoch_pred_obs,axis=0) - jnp.mean(lin_stoch_pred2_obs, axis=0)\n",
    "lin_te_stoch_pred_obs = lin_stoch_pred_obs - lin_stoch_pred2_obs\n",
    "print(compute_error_stats(lin_te_stoch_pred_obs, te_stoch))\n",
    "print(compute_error_stats(lin_stoch_pred_obs, stoch_esti))"
   ],
   "id": "79a130367c673c74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stochastic TE HSGP obs\n",
    "hsgp_stoch_pred_obs = hsgp_pred(Z_stoch, Stoch_zeigen, samples_GP_net_obs, x, x2, ell_obs)\n",
    "hsgp_stoch_pred2_obs = hsgp_pred(Z_stoch2, Stoch_zeigen2, samples_GP_net_obs, x, x2, ell_obs)\n",
    "# hsgp_te_stoch_pred_obs = jnp.mean(hsgp_stoch_pred_obs,axis=0) - jnp.mean(hsgp_stoch_pred2_obs, axis=0)\n",
    "hsgp_te_stoch_pred_obs = hsgp_stoch_pred_obs - hsgp_stoch_pred2_obs\n",
    "print(compute_error_stats(hsgp_te_stoch_pred_obs, te_stoch))\n",
    "print(compute_error_stats(hsgp_stoch_pred_obs, stoch_esti))"
   ],
   "id": "d63b2bd6dbdc6f1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# post_pred_true_net = true_net_lin_predictive(rng_key, X=X_h_true)\n",
    "# post_pred_true_net = linear_pred(h_1, h_zeigen, samples_net_true, x,x2)\n",
    "# post_pred_true_net = outcome_jit_pred(samples_net_true, jnp.array([[1]*n, h_1, x, h_zeigen]))\n",
    "# post_pred_true_net = outcome_jit_pred(samples_net_true, X_h_true)\n",
    "# print(post_pred_true_net.shape)\n",
    "# print(post_pred_true_net[\"mu\"].shape)\n",
    "# post_pred_true_net2 = true_net_lin_predictive(rng_key, X=X_test_true2)\n",
    "# lin_pred_true_net = np.mean(post_pred_true_net[\"mu\"],axis=0)\n",
    "# lin_pred_true_net = np.mean(post_pred_true_net,axis=0)\n",
    "# lin_pred_true_net_it = np.mean(post_pred_true_net[\"mu\"],axis=1)\n",
    "# lin_pred_true_net_it = np.mean(post_pred_true_net,axis=1)\n",
    "# print(lin_pred_true_net_it.shape)\n",
    "\n",
    "# lin_pred_true_stats = compute_error_stats(lin_pred_true_net_it, hestimand)\n",
    "# print(lin_pred_true_stats)\n",
    "# lin_hte_pred, hte\n",
    "lin_pred_true_net = np.mean(lin_hte_pred, axis=0)\n",
    "print(\"MAE:\", np.mean(np.abs(lin_pred_true_net - hte)))\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.scatter(x=lin_pred_true_net,y=hte, c=\"orange\")\n",
    "plt.axline((0,0),slope=1,ls=\"--\",alpha=0.8,lw=1.3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1e8b85cdb7c2a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# post_pred_obs = outcome_jit_pred(samples_net_obs, X_h_obs)\n",
    "# post_pred_obs = linear_pred(h_1, h_zeigen_obs, samples_net_obs, x,x2)\n",
    "# post_pred_obs = outcome_jit_pred(samples_net_obs, X_h_obs)\n",
    "# lin_pred_obs = np.mean(post_pred_obs,axis=0)\n",
    "# lin_pred_obs_it = np.mean(post_pred_obs,axis=1)\n",
    "# print(lin_pred_obs.shape, lin_pred_obs_it.shape)\n",
    "\n",
    "# lin_pred_obs_stats = compute_error_stats(np.mean(post_pred_obs[\"Y\"],axis=1), hestimand)\n",
    "# print(lin_pred_obs_stats)\n",
    "lin_pred_obs = np.mean(lin_hte_pred_obs,axis=0)\n",
    "print(\"MAE: \", np.mean(np.abs(lin_pred_obs - hte)))\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.scatter(x=lin_pred_obs,y=hte, c=\"orange\")\n",
    "plt.axline((0,0),slope=1,ls=\"--\",alpha=0.8,lw=1.3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "289c6f602ba3548d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# HSGP_posterior_predictive = HSGP_true_predictive(rng_key, Xgp=X_h_true[:,3:], Xlin = X_h_true[:,0:3],ell=ell, m=M)\n",
    "# HSGP_posterior_predictive = hsgp_pred(h_1, h_zeigen, samples_GP_net_true, x, x2, ell)\n",
    "# HSGP_posterior_predictive = HSGP_true_predictive(rng_key, Xgp=X_h_true[:,3:], Xlin = X_h_true[:,0:3],ell=ell, m=m)\n",
    "# HSGP_posterior_predictive = HSGP_jit_pred(samples_GP_net_true,Xgp=X_h_true[:,3:], Xlin = X_h_true[:,0:3],ell=ell, m=m)\n",
    "# print(HSGP_posterior_predictive[\"Y\"].shape)\n",
    "# post_mu = np.mean(HSGP_posterior_predictive,axis=0)\n",
    "# post_mu_iter = np.mean(HSGP_posterior_predictive,axis=1)\n",
    "\n",
    "# hsgp_pred_true_stats = compute_error_stats(post_mu_iter, hestimand)\n",
    "# print(hsgp_pred_true_stats)\n",
    "post_mu = np.mean(hsgp_hte_pred,axis=0)\n",
    "print(\"MAE: \", np.mean(np.abs(post_mu - hte)))\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.scatter(x=post_mu,y=hte, c=\"orange\")\n",
    "# plt.scatter(x=post_mu2,y=mu_test2, c=true_h_x_eigen2)\n",
    "plt.axline((0,0),slope=1,ls=\"--\",alpha=0.8,lw=1.3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dd24860efc085be",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# HSGP_posterior_predictive_obs = hsgp_pred(h_1, h_zeigen_obs, samples_GP_net_obs, x, x2, ell_obs)\n",
    "# # HSGP_posterior_predictive_obs = HSGP_obs_predictive(rng_key, Xgp=X_h_obs[:,3:], Xlin = X_h_obs[:,0:3], ell=ell_obs, m=M)\n",
    "# # HSGP_posterior_predictive = HSGP_jit_pred(samples_GP_net_true,Xgp=X_h_true[:,3:], Xlin = X_h_true[:,0:3],ell=ell, m=m)\n",
    "# # print(HSGP_posterior_predictive_obs[\"Y\"].shape)\n",
    "# obs_post_mu = np.mean(HSGP_posterior_predictive_obs,axis=0)\n",
    "# obs_post_mu_iter = np.mean(HSGP_posterior_predictive_obs,axis=1)\n",
    "\n",
    "# hsgp_pred_obs_stats = compute_error_stats(np.mean(HSGP_posterior_predictive_obs[\"Y\"],axis=1), hestimand)\n",
    "# print(hsgp_pred_obs_stats)\n",
    "obs_post_mu = np.mean(hsgp_hte_pred_obs,axis=0)\n",
    "print(\"MAE: \", np.mean(np.abs(obs_post_mu - hte)))\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.scatter(x=obs_post_mu,y=hte, c=\"orange\")\n",
    "# plt.scatter(x=post_mu2,y=mu_test2, c=true_h_x_eigen2)\n",
    "plt.axline((0,0),slope=1,ls=\"--\",alpha=0.8,lw=1.3)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e9c5b8ae1959a94",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "post_pred_mean = {\"beta\" : jnp.expand_dims(jnp.mean(samples_network[\"beta\"],axis=0),-2),\n",
    "                  \"gamma0\" : jnp.expand_dims(jnp.mean(samples_network[\"gamma0\"]),-1),\n",
    "                  \"gamma1\" : jnp.expand_dims(jnp.mean(samples_network[\"gamma1\"]),-1)}\n",
    "print(post_pred_mean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69c784f97661a2b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(samples_network[\"beta\"].shape)\n",
    "print(samples_network[\"gamma0\"].shape)\n",
    "post_pred_mean = {\"beta\" : jnp.expand_dims(jnp.mean(samples_network[\"beta\"],axis=0),-2),\n",
    "                  \"gamma0\" : jnp.expand_dims(jnp.mean(samples_network[\"gamma0\"]),-1),\n",
    "                  \"gamma1\" : jnp.expand_dims(jnp.mean(samples_network[\"gamma1\"]),-1)}\n",
    "# print(jnp.array(post_pred_mean))\n",
    "# The following is how to sample multiple different A* with the mean posterior of A* and A|A* parameters\n",
    "# a_star_pred1 = a_star_pred(random.PRNGKey(0), X=x_diff,TriU=triu_obs,n=n)\n",
    "# a_star_pred1 = Astar_pred(random.PRNGKey(0),post_samples=samples_network, Xd=x_diff,triu=triu_obs)\n",
    "a_star_pred1 = Astar_pred(random.PRNGKey(0),post_samples=post_pred_mean, Xd=x_diff,triu=triu_obs)\n",
    "a_star_pred2 = Astar_pred(random.PRNGKey(3),post_samples=post_pred_mean, Xd=x_diff,triu=triu_obs)\n",
    "# a_star_pred2 = Astar_pred(random.PRNGKey(),post_samples=jnp.array(post_pred_mean), Xd=x_diff,triu=triu_obs, mean_post=True)\n",
    "# a_star_pred2 = a_star_pred(random.PRNGKey(1), X=x_diff,TriU=triu_obs,n=n)\n",
    "# a_star_pred2 = a_star_pred(random.PRNGKey(1), X=x_diff,TriU=triu_obs,n=n)\n",
    "# print(a_star_pred)\n",
    "# print(a_star_pred1[\"triu_star\"])\n",
    "print(a_star_pred1[\"triu_star\"].shape)\n",
    "print(a_star_pred1[\"triu_star\"][0].shape)\n",
    "print(a_star_pred2[\"triu_star\"].shape)\n",
    "print(jnp.array_equal(a_star_pred1[\"triu_star\"],a_star_pred2[\"triu_star\"]))\n",
    "\n",
    "twoAst = pd.DataFrame({'first' : a_star_pred1[\"triu_star\"][1], 'sec' : a_star_pred2[\"triu_star\"][1]})\n",
    "# print(twoAst)\n",
    "pd.crosstab(index=twoAst['first'], columns=twoAst['sec'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c196f5a2954cdacf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "post_predictive = Predictive(network_model, samples_network, infer_discrete=True)\n",
    "# post_predictive = post_predictive(rng_key, X=x_diff, X2=x2_equal, TriU=triu_obs)\n",
    "post_predictive = post_predictive(rng_key, X=x_diff, X2=x2_or, TriU=triu_obs)\n",
    "# samples_network[\"triu_star\"] = post_predictive[\"triu_star\"]\n",
    "# print(post_predictive[\"triu_star\"])\n",
    "print(post_predictive[\"triu_star\"].shape)\n",
    "print(post_predictive[\"triu_star\"][0,].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c52677fbf71e4079",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def get_mape_zeig(triu):\n",
    "    mat = Triu_to_mat(triu)\n",
    "    eig_cen = eigen_centrality(mat)\n",
    "    zeig = zeigen_value(Z, eig_cen, mat)\n",
    "    return jnp.mean(jnp.abs(zeig - Zeigen)/jnp.abs(Zeigen))\n",
    "\n",
    "vm_mape_zeig = vmap(get_mape_zeig, in_axes=(0))\n",
    "\n",
    "MS_zeig = vm_mape_zeig(post_predictive[\"triu_star\"])\n",
    "\n",
    "print(\"MS zeigen MAPE:\", jnp.mean(MS_zeig))\n",
    "print(\"Obs zeigen MAPE:\", jnp.mean(jnp.abs(obs_Zeigen - Zeigen)/jnp.abs(Zeigen)))\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(MS_zeig)\n",
    "      "
   ],
   "id": "982fec7e97c351fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rand_mat = Triu_to_mat(post_predictive[\"triu_star\"][77,])\n",
    "rand_eig_cen = eigen_centrality(rand_mat)\n",
    "rand_zeigen = zeigen_value(Z, rand_eig_cen, rand_mat)\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.scatter(rand_zeigen, Zeigen)\n",
    "print(np.corrcoef(rand_zeigen, Zeigen))\n",
    "print(np.corrcoef(obs_Zeigen, Zeigen))"
   ],
   "id": "d60f877aa8b7f0a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# rand_df = np.transpose(np.array([[1]*n, Z, x, x2, rand_zeigen]))\n",
    "rand_df = np.transpose(np.array([[1]*n, Z, x, rand_zeigen]))\n",
    "rand_samples_net = linear_model_samples(rng_key,Y=Y, df=rand_df)\n"
   ],
   "id": "c5785a6e0c534481",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print rand_samples_net posterior mean by item\n",
    "rand_post_mean = {\"alpha\" : jnp.expand_dims(jnp.mean(rand_samples_net[\"alpha\"],axis=0),-2)}\n",
    "rand_post_mean"
   ],
   "id": "405bab528bc35069",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# NN = n\n",
    "\n",
    "n_warmup = 500\n",
    "n_samples = 100\n",
    "n_chains = 1\n",
    "# M = m\n",
    "\n",
    "@jit\n",
    "def linear_model_sampless(key, Y, df):\n",
    "    kernel_outcome = NUTS(outcome_model)\n",
    "    lin_mcmc = MCMC(kernel_outcome, num_warmup=n_warmup, num_samples=n_samples,num_chains=n_chains, progress_bar=False, chain_method=\"vectorized\")\n",
    "    # mcmc.run(key, Y=Y, Z=Z, X=X, A=A, n=NN)\n",
    "    lin_mcmc.run(key, X=df, Y=Y)\n",
    "    return lin_mcmc.get_samples()\n",
    "\n",
    "@jit    \n",
    "def HSGP_model_sampless(key, Y, Xgp, Xlin, ell):\n",
    "    kernel_hsgp = NUTS(HSGP_model)\n",
    "    hsgp_mcmc = MCMC(kernel_hsgp, num_warmup=n_warmup, num_samples=n_samples,num_chains=n_chains, progress_bar=False, chain_method=\"vectorized\")\n",
    "    # mcmc.run(key, Y=Y, Z=Z, X=X, A=A, n=NN)\n",
    "    hsgp_mcmc.run(key, Xgp=Xgp, Xlin=Xlin, ell=ell ,m=M, Y=Y)\n",
    "    # samples = hsgp_mcmc.get_samples()\n",
    "    # hsgp_pred = Predictive(HSGP_model, hsgp_mcmc.get_samples())\n",
    "    # return hsgp_pred\n",
    "    return hsgp_mcmc.get_samples()\n",
    "\n",
    "@jit\n",
    "def compute_net_stats(Astar, Z):\n",
    "    cur_eigen_cen = eigen_centrality(Astar)\n",
    "    cur_Zeigen = zeigen_value(Z, cur_eigen_cen, Astar)\n",
    "    # if return_ell:\n",
    "    cur_ell = c*jnp.max(jnp.abs(cur_Zeigen))\n",
    "        # cur_ell = jnp.array(cur_ell)[..., None]\n",
    "    cur_ell = jnp.array(cur_ell).reshape(1,1)\n",
    "    return cur_Zeigen, cur_ell\n",
    "    # else:\n",
    "    #     return cur_Zeigen\n",
    "    \n",
    "@jit    \n",
    "def get_samples_new_Astar(Y, Z, X, X2, curr_Astar):\n",
    "        cur_Zeigen, ell = compute_net_stats(curr_Astar, Z)\n",
    "        # get df\n",
    "        cur_df = jnp.transpose(jnp.array([[1]*NN, Z, X, cur_Zeigen]))\n",
    "        # cur_df = jnp.transpose(jnp.array([[1]*NN, Z, X, X2, cur_Zeigen]))\n",
    "        # Run MCMC\n",
    "        cur_lin_samples = linear_model_sampless(rng_key_, Y, cur_df)\n",
    "        cur_hsgp_samples = HSGP_model_sampless(rng_key_, Y=Y, Xgp=cur_df[:,3:],\n",
    "                                                    Xlin=cur_df[:,0:3], ell=ell)\n",
    "        # cur_hsgp_samples = HSGP_model_sampless(rng_key_, Y=Y, Xgp=cur_df[:,4:],\n",
    "        #                                             Xlin=cur_df[:,0:4], ell=ell)\n",
    "        # cur_hsgp_predictive = Predictive(HSGP_model, HSGP_model_samples(key, Y=Y, Xgp=cur_df[:,3:],\n",
    "        #                                     Xlin=cur_df[:,0:3], ell=ell))\n",
    "        # cur_hsgp_predictive = HSGP_model_samples(key, Y=Y, Xgp=cur_df[:,3:],\n",
    "        #                                     Xlin=cur_df[:,0:3], ell=ell)\n",
    "                                                 # m=jnp.array(m).reshape(1,1))\n",
    "        return cur_lin_samples, cur_hsgp_samples, ell \n",
    "        # return cur_lin_samples, cur_hsgp_samples, cur_df, ell \n",
    "        # return cur_lin_samples, cur_hsgp_predictive, cur_df, ell \n",
    "\n",
    "@jit        \n",
    "def get_predicted_values(z, zeigen, x, x2, lin_samples, hsgp_samples, ell):\n",
    "        # get predicted values \n",
    "        # each has shape (#lin_samples, n)\n",
    "        cur_lin_pred = linear_pred(z, zeigen, lin_samples, x, x2)\n",
    "        cur_hsgp_pred = hsgp_pred(z, zeigen, hsgp_samples, x, x2, ell)\n",
    "        # cur_lin_pred = outcome_jit_pred(lin_samples, curr_df, key)\n",
    "        # cur_hsgp_pred = HSGP_jit_pred(hsgp_samples,Xgp=curr_df[:,3:], Xlin = curr_df[:,0:3],ell=ell)\n",
    "        # cur_hsgp_pred = hsgp_pred(key, Xgp=curr_df[:,3:], Xlin=curr_df[:,0:3],\n",
    "        #                                     ell=ell, m=M)\n",
    "                                            # m=m)\n",
    "        # get estimands for each sample (sample mean across units)\n",
    "        # lin_estimates = jnp.mean(cur_lin_pred[\"Y\"],axis=1)\n",
    "        # lin_estimates =  jnp.mean(cur_lin_pred, axis=0)\n",
    "        # hsgp_estimates = jnp.mean(cur_hsgp_pred[\"Y\"],axis=1)\n",
    "        # hsgp_estimates = jnp.mean(cur_hsgp_pred,axis=0)\n",
    "        # return lin_estimates, hsgp_estimates\n",
    "        return cur_lin_pred, cur_hsgp_pred\n",
    "               \n",
    "@jit\n",
    "def multistage_mcmc(samp_net, Y, Z_obs, Z_new, X, X2):\n",
    "    # cur_key = random.PRNGKey(i)\n",
    "    # sample network\n",
    "    # curr_triu_star = Astar_pred(cur_key, post_samples=post_mean, Xd=x_diff,triu=triu_obs)\n",
    "    # curr_Astar = Triu_to_mat(curr_triu_star[\"triu_star\"][0])\n",
    "    curr_Astar = Triu_to_mat(samp_net)\n",
    "    # re-run MCMC with new network\n",
    "    curr_lin_samples, curr_hsgp_samples, cur_ell = get_samples_new_Astar(Y, Z_obs, X, X2, curr_Astar)\n",
    "    # curr_lin_samples, curr_hsgp_samples, cur_df, cur_ell = get_samples_new_Astar(rng_key_, Y, Z_obs, X, curr_Astar)\n",
    "    # curr_lin_samples, curr_hsgp_pred, cur_df, cur_ell = get_samples_new_Astar(cur_key, Y, Z_obs, X, curr_Astar, m)\n",
    "    zeigen_new, _ = compute_net_stats(curr_Astar, Z_new)\n",
    "    # get predicted estimands for new `Z' values\n",
    "    # if Z_new.ndim == 1:\n",
    "        # new_df = np.copy(cur_df)\n",
    "        # new_df[:,1] = Z_new\n",
    "        # new_df = cur_df.at[:,1].set(Z_new)\n",
    "    lin_estimates, hsgp_estimates = get_predicted_values(Z_new, zeigen_new, x, x2, curr_lin_samples, curr_hsgp_samples, cur_ell)\n",
    "        # lin_estimates, hsgp_estimates = get_predicted_values(cur_key, new_df, curr_lin_samples, curr_hsgp_pred, cur_ell, m)\n",
    "    return jnp.array([lin_estimates, hsgp_estimates])\n",
    "    # if Z_new.ndim == 2:\n",
    "    #     n_iter = Z_new.shape[0]\n",
    "    #     lin_results = []\n",
    "    #     hsgp_results = []\n",
    "    #     for j in range(n_iter):\n",
    "    #         new_df = np.copy(cur_df)\n",
    "    #         new_df[:,1] = Z_new[j]\n",
    "    #         lin_estimates, hsgp_estimates = get_predicted_values(rng_key_, new_df, curr_lin_samples, curr_hsgp_pred, cur_ell, M)\n",
    "    #         lin_results.append([lin_estimates])\n",
    "    #         hsgp_results.append([hsgp_estimates])\n",
    "    #         return np.array(lin_results), np.array(hsgp_results)\n",
    "     \n",
    "\n",
    "@jit\n",
    "def Astar_pred(i, post_samples, Xd, X2, triu):\n",
    "    # if mean_post:\n",
    "    pred_func = Predictive(model=network_model, posterior_samples=post_samples, infer_discrete=True,num_samples=1)\n",
    "    # else:\n",
    "    #     pred_func = Predictive(model=network_model, posterior_samples=post_samples, infer_discrete=True)\n",
    "    samp_net = pred_func(random.PRNGKey(i**2), X=Xd, X2=X2, TriU=triu)[\"triu_star\"]\n",
    "    return jnp.squeeze(samp_net, axis=0)     \n",
    "     \n",
    "     \n",
    "vectorized_astar_pred = jax.vmap(Astar_pred, in_axes=(0, None, None, None, None))\n",
    "vectorized_multistage = jax.vmap(multistage_mcmc, in_axes=(0, None, None, None, None, None))\n",
    "parallel_multistage = jax.pmap(multistage_mcmc, in_axes=(0, None, None, None, None, None))\n",
    "\n",
    "\n",
    "\n",
    "#      \n",
    "# start = time.time()\n",
    "# # twostage_post_samp_jit = pd.DataFrame()\n",
    "# twostage_linear = []\n",
    "# twostage_hsgp = []\n",
    "# # for i in tqdm(range(50)):\n",
    "# for i in tqdm(range(3)):\n",
    "#     cur_lin, cur_hsgp = twostage_jj(Astar_pred, post_pred_mean, Y, Z, h_1, x, x_diff, triu_obs, i)\n",
    "#     # cur_lin, cur_hsgp = twostage_jj(Astar_pred, post_pred_mean, Y, Z, h_1, x, x_diff, triu_obs, i, m)\n",
    "#     twostage_linear.append(cur_lin)\n",
    "#     twostage_hsgp.append(cur_hsgp)\n",
    "#     # twostage_post_samp_jit = pd.concat([twostage_post_samp_jit,cur_res])    \n",
    "# # print(twostage_post_samp_jit)\n",
    "# twostage_linear = np.array(twostage_linear)\n",
    "# twostage_hsgp = np.array(twostage_hsgp)\n",
    "# print(\"lin shape:\", twostage_linear.shape, \"\\n\",\n",
    "#       \"hsgp shape:\", twostage_hsgp.shape)\n",
    "# print(\"with JIT (non parallel) takes: \", time.time() - start)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "739b60168409e8f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "print(\"Start time:\", datetime.datetime.now())\n",
    "start = time.time()\n",
    "K = 200\n",
    "irange = jnp.arange(K)\n",
    "many_samp_triu = vectorized_astar_pred(irange, post_pred_mean, jnp.array(x_diff), jnp.array(triu_obs))\n",
    "print(time.time() -start)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ae5d6e2cf095950",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "many_samp_triu.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67e94c2ebe133bb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "test_twos = vectorized_multistage(many_samp_triu, Y, jnp.array(Z), jnp.array(h_1), jnp.array(x))\n",
    "print(test_twos.shape)\n",
    "print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94a979b026294df0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "samp = test_twos.shape[2]\n",
    "twostage_lin = test_twos[:, 0, :]\n",
    "twostage_hsgp = test_twos[:, 1, :]\n",
    "twostage_linear_long = twostage_lin.reshape(K*samp)\n",
    "twostage_hsgp_long = twostage_hsgp.reshape(K*samp)\n",
    "\n",
    "print(compute_error_stats(twostage_linear_long, true_estimand, \"linear two stage\"), \"\\n\",\n",
    "      compute_error_stats(twostage_hsgp_long, true_estimand, \"hsgp two stage\"))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44c9f3086b245808",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Start time:\", datetime.datetime.now())\n",
    "start = time.time()\n",
    "test_twos_pm = []\n",
    "for i in range(0,K,4):\n",
    "    # i_results = parallel_multistage(many_samp_triu[i:(i+4),], Y, jnp.array(Z), jnp.array(h_1), jnp.array(x))\n",
    "    i_results = parallel_multistage(many_samp_triu[i:(i+4),], Y, jnp.array(Z), jnp.array(Z_stoch), jnp.array(x))\n",
    "    test_twos_pm.append(i_results)\n",
    "    # test_twos_pm = test_twos_pm.at[i:(i+3),:,:].set(i_samp)\n",
    "test_twos_pm = jnp.concatenate(test_twos_pm, axis=0)\n",
    "print(test_twos_pm.shape)\n",
    "print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3da8768149af7e57",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "samp = test_twos_pm.shape[2]\n",
    "twostage_lin = test_twos_pm[:, 0, :]\n",
    "twostage_hsgp = test_twos_pm[:, 1, :]\n",
    "twostage_linear_long = twostage_lin.reshape(K*samp)\n",
    "twostage_hsgp_long = twostage_hsgp.reshape(K*samp)\n",
    "\n",
    "print(compute_error_stats(twostage_linear_long, stoch_estimand, \"linear two stage\"), \"\\n\",\n",
    "      compute_error_stats(twostage_hsgp_long, stoch_estimand, \"hsgp two stage\"))\n",
    "\n",
    "# print(compute_error_stats(twostage_linear_long, true_estimand, \"linear two stage\"), \"\\n\",\n",
    "#       compute_error_stats(twostage_hsgp_long, true_estimand, \"hsgp two stage\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c67576da12906557",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# import importlib\n",
    "# import sys\n",
    "# modules_to_reload = list(sys.modules.keys())\n",
    "# \n",
    "# for module_name in modules_to_reload:\n",
    "#     if module_name.startswith(\"hsgp\"):  # Specify the module prefix or condition\n",
    "#         importlib.reload(sys.modules[module_name])\n",
    "#         print(f\"Reloaded module: {module_name}\")\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe6ae28c8fc9ed40",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# print(\"Start time:\", datetime.datetime.now())\n",
    "K=200\n",
    "# threestage\n",
    "i_range = np.random.choice(a = range(post_predictive[\"triu_star\"].shape[0]), size = K, replace = False)\n",
    "multi_post_triu = post_predictive[\"triu_star\"][i_range,]\n",
    "\n",
    "start = time.time()\n",
    "test_threes_pm = []\n",
    "for i in range(0,K,4):\n",
    "    i_results = parallel_multistage(multi_post_triu[i:(i+4),], Y, jnp.array(Z), jnp.array(h_1), jnp.array(x), jnp.array(x2))\n",
    "    test_threes_pm.append(i_results)\n",
    "    # test_twos_pm = test_twos_pm.at[i:(i+3),:,:].set(i_samp)\n",
    "test_threes_pm = jnp.concatenate(test_threes_pm, axis=0)\n",
    "print(test_threes_pm.shape)\n",
    "print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4743d2f318b5a582",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "samp = test_threes_pm.shape[2]\n",
    "threestage_lin = test_threes_pm[:,0,:,:]\n",
    "threestage_hsgp = test_threes_pm[:,1,:,:]\n",
    "print(threestage_lin.shape, threestage_hsgp.shape)\n",
    "threestage_linear_long = threestage_lin.reshape((K*samp,n))\n",
    "threestage_hsgp_long = threestage_hsgp.reshape((K*samp,n))\n",
    "print(threestage_linear_long.shape, threestage_hsgp_long.shape)\n",
    "print(compute_error_stats(threestage_linear_long, hestimand, \"linear 3-stage\"), \"\\n\",\n",
    "      compute_error_stats(threestage_hsgp_long, hestimand, \"hsgp 3-stage\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06888779d4f60b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def threestage_jj(triu_post_samp, Y, Z_obs, Z_new, X, i):\n",
    "    cur_key = random.PRNGKey(i)\n",
    "    # triu to adj mat\n",
    "    curr_Astar = Triu_to_mat(triu_post_samp)\n",
    "    # re-run MCMC with new network\n",
    "    curr_lin_samples, curr_hsgp_pred, cur_df, cur_ell = get_samples_new_Astar(cur_key, Y, Z_obs, X, curr_Astar)\n",
    "    # curr_lin_samples, curr_hsgp_pred, cur_df, cur_ell = get_samples_new_Astar(cur_key, Y, Z_obs, X, curr_Astar, m)\n",
    "    # get predicted estimands for new `Z' values\n",
    "    if Z_new.ndim == 1:\n",
    "        # new_df = np.copy(cur_df)\n",
    "        # new_df[:,1] = Z_new\n",
    "        new_df = cur_df.at[:,1].set(Z_new)\n",
    "        lin_estimates, hsgp_estimates = get_predicted_values(cur_key, new_df, curr_lin_samples, curr_hsgp_pred, cur_ell)\n",
    "        # lin_estimates, hsgp_estimates = get_predicted_values(cur_key, new_df, curr_lin_samples, curr_hsgp_pred, cur_ell, m)\n",
    "        return lin_estimates, hsgp_estimates\n",
    "    if Z_new.ndim == 2:\n",
    "        n_iter = Z_new.shape[0]\n",
    "        lin_results = []\n",
    "        hsgp_results = []\n",
    "        for j in range(n_iter):\n",
    "            new_df = np.copy(cur_df)\n",
    "            new_df[:,1] = Z_new[j]\n",
    "            lin_estimates, hsgp_estimates = get_predicted_values(cur_key, new_df, curr_lin_samples, curr_hsgp_pred, cur_ell, m)\n",
    "            lin_results.append(lin_estimates)\n",
    "            hsgp_results.append(hsgp_estimates)\n",
    "            return np.array(lin_results), np.array(hsgp_results)\n",
    "     \n",
    "     \n",
    "start = time.time()\n",
    "K = 100\n",
    "i_range = np.random.choice(a = range(post_predictive[\"triu_star\"].shape[0]), size = K, replace = False)\n",
    "# twostage_post_samp_jit = pd.DataFrame()\n",
    "threestage_linear = []\n",
    "threestage_hsgp = []\n",
    "# for i in tqdm(range(50)):\n",
    "for i in tqdm(i_range):\n",
    "    cur_lin, cur_hsgp = threestage_jj(post_predictive[\"triu_star\"][i,], Y, Z, h_1, x, i)\n",
    "    # cur_lin, cur_hsgp = twostage_jj(Astar_pred, post_pred_mean, Y, Z, h_1, x, x_diff, triu_obs, i, m)\n",
    "    threestage_linear.append(cur_lin)\n",
    "    threestage_hsgp.append(cur_hsgp)\n",
    "    # twostage_post_samp_jit = pd.concat([twostage_post_samp_jit,cur_res])    \n",
    "# print(twostage_post_samp_jit)\n",
    "threestage_linear = np.array(threestage_linear)\n",
    "threestage_hsgp = np.array(threestage_hsgp)\n",
    "print(\"lin shape:\", threestage_linear.shape, \"\\n\",\n",
    "      \"hsgp shape:\", threestage_hsgp.shape)\n",
    "print(\"with JIT (non parallel) takes: \", time.time() - start)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d89cd4bf5f75ee0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "m, samp = threestage_linear.shape\n",
    "threestage_linear_long = threestage_linear.reshape(m*samp)\n",
    "threestage_hsgp_long = threestage_hsgp.reshape(m*samp)\n",
    "print(compute_error_stats(threestage_linear_long, true_estimand, \"linear three stage\"), \"\\n\",\n",
    "      compute_error_stats(threestage_hsgp_long, true_estimand, \"hsgp three stage\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65c42630a5828aef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "mean_alpha2 = np.mean(threestage_post_samp_jit[\"alpha_2\"])\n",
    "\n",
    "def between_var(x, mean_all):\n",
    "    n_rep = len(x)\n",
    "    return (1/(n_rep - 1))*np.sum(np.square(x-mean_alpha2))\n",
    "\n",
    "alpha2_agg_by_iter = threestage_post_samp_jit[[\"iter\",\"alpha_2\"]].groupby(\"iter\").agg([\"mean\",\"var\"])\n",
    "alpha2_agg_by_iter.columns = [\"mean\",\"var\"]\n",
    "alpha2_VB = between_var(alpha2_agg_by_iter[\"mean\"], mean_alpha2)\n",
    "alpha2_VW = np.mean(alpha2_agg_by_iter[\"var\"])\n",
    "\n",
    "alpha2_MI_var = alpha2_VB*(1 + 1/K) + alpha2_VW\n",
    "print(\"VB: \", alpha2_VB, \"VW: \", alpha2_VW, \" Total var: \", alpha2_MI_var,\n",
    "      \" Naive var: \", np.var(threestage_post_samp_jit[\"alpha_2\"]))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ad6a7559b22dfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# `cut posterior` : for each A* and A|A* param, sample one A*, and compute network statistics distribution (e.g., degrees and exposures)\n",
    "# The uncertainty in A* and A|A* *doesn't* propagate into the outcome model which depends on the network statistics\n",
    "# Use the mean network stat in the outcome model (i.e., only one outcome model run!).\n",
    "\n",
    "@jit\n",
    "def network_posterior_stats(triu_sample, z):\n",
    "    curr_Astar = Triu_to_mat(triu_sample)    \n",
    "    # curr_degree = jnp.sum(curr_Astar,1)\n",
    "    cur_eig_cen = eigen_centrality(curr_Astar)\n",
    "    zeigen = zeigen_value(z, cur_eig_cen, curr_Astar)\n",
    "    # Zeigen_old = zeigen_value(Z_old, cur_eig_cen, curr_Astar)\n",
    "    # Zeigen_new = zeigen_value(Z_new, cur_eig_cen, curr_Astar)\n",
    "    return zeigen\n",
    "    # return jnp.array(Zeigen_old, Zeigen_new)\n",
    "    \n",
    "parallel_network_post_stats = jax.pmap(network_posterior_stats, in_axes=(0, None))\n",
    "vectorized_network_post_stats = jax.vmap(network_posterior_stats, in_axes=(0, None))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d38c438349af4a3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# start = time.time()\n",
    "# mean_zeigen_vec = vectorized_network_post_stats(post_predictive[\"triu_star\"], Z, h_1)\n",
    "# print(mean_zeigen_vec.shape)\n",
    "# print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8acc690cba1025c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "stt = network_posterior_stats(post_predictive[\"triu_star\"][0,], h_1)\n",
    "print(stt.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f869d8e0ce427854",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "mean_obs_zeigen = []\n",
    "mean_h_zeigen = []\n",
    "mean_h2_zeigen = []\n",
    "mean_stoch_zeigen = []\n",
    "mean_stoch2_zeigen = []\n",
    "for i in range(0,post_predictive[\"triu_star\"].shape[0],4):\n",
    "    # i_results = parallel_network_post_stats(post_predictive[\"triu_star\"][i:(i+4),], Z, h_1)\n",
    "    obs_results = parallel_network_post_stats(post_predictive[\"triu_star\"][i:(i+4),], Z)\n",
    "    mean_obs_zeigen.append(obs_results)\n",
    "    new_results = parallel_network_post_stats(post_predictive[\"triu_star\"][i:(i+4),], h_1)\n",
    "    mean_h_zeigen.append(new_results)\n",
    "    new_h2_results = parallel_network_post_stats(post_predictive[\"triu_star\"][i:(i+4),], h_2)\n",
    "    mean_h2_zeigen.append(new_h2_results)\n",
    "    stoch_results = parallel_network_post_stats(post_predictive[\"triu_star\"][i:(i+4),], Z_stoch)\n",
    "    mean_stoch_zeigen.append(stoch_results)\n",
    "    stoch2_results = parallel_network_post_stats(post_predictive[\"triu_star\"][i:(i+4),], Z_stoch2)\n",
    "    mean_stoch2_zeigen.append(stoch2_results)\n",
    "    # test_twos_pm = test_twos_pm.at[i:(i+3),:,:].set(i_samp)\n",
    "mean_obs_zeigen = jnp.concatenate(mean_obs_zeigen, axis=0)\n",
    "mean_h_zeigen = jnp.concatenate(mean_h_zeigen, axis=0)\n",
    "mean_h2_zeigen = jnp.concatenate(mean_h2_zeigen, axis=0)\n",
    "mean_stoch_zeigen = jnp.concatenate(mean_stoch_zeigen, axis=0)\n",
    "mean_stoch2_zeigen = jnp.concatenate(mean_stoch2_zeigen, axis=0)\n",
    "print(mean_obs_zeigen.shape)\n",
    "print(mean_h_zeigen.shape)\n",
    "print(mean_stoch_zeigen.shape)\n",
    "print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56447a7f6a51a9e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "post_zeigen_obs = jnp.mean(mean_obs_zeigen, axis=0)\n",
    "post_zeigen_h = jnp.mean(mean_h_zeigen, axis=0)\n",
    "post_zeigen_h2 = jnp.mean(mean_h2_zeigen, axis=0)\n",
    "post_zeigen_stoch = jnp.mean(mean_stoch_zeigen, axis=0)\n",
    "post_zeigen_stoch2 = jnp.mean(mean_stoch2_zeigen, axis=0)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34b761670f9d9275",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(np.mean(np.abs(post_zeigen_obs-Zeigen)))\n",
    "print(np.mean(np.abs(obs_Zeigen-Zeigen)))"
   ],
   "id": "34a77716e991966e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "n_warmup = 1000\n",
    "n_samples = 2000\n",
    "n_chains = 4\n",
    "M = m\n",
    "\n",
    "@jit\n",
    "def linear_model_samples(key, Y, df):\n",
    "    kernel_outcome = NUTS(outcome_model)\n",
    "    lin_mcmc = MCMC(kernel_outcome, num_warmup=n_warmup, num_samples=n_samples,num_chains=n_chains, progress_bar=False, chain_method=\"vectorized\")\n",
    "    # mcmc.run(key, Y=Y, Z=Z, X=X, A=A, n=NN)\n",
    "    lin_mcmc.run(key, X=df, Y=Y)\n",
    "    return lin_mcmc.get_samples()\n",
    "\n",
    "\n",
    "@jit    \n",
    "def HSGP_model_samples(key, Y, Xgp, Xlin, ell):\n",
    "    kernel_hsgp = NUTS(HSGP_model)\n",
    "    hsgp_mcmc = MCMC(kernel_hsgp, num_warmup=num_warmup, num_samples=num_samples,num_chains=num_chains, progress_bar=False, chain_method=\"vectorized\")\n",
    "    # mcmc.run(key, Y=Y, Z=Z, X=X, A=A, n=NN)\n",
    "    hsgp_mcmc.run(key, Xgp=Xgp, Xlin=Xlin, ell=ell ,m=M, Y=Y)\n",
    "    # hsgp_mcmc.print_summary()\n",
    "    # samples = hsgp_mcmc.get_samples()\n",
    "    # hsgp_pred = Predictive(HSGP_model, hsgp_mcmc.get_samples())\n",
    "    # return hsgp_pred\n",
    "    return hsgp_mcmc.get_samples()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79feb508275afe2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# one_stage_df_obs = jnp.array(np.transpose(np.array([[1]*n, Z, x,x2, post_zeigen_obs])))\n",
    "one_stage_df_obs = jnp.array(np.transpose(np.array([[1]*n, Z, x, post_zeigen_obs])))\n",
    "one_stage_ell = jnp.array(c*jnp.max(jnp.abs(post_zeigen_obs))).reshape(1,1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68e564dcd89a0f45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(ell, ell_obs, one_stage_ell)",
   "id": "8202cd6ac7f47b55",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "onestage_samples_linear = linear_model_samples(rng_key_,Y=Y, df=one_stage_df_obs)\n",
    "onestage_samples_hsgp = HSGP_model_samples(rng_key_,Y=Y, Xgp=one_stage_df_obs[:,3], Xlin=one_stage_df_obs[:,:3], ell=one_stage_ell)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98c08b497bf3f7b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Onestage linear HTE\n",
    "lin_h1_pred_os = linear_pred(h_1, post_zeigen_h, onestage_samples_linear, x, x2)\n",
    "lin_h2_pred_os = linear_pred(h_2, post_zeigen_h2, onestage_samples_linear, x, x2)\n",
    "# lin_hte_pred_os = jnp.mean(lin_h1_pred - lin_h2_pred, axis=0)\n",
    "lin_hte_pred_os = lin_h1_pred - lin_h2_pred\n",
    "print(compute_error_stats(lin_hte_pred_os, hte))"
   ],
   "id": "1570aefe0f309c82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Onestage HSGP HTE\n",
    "hsgp_h1_pred_os = hsgp_pred(h_1, post_zeigen_h, onestage_samples_hsgp, x, x2, one_stage_ell)\n",
    "hsgp_h2_pred_os = hsgp_pred(h_2, post_zeigen_h2, onestage_samples_hsgp, x, x2, one_stage_ell)\n",
    "# hsgp_hte_pred_os = jnp.mean(hsgp_h1_pred_os - hsgp_h2_pred_os, axis=0)\n",
    "hsgp_hte_pred_os = hsgp_h1_pred_os - hsgp_h2_pred_os\n",
    "print(compute_error_stats(hsgp_hte_pred_os, hte))\n"
   ],
   "id": "16f6b9b607c4bc0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Onestage linear Stoch\n",
    "lin_stoch_pred_os = linear_pred(Z_stoch, post_zeigen_stoch, onestage_samples_linear, x, x2)\n",
    "lin_stoch2_pred_os = linear_pred(Z_stoch2, post_zeigen_stoch2, onestage_samples_linear, x, x2)\n",
    "lin_stoch_pred_os = jnp.mean(lin_stoch_pred_os - lin_stoch2_pred_os, axis=0)\n",
    "print(compute_error_stats(lin_stoch_pred_os, te_stoch))\n"
   ],
   "id": "c0649772ffecac47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Onestage HSGP Stoch\n",
    "hsgp_stoch_pred_os = hsgp_pred(Z_stoch, post_zeigen_stoch, onestage_samples_hsgp, x, x2, one_stage_ell)\n",
    "hsgp_stoch2_pred_os = hsgp_pred(Z_stoch2, post_zeigen_stoch2, onestage_samples_hsgp, x, x2, one_stage_ell)\n",
    "hsgp_stoch_pred_os = jnp.mean(hsgp_stoch_pred_os - hsgp_stoch2_pred_os, axis=0)\n",
    "print(compute_error_stats(hsgp_stoch_pred_os, te_stoch))"
   ],
   "id": "8a15b3a200915075",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "lin_h_1_onestage_pred = linear_pred(h_1, post_zeigen_h, onestage_samples_linear, x, x2)\n",
    "print(lin_h_1_onestage_pred.shape)\n",
    "print(jnp.mean(lin_h_1_onestage_pred, axis=0).shape)\n",
    "print(compute_error_stats(jnp.mean(lin_h_1_onestage_pred, axis=0), hestimand))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a073a1066105e7ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "hsgp_h_1_onestage_pred = hsgp_pred(h_1, post_zeigen_h, onestage_samples_hsgp, x, one_stage_ell)\n",
    "print(hsgp_h_1_onestage_pred.shape)\n",
    "print(jnp.mean(hsgp_h_1_onestage_pred, axis=0).shape)\n",
    "print(compute_error_stats(jnp.mean(hsgp_h_1_onestage_pred, axis=0), hestimand))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45a63b56a5857649",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "lin_stoch_onestage_pred = linear_pred(Z_stoch, post_zeigen_stoch, onestage_samples_linear, x)\n",
    "print(lin_stoch_onestage_pred.shape)\n",
    "print(jnp.mean(lin_stoch_onestage_pred, axis=0).shape)\n",
    "linstoch_stat = compute_error_stats(jnp.mean(lin_stoch_onestage_pred, axis=0), stoch_estimand, \"Onestage_linear\")\n",
    "print(linstoch_stat)\n",
    "# print(compute_error_stats(jnp.mean(lin_stoch_onestage_pred, axis=0), stoch_estimand))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ddd57e7d4218364",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "hsgp_stoch_onestage_pred = hsgp_pred(Z_stoch, post_zeigen_stoch, onestage_samples_hsgp, x, one_stage_ell)\n",
    "print(hsgp_stoch_onestage_pred.shape)\n",
    "print(jnp.mean(hsgp_stoch_onestage_pred, axis=0).shape)\n",
    "hsgpstoch_stat = compute_error_stats(jnp.mean(hsgp_stoch_onestage_pred, axis=0), stoch_estimand, \"Onestage_hsgp\")\n",
    "print(hsgpstoch_stat)\n",
    "# print(compute_error_stats(jnp.mean(hsgp_stoch_onestage_pred, axis=0), stoch_estimand))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8da94571a73d54a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pd.concat([linstoch_stat, hsgpstoch_stat])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71135fd7053c03a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "true_deg = np.sum(mat,1)\n",
    "obs_deg = np.sum(obs_mat,1)\n",
    "\n",
    "beige_color = '#FFFDFA'\n",
    "pastel_purple_dark = (0.4, 0.2, 0.6)\n",
    "# sns.set_theme(style=\"ticks\", rc={'figure.facecolor': beige_color, 'axes.facecolor' : beige_color})\n",
    "# sns.set_context(\"poster\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1,ncols=2, sharex=True, sharey=True)\n",
    "ax1, ax2 = axes.flatten()\n",
    "# fig.patch.set_facecolor(beige_color)\n",
    "# for ax in axes:\n",
    "    # ax.set_facecolor(beige_color) \n",
    "\n",
    "# ax1.plot(a_star_pred_deg, a_star_true_deg,\"o\")\n",
    "ax1.scatter(x=obs_deg,y=true_deg, marker = 'o',alpha=0.8,color=pastel_purple_dark,edgecolors=\"none\")\n",
    "ax1.axline((0,0),slope=1,c=\"black\",ls=\"--\",alpha=0.8,lw=1.3)\n",
    "# plt.gca().set_facecolor(beige_color)\n",
    "# ax2.plot(a_obs_deg, a_star_true_deg,\"o\")\n",
    "ax2.scatter(x=mean_deg, y=true_deg, marker = 'o',alpha=0.8,color=pastel_purple_dark,edgecolors=\"none\")\n",
    "ax2.axline((0,0),slope=1,c=\"black\",ls=\"--\",alpha=0.8,lw=1.3)\n",
    "# plt.gca().set_facecolor(beige_color)\n",
    "ax2.set_xlabel(\"Posterior mean\")\n",
    "ax1.set_xlabel(\"Observed\")\n",
    "ax1.set_ylabel(\"True\")\n",
    "ax2.set_ylabel(\"True\")\n",
    "\n",
    "fig.suptitle('Degree distribution', fontsize=16)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 4)\n",
    "fig.savefig('deg_dist.png', dpi=300)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97d238fcf0b34475",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
