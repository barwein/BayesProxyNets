{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Example: Gaussian Process\n",
    "\n",
    "In this example we show how to use NUTS to sample from the posterior\n",
    "over the hyperparameters of a gaussian process.\n",
    "\n",
    "<img src=\"file://../_static/img/examples/gp.png\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T07:12:31.773507900Z",
     "start_time": "2024-06-03T07:12:29.654764700Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "from jax import vmap\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import (\n",
    "    MCMC,\n",
    "    NUTS,\n",
    "    init_to_feasible,\n",
    "    init_to_median,\n",
    "    init_to_sample,\n",
    "    init_to_uniform,\n",
    "    init_to_value,\n",
    ")\n",
    "from scipy.spatial.distance import squareform, cdist\n",
    "\n",
    "\n",
    "# matplotlib.use(\"Agg\")  # noqa: E402\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# squared exponential kernel with diagonal noise term\n",
    "def kernel(X,Xs, var, length, noise, jitter=1.0e-6, include_noise=True):\n",
    "    delta_sq =  jnp.power(cdist(X, Xs, metric=\"euclidean\") / length, 2.0)\n",
    "    k = var * jnp.exp(-0.5 * delta_sq)\n",
    "    if include_noise:\n",
    "        k += (noise + jitter) * jnp.eye(delta_sq.shape[0])\n",
    "    return k\n",
    "\n",
    "\n",
    "def model(Xgp,Xlin, Y):\n",
    "    # set uninformative log-normal priors on our three kernel hyperparameters\n",
    "    var = numpyro.sample(\"kernel_var\", dist.LogNormal(0.0, 10.0))\n",
    "    noise = numpyro.sample(\"kernel_noise\", dist.LogNormal(0.0, 10.0))\n",
    "    length = numpyro.sample(\"kernel_length\", dist.LogNormal(0.0, 10.0))\n",
    "\n",
    "    # compute kernel\n",
    "    k = kernel(Xgp,Xgp, var, length, noise)\n",
    "    \n",
    "    # Linear mean\n",
    "    beta = numpyro.sample(\"beta\",dist.Normal(0,5))\n",
    "    mu = Xlin*beta\n",
    "    \n",
    "    # sample Y according to the standard gaussian process formula\n",
    "    numpyro.sample(\n",
    "        \"Y\",\n",
    "        # dist.MultivariateNormal(loc=jnp.zeros(Y.shape[0]), covariance_matrix=k),\n",
    "        dist.MultivariateNormal(loc=mu, covariance_matrix=k),\n",
    "        obs=Y,\n",
    "    )\n",
    "\n",
    "\n",
    "# helper function for doing hmc inference\n",
    "def run_inference(model, rng_key, Xgp,Xlin, Y):\n",
    "    start = time.time()\n",
    "    # demonstrate how to use different HMC initialization strategies\n",
    "    # if args.init_strategy == \"value\":\n",
    "    #     init_strategy = init_to_value(\n",
    "    #         values={\"kernel_var\": 1.0, \"kernel_noise\": 0.05, \"kernel_length\": 0.5}\n",
    "    #     )\n",
    "    # elif args.init_strategy == \"median\":\n",
    "    #     init_strategy = init_to_median(num_samples=10)\n",
    "    # elif args.init_strategy == \"feasible\":\n",
    "    #     init_strategy = init_to_feasible()\n",
    "    # elif args.init_strategy == \"sample\":\n",
    "    #     init_strategy = init_to_sample()\n",
    "    # elif args.init_strategy == \"uniform\":\n",
    "    #     init_strategy = init_to_uniform(radius=1)\n",
    "    # kernel = NUTS(model, init_strategy=init_strategy)\n",
    "    kernel = NUTS(model, init_strategy=init_to_median(num_samples=10))\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=1000,\n",
    "        num_samples=2000,\n",
    "        num_chains=2,\n",
    "        # thinning=2,\n",
    "        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
    "    )\n",
    "    mcmc.run(rng_key, Xgp, Xlin, Y)\n",
    "    mcmc.print_summary()\n",
    "    print(\"\\nMCMC elapsed time:\", time.time() - start)\n",
    "    return mcmc.get_samples()\n",
    "\n",
    "\n",
    "# do GP prediction for a given set of hyperparameters. this makes use of the well-known\n",
    "# formula for Gaussian process predictions\n",
    "def predict(rng_key, Xgp, Y, Xgp_test, var, length, noise, use_cholesky=True):\n",
    "    # compute kernels between train and test data, etc.\n",
    "    k_pp = kernel(Xgp_test, Xgp_test, var, length, noise, include_noise=True) # mXm\n",
    "    k_pX = kernel(Xgp_test, Xgp, var, length, noise, include_noise=False) # mXn\n",
    "    k_XX = kernel(Xgp, Xgp, var, length, noise, include_noise=True) #nXn\n",
    "\n",
    "    # since K_xx is symmetric positive-definite, we can use the more efficient and\n",
    "    # stable Cholesky decomposition instead of matrix inversion\n",
    "    if use_cholesky:\n",
    "        K_xx_cho = jax.scipy.linalg.cho_factor(k_XX)\n",
    "        K = k_pp - jnp.matmul(k_pX, jax.scipy.linalg.cho_solve(K_xx_cho, k_pX.T))\n",
    "        mean = jnp.matmul(k_pX, jax.scipy.linalg.cho_solve(K_xx_cho, Y))\n",
    "    else:\n",
    "        K_xx_inv = jnp.linalg.inv(k_XX) #nXn\n",
    "        K = k_pp - jnp.matmul(k_pX, jnp.matmul(K_xx_inv, jnp.transpose(k_pX))) #mXm \n",
    "        mean = jnp.matmul(k_pX, jnp.matmul(K_xx_inv, Y)) \n",
    "        # mean += np.dot(beta[:,np.newaxis],np.transpose(Xlin_test))\n",
    "        \n",
    "    # mean += np.dot(beta[:,np.newaxis], np.transpose(Xlin_test[:,np.newaxis]))\n",
    "    sigma_noise = jnp.sqrt(jnp.clip(jnp.diag(K), a_min=0.0)) * jax.random.normal(\n",
    "        rng_key, X_test.shape[:1]\n",
    "    )\n",
    "\n",
    "    # we return both the mean function and a sample from the posterior predictive for the\n",
    "    # given set of hyperparameters\n",
    "    return mean, mean + sigma_noise\n",
    "\n",
    "\n",
    "# create artificial regression dataset\n",
    "def get_data(N=150, sigma_obs=0.25):\n",
    "    np.random.seed(0)\n",
    "    # X = jnp.linspace(-1, 1, N)\n",
    "    X = np.random.uniform(size=N, low=0, high=1)\n",
    "    # X = np.sort(X,axis=0)\n",
    "    X2 = np.random.uniform(size=N,low=-1,high=0)\n",
    "    X3 = np.random.binomial(n=1,p=0.5,size=N)\n",
    "    # X2 = np.random.normal(size=N,loc=1,scale=1)\n",
    "    # Y = X + 0.2 * jnp.power(X, 3.0) + 0.5 * jnp.power(0.5 + X, 2.0) * jnp.sin(4.0 * X)\n",
    "    Y = 2*X3 + 1.5*X + X2 + 2*X*X2 + 4*np.exp(X2*X)\n",
    "    # Y = X + 2*X2 + 0.2 * jnp.power(X, 3.0) + 0.5 * jnp.power(0.5 + X, 2.0)\n",
    "    # Y += sigma_obs * np.random.randn(N)\n",
    "    Y += sigma_obs * np.random.standard_normal(N)\n",
    "    # Y -= jnp.mean(Y)\n",
    "    # Y /= jnp.std(Y)\n",
    "\n",
    "    # assert X.shape == (N,)\n",
    "    # assert Y.shape == (N,)\n",
    "\n",
    "    # X_test = np.random.uniform(size=N, low=-0.2, high=1.2)\n",
    "    # X_test = np.sort(X_test, axis=0)\n",
    "    # Y_test = X_test + 0.2 * jnp.power(X_test, 3.0) + 0.5 * jnp.power(0.5 + X_test, 2.0) * jnp.sin(4.0 * X_test)\n",
    "    # Y_test += sigma_obs * np.random.randn(N)\n",
    "    X_t = np.random.uniform(size=N, low=-0.25, high=1.25)\n",
    "    X2_t = np.random.uniform(size=N,low=-1.5,high=0)\n",
    "    X3_t = np.random.binomial(n=1,p=0.7,size=N)\n",
    "    # X2_t = np.random.normal(size=N,loc=1,scale=1)\n",
    "    # Y = X + 0.2 * jnp.power(X, 3.0) + 0.5 * jnp.power(0.5 + X, 2.0) * jnp.sin(4.0 * X)\n",
    "    # Y_t = X_t + 2*X2_t + 0.2 * np.power(X_t, 3.0) + 0.5 * np.power(0.5 + X_t, 2.0)\n",
    "    # Y_t = X2_t + np.power(X2,2) + 0.5*np.power(0.5 + X_t, 2.0)*jnp.sin(4*X_t)\n",
    "    Y_t = 2*X3_t + 1.5*X_t + X2_t + 2*X_t*X2_t + 0.5*np.exp(2*X_t)\n",
    "    # Y_t = X_t + 2*X2_t + 0.2 * jnp.power(X_t, 3.0) + 0.5 * jnp.power(0.5 + X_t, 2.0)\n",
    "    # Y_t += sigma_obs * np.random.randn(N)\n",
    "    Y_t += sigma_obs * np.random.standard_normal(N)\n",
    "    # Y -= jnp.mean(Y)\n",
    "    # Y /= jnp.std(Y)\n",
    "\n",
    "    # return np.transpose(np.array([X,X2])), Y, np.transpose(np.array([X_t,X2_t])), Y_t\n",
    "    return X,X2,X3,Y, X_t,X2_t,X3_t,Y_t\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:34:23.610397500Z",
     "start_time": "2024-06-03T08:34:23.595101700Z"
    }
   },
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2) (150,) (150,)\n",
      "(150, 2) (150,) (150,)\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "\n",
    "# X, Y, X_test = get_data(N=args.num_data)\n",
    "X, X2, X3, Y, X_test, X2_test,X3_test, Y_test = get_data()\n",
    "\n",
    "Xgp = np.transpose(np.array([X,X2]))\n",
    "Xgp_test = np.transpose(np.array([X_test,X2_test]))\n",
    "\n",
    "print(Xgp.shape,X3.shape, Y.shape)\n",
    "print(Xgp_test.shape,X3_test.shape, Y_test.shape)\n",
    "\n",
    "# Xarr = np.transpose(np.array([X,X2]))\n",
    "# Xarr_tst = np.transpose(np.array([X_test,X2_test]))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:34:24.425835500Z",
     "start_time": "2024-06-03T08:34:24.409734100Z"
    }
   },
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bar21\\AppData\\Local\\Temp\\ipykernel_5736\\2727657575.py:50: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  mcmc = MCMC(\n",
      "sample: 100%|██████████| 3000/3000 [00:26<00:00, 111.37it/s, 15 steps of size 3.87e-01. acc. prob=0.94]\n",
      "sample: 100%|██████████| 3000/3000 [00:24<00:00, 121.67it/s, 7 steps of size 3.49e-01. acc. prob=0.93] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "           beta      2.04      0.04      2.04      1.97      2.11   2694.75      1.00\n",
      "  kernel_length      1.36      0.37      1.30      0.81      1.88   1440.96      1.00\n",
      "   kernel_noise      0.06      0.01      0.06      0.05      0.07   2562.47      1.00\n",
      "     kernel_var     61.08    189.48     25.19      2.64    114.88   1262.07      1.01\n",
      "\n",
      "Number of divergences: 0\n",
      "\n",
      "MCMC elapsed time: 51.98292517662048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# do inference\n",
    "rng_key, rng_key_predict = random.split(random.PRNGKey(0))\n",
    "samples = run_inference(model, rng_key, Xgp, X3, Y)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:35:17.509246100Z",
     "start_time": "2024-06-03T08:34:25.522799600Z"
    }
   },
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'beta'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[79], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43msamples\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbeta\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[:,np\u001B[38;5;241m.\u001B[39mnewaxis]\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39mtranspose(X2_test[:,np\u001B[38;5;241m.\u001B[39mnewaxis])\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39mdot(samples[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbeta\u001B[39m\u001B[38;5;124m'\u001B[39m][:,np\u001B[38;5;241m.\u001B[39mnewaxis],np\u001B[38;5;241m.\u001B[39mtranspose(X2_test[:,np\u001B[38;5;241m.\u001B[39mnewaxis]))\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'beta'"
     ]
    }
   ],
   "source": [
    "# print(samples['beta'][:,np.newaxis].shape)\n",
    "# print(np.transpose(X2_test[:,np.newaxis]).shape)\n",
    "# print(np.dot(samples['beta'][:,np.newaxis],np.transpose(X2_test[:,np.newaxis])).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:13:22.228760Z",
     "start_time": "2024-06-03T08:13:22.192559200Z"
    }
   },
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# do prediction\n",
    "vmap_args = (\n",
    "    random.split(rng_key_predict, samples[\"kernel_var\"].shape[0]),\n",
    "    samples[\"kernel_var\"],\n",
    "    samples[\"kernel_length\"],\n",
    "    samples[\"kernel_noise\"],\n",
    "    # samples[\"beta\"],\n",
    ")\n",
    "means, predictions = vmap(\n",
    "    lambda rng_key, var, length, noise: predict(\n",
    "        rng_key, Xgp, Y, Xgp_test, var, length, noise, use_cholesky=True\n",
    "    )\n",
    ")(*vmap_args)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:35:41.163131400Z",
     "start_time": "2024-06-03T08:35:22.539848400Z"
    }
   },
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lin_pred = np.dot(samples['beta'][:,np.newaxis],np.transpose(X3_test[:,np.newaxis]))\n",
    "\n",
    "means += lin_pred\n",
    "predictions += lin_pred\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:36:15.296461700Z",
     "start_time": "2024-06-03T08:36:15.276393Z"
    }
   },
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,) \n",
      " (4000, 150) mean: 4.50788 \n",
      " mean ytest 2.3724102412687276 \n",
      " mean lin_pred 1.3584474298556646 \n",
      " perc mean: [-0.68139954  7.92929771] \n",
      " perc: [-0.71212576  7.96811142] \n",
      " rmse: 12.220756\n"
     ]
    }
   ],
   "source": [
    "mean_prediction = np.mean(means, axis=0)\n",
    "percentiles = np.percentile(predictions, [2.5, 97.5], axis=0)\n",
    "mean_percentiles = np.percentile(means, [2.5, 97.5], axis=0)\n",
    "\n",
    "print(mean_prediction.shape, '\\n',\n",
    "      means.shape, \n",
    "    'mean:', np.mean(mean_prediction), '\\n',\n",
    "      'mean ytest', np.mean(Y_test), '\\n',\n",
    "      'mean lin_pred', np.mean(lin_pred), '\\n',\n",
    "      'perc mean:', np.percentile(means, [2.5, 97.5]), '\\n',\n",
    "      'perc:', np.percentile(predictions, [2.5, 97.5]), '\\n',\n",
    "      'rmse:', np.mean(np.power(mean_prediction - Y_test,2))) \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:36:17.291722700Z",
     "start_time": "2024-06-03T08:36:17.231470300Z"
    }
   },
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "matplotlib.use('Qt5Agg')\n",
    "\n",
    "# make plots\n",
    "fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
    "\n",
    "# plot training data\n",
    "# ax.scatter(X_test.flatten(), Y_test, c=X2_test, cmap = \"prism\")\n",
    "ax.scatter(X_test.flatten(), Y_test-np.mean(lin_pred,axis=0))\n",
    "# plot 90% confidence level of predictions\n",
    "# ax.fill_between(X_test.flatten(), percentiles[0,:], percentiles[1,:], color=\"lightblue\")\n",
    "# ax.fill_between(X_test.flatten(), mean_percentiles[0, :], mean_percentiles[1, :], color=\"lightblue\")\n",
    "# plot mean prediction\n",
    "# ax.scatter(X_test.flatten(), mean_prediction, c=\"red\")\n",
    "ax.scatter(X_test.flatten(), mean_prediction-np.mean(lin_pred,axis=0), c=\"red\")\n",
    "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 95% CI\")\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(\"gp_plot.pdf\")\n",
    "# plt.savefig(\"gp_plot2.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:36:45.076465600Z",
     "start_time": "2024-06-03T08:36:35.927436600Z"
    }
   },
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# make plots\n",
    "fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
    "\n",
    "# plot training data\n",
    "# ax.scatter(X_test.flatten(), Y_test, c=X2_test, cmap = \"prism\")\n",
    "ax.scatter(X2_test.flatten(), Y_test)\n",
    "# plot 90% confidence level of predictions\n",
    "# ax.fill_between(X_test.flatten(), percentiles[0,:], percentiles[1,:], color=\"lightblue\")\n",
    "# ax.fill_between(X_test.flatten(), mean_percentiles[0, :], mean_percentiles[1, :], color=\"lightblue\")\n",
    "# plot mean prediction\n",
    "# ax.scatter(X_test.flatten(), mean_prediction, c=\"red\")\n",
    "ax.scatter(X2_test.flatten(), mean_prediction, c=\"red\")\n",
    "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 95% CI\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:36:50.469898300Z",
     "start_time": "2024-06-03T08:36:48.101320900Z"
    }
   },
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
    "\n",
    "# plot training data\n",
    "# ax.scatter(X_test.flatten(), Y_test, c=X2_test, cmap = \"prism\")\n",
    "ax.scatter(X2_test.flatten(), Y_test)\n",
    "# plot 90% confidence level of predictions\n",
    "# ax.fill_between(X_test.flatten(), percentiles[0,:], percentiles[1,:], color=\"lightblue\")\n",
    "# ax.fill_between(X_test.flatten(), mean_percentiles[0, :], mean_percentiles[1, :], color=\"lightblue\")\n",
    "# plot mean prediction\n",
    "# ax.scatter(X_test.flatten(), mean_prediction, c=\"red\")\n",
    "ax.scatter(X2_test.flatten(), mean_prediction, c=\"red\")\n",
    "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 95% CI\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:32:15.649982500Z",
     "start_time": "2024-06-03T08:32:12.492938500Z"
    }
   },
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
